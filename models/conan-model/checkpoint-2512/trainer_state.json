{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2512,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00796495420151334,
      "grad_norm": 133.13047790527344,
      "learning_rate": 3.6e-05,
      "loss": 8.5937,
      "step": 10
    },
    {
      "epoch": 0.01592990840302668,
      "grad_norm": 15.227502822875977,
      "learning_rate": 7.6e-05,
      "loss": 4.6764,
      "step": 20
    },
    {
      "epoch": 0.023894862604540025,
      "grad_norm": 4.609999179840088,
      "learning_rate": 0.000116,
      "loss": 2.0707,
      "step": 30
    },
    {
      "epoch": 0.03185981680605336,
      "grad_norm": 0.696156919002533,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.0088,
      "step": 40
    },
    {
      "epoch": 0.039824771007566706,
      "grad_norm": 0.6791530847549438,
      "learning_rate": 0.000196,
      "loss": 0.9243,
      "step": 50
    },
    {
      "epoch": 0.04778972520908005,
      "grad_norm": 0.5873997807502747,
      "learning_rate": 0.00019951586874663799,
      "loss": 0.8584,
      "step": 60
    },
    {
      "epoch": 0.05575467941059339,
      "grad_norm": 0.5680383443832397,
      "learning_rate": 0.0001989779451317913,
      "loss": 0.8108,
      "step": 70
    },
    {
      "epoch": 0.06371963361210672,
      "grad_norm": 0.5462983250617981,
      "learning_rate": 0.00019844002151694462,
      "loss": 0.8032,
      "step": 80
    },
    {
      "epoch": 0.07168458781362007,
      "grad_norm": 0.5560625791549683,
      "learning_rate": 0.0001979020979020979,
      "loss": 0.7863,
      "step": 90
    },
    {
      "epoch": 0.07964954201513341,
      "grad_norm": 0.5551924705505371,
      "learning_rate": 0.00019736417428725122,
      "loss": 0.7818,
      "step": 100
    },
    {
      "epoch": 0.08761449621664676,
      "grad_norm": 0.5794554948806763,
      "learning_rate": 0.00019682625067240453,
      "loss": 0.7911,
      "step": 110
    },
    {
      "epoch": 0.0955794504181601,
      "grad_norm": 0.6070852875709534,
      "learning_rate": 0.00019628832705755785,
      "loss": 0.7937,
      "step": 120
    },
    {
      "epoch": 0.10354440461967343,
      "grad_norm": 0.5872889161109924,
      "learning_rate": 0.00019575040344271114,
      "loss": 0.7998,
      "step": 130
    },
    {
      "epoch": 0.11150935882118677,
      "grad_norm": 0.5939893126487732,
      "learning_rate": 0.00019521247982786445,
      "loss": 0.7531,
      "step": 140
    },
    {
      "epoch": 0.11947431302270012,
      "grad_norm": 0.5536476969718933,
      "learning_rate": 0.00019467455621301777,
      "loss": 0.7962,
      "step": 150
    },
    {
      "epoch": 0.12743926722421345,
      "grad_norm": 0.5763424634933472,
      "learning_rate": 0.00019413663259817108,
      "loss": 0.7622,
      "step": 160
    },
    {
      "epoch": 0.1354042214257268,
      "grad_norm": 0.5685245990753174,
      "learning_rate": 0.00019359870898332437,
      "loss": 0.7692,
      "step": 170
    },
    {
      "epoch": 0.14336917562724014,
      "grad_norm": 0.5661225318908691,
      "learning_rate": 0.0001930607853684777,
      "loss": 0.7409,
      "step": 180
    },
    {
      "epoch": 0.1513341298287535,
      "grad_norm": 0.5888197422027588,
      "learning_rate": 0.000192522861753631,
      "loss": 0.7779,
      "step": 190
    },
    {
      "epoch": 0.15929908403026682,
      "grad_norm": 0.5563907623291016,
      "learning_rate": 0.00019198493813878432,
      "loss": 0.7681,
      "step": 200
    },
    {
      "epoch": 0.16726403823178015,
      "grad_norm": 0.5455360412597656,
      "learning_rate": 0.0001914470145239376,
      "loss": 0.7614,
      "step": 210
    },
    {
      "epoch": 0.1752289924332935,
      "grad_norm": 0.5019347667694092,
      "learning_rate": 0.00019090909090909092,
      "loss": 0.7718,
      "step": 220
    },
    {
      "epoch": 0.18319394663480684,
      "grad_norm": 0.553898274898529,
      "learning_rate": 0.0001903711672942442,
      "loss": 0.7723,
      "step": 230
    },
    {
      "epoch": 0.1911589008363202,
      "grad_norm": 0.5552647113800049,
      "learning_rate": 0.00018983324367939755,
      "loss": 0.762,
      "step": 240
    },
    {
      "epoch": 0.19912385503783353,
      "grad_norm": 0.5925323367118835,
      "learning_rate": 0.00018929532006455084,
      "loss": 0.7665,
      "step": 250
    },
    {
      "epoch": 0.20708880923934686,
      "grad_norm": 0.4991273283958435,
      "learning_rate": 0.00018875739644970416,
      "loss": 0.799,
      "step": 260
    },
    {
      "epoch": 0.21505376344086022,
      "grad_norm": 0.5987207293510437,
      "learning_rate": 0.00018821947283485744,
      "loss": 0.81,
      "step": 270
    },
    {
      "epoch": 0.22301871764237355,
      "grad_norm": 0.5108638405799866,
      "learning_rate": 0.0001876815492200108,
      "loss": 0.7694,
      "step": 280
    },
    {
      "epoch": 0.2309836718438869,
      "grad_norm": 0.4778127074241638,
      "learning_rate": 0.00018714362560516407,
      "loss": 0.769,
      "step": 290
    },
    {
      "epoch": 0.23894862604540024,
      "grad_norm": 0.49839282035827637,
      "learning_rate": 0.0001866057019903174,
      "loss": 0.7449,
      "step": 300
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 0.5170665383338928,
      "learning_rate": 0.00018606777837547068,
      "loss": 0.7877,
      "step": 310
    },
    {
      "epoch": 0.2548785344484269,
      "grad_norm": 0.5087386965751648,
      "learning_rate": 0.00018552985476062402,
      "loss": 0.7811,
      "step": 320
    },
    {
      "epoch": 0.2628434886499403,
      "grad_norm": 0.5170137882232666,
      "learning_rate": 0.0001849919311457773,
      "loss": 0.8107,
      "step": 330
    },
    {
      "epoch": 0.2708084428514536,
      "grad_norm": 0.5296459794044495,
      "learning_rate": 0.0001844540075309306,
      "loss": 0.7449,
      "step": 340
    },
    {
      "epoch": 0.27877339705296694,
      "grad_norm": 0.490922212600708,
      "learning_rate": 0.0001839160839160839,
      "loss": 0.7663,
      "step": 350
    },
    {
      "epoch": 0.2867383512544803,
      "grad_norm": 0.519218921661377,
      "learning_rate": 0.00018337816030123723,
      "loss": 0.7249,
      "step": 360
    },
    {
      "epoch": 0.2947033054559936,
      "grad_norm": 0.549088716506958,
      "learning_rate": 0.00018284023668639054,
      "loss": 0.7524,
      "step": 370
    },
    {
      "epoch": 0.302668259657507,
      "grad_norm": 0.4940321743488312,
      "learning_rate": 0.00018230231307154383,
      "loss": 0.7297,
      "step": 380
    },
    {
      "epoch": 0.3106332138590203,
      "grad_norm": 0.46705177426338196,
      "learning_rate": 0.00018176438945669715,
      "loss": 0.7748,
      "step": 390
    },
    {
      "epoch": 0.31859816806053365,
      "grad_norm": 0.539527952671051,
      "learning_rate": 0.00018122646584185046,
      "loss": 0.7599,
      "step": 400
    },
    {
      "epoch": 0.326563122262047,
      "grad_norm": 0.5246395468711853,
      "learning_rate": 0.00018068854222700378,
      "loss": 0.7363,
      "step": 410
    },
    {
      "epoch": 0.3345280764635603,
      "grad_norm": 0.5225979685783386,
      "learning_rate": 0.00018015061861215707,
      "loss": 0.779,
      "step": 420
    },
    {
      "epoch": 0.3424930306650737,
      "grad_norm": 0.5296785235404968,
      "learning_rate": 0.00017961269499731038,
      "loss": 0.7487,
      "step": 430
    },
    {
      "epoch": 0.350457984866587,
      "grad_norm": 0.49650877714157104,
      "learning_rate": 0.0001790747713824637,
      "loss": 0.7392,
      "step": 440
    },
    {
      "epoch": 0.35842293906810035,
      "grad_norm": 0.4982154965400696,
      "learning_rate": 0.000178536847767617,
      "loss": 0.7535,
      "step": 450
    },
    {
      "epoch": 0.3663878932696137,
      "grad_norm": 0.5009126663208008,
      "learning_rate": 0.0001779989241527703,
      "loss": 0.7537,
      "step": 460
    },
    {
      "epoch": 0.374352847471127,
      "grad_norm": 0.494118869304657,
      "learning_rate": 0.00017746100053792362,
      "loss": 0.7849,
      "step": 470
    },
    {
      "epoch": 0.3823178016726404,
      "grad_norm": 0.5648855566978455,
      "learning_rate": 0.00017692307692307693,
      "loss": 0.7457,
      "step": 480
    },
    {
      "epoch": 0.39028275587415373,
      "grad_norm": 0.4557546377182007,
      "learning_rate": 0.00017638515330823025,
      "loss": 0.7284,
      "step": 490
    },
    {
      "epoch": 0.39824771007566706,
      "grad_norm": 0.5763673782348633,
      "learning_rate": 0.00017584722969338353,
      "loss": 0.7501,
      "step": 500
    },
    {
      "epoch": 0.4062126642771804,
      "grad_norm": 0.5994834303855896,
      "learning_rate": 0.00017530930607853685,
      "loss": 0.7547,
      "step": 510
    },
    {
      "epoch": 0.4141776184786937,
      "grad_norm": 0.511115312576294,
      "learning_rate": 0.00017477138246369016,
      "loss": 0.7666,
      "step": 520
    },
    {
      "epoch": 0.4221425726802071,
      "grad_norm": 0.4372538924217224,
      "learning_rate": 0.00017423345884884348,
      "loss": 0.744,
      "step": 530
    },
    {
      "epoch": 0.43010752688172044,
      "grad_norm": 0.5074840188026428,
      "learning_rate": 0.00017369553523399677,
      "loss": 0.7809,
      "step": 540
    },
    {
      "epoch": 0.43807248108323377,
      "grad_norm": 0.5417396426200867,
      "learning_rate": 0.00017315761161915008,
      "loss": 0.7411,
      "step": 550
    },
    {
      "epoch": 0.4460374352847471,
      "grad_norm": 0.5052236914634705,
      "learning_rate": 0.0001726196880043034,
      "loss": 0.7448,
      "step": 560
    },
    {
      "epoch": 0.4540023894862604,
      "grad_norm": 0.5071873664855957,
      "learning_rate": 0.00017208176438945671,
      "loss": 0.771,
      "step": 570
    },
    {
      "epoch": 0.4619673436877738,
      "grad_norm": 0.5701971054077148,
      "learning_rate": 0.00017154384077461,
      "loss": 0.765,
      "step": 580
    },
    {
      "epoch": 0.46993229788928714,
      "grad_norm": 0.522875964641571,
      "learning_rate": 0.00017100591715976332,
      "loss": 0.7561,
      "step": 590
    },
    {
      "epoch": 0.4778972520908005,
      "grad_norm": 0.5036452412605286,
      "learning_rate": 0.00017046799354491663,
      "loss": 0.7974,
      "step": 600
    },
    {
      "epoch": 0.4858622062923138,
      "grad_norm": 0.517601728439331,
      "learning_rate": 0.00016993006993006995,
      "loss": 0.7614,
      "step": 610
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 0.5130582451820374,
      "learning_rate": 0.00016939214631522324,
      "loss": 0.7422,
      "step": 620
    },
    {
      "epoch": 0.5017921146953405,
      "grad_norm": 0.46496492624282837,
      "learning_rate": 0.00016885422270037655,
      "loss": 0.7675,
      "step": 630
    },
    {
      "epoch": 0.5097570688968538,
      "grad_norm": 0.46519261598587036,
      "learning_rate": 0.00016831629908552987,
      "loss": 0.7535,
      "step": 640
    },
    {
      "epoch": 0.5177220230983672,
      "grad_norm": 0.4690667390823364,
      "learning_rate": 0.00016777837547068318,
      "loss": 0.7376,
      "step": 650
    },
    {
      "epoch": 0.5256869772998806,
      "grad_norm": 0.47642776370048523,
      "learning_rate": 0.00016724045185583647,
      "loss": 0.7435,
      "step": 660
    },
    {
      "epoch": 0.5336519315013939,
      "grad_norm": 0.4922437071800232,
      "learning_rate": 0.00016670252824098979,
      "loss": 0.7878,
      "step": 670
    },
    {
      "epoch": 0.5416168857029072,
      "grad_norm": 0.427765429019928,
      "learning_rate": 0.0001661646046261431,
      "loss": 0.7383,
      "step": 680
    },
    {
      "epoch": 0.5495818399044206,
      "grad_norm": 0.5159996151924133,
      "learning_rate": 0.00016562668101129642,
      "loss": 0.7415,
      "step": 690
    },
    {
      "epoch": 0.5575467941059339,
      "grad_norm": 0.4753890931606293,
      "learning_rate": 0.0001650887573964497,
      "loss": 0.7647,
      "step": 700
    },
    {
      "epoch": 0.5655117483074472,
      "grad_norm": 0.5033360123634338,
      "learning_rate": 0.00016455083378160302,
      "loss": 0.7617,
      "step": 710
    },
    {
      "epoch": 0.5734767025089605,
      "grad_norm": 0.5544610619544983,
      "learning_rate": 0.00016401291016675634,
      "loss": 0.7655,
      "step": 720
    },
    {
      "epoch": 0.5814416567104739,
      "grad_norm": 0.4957584738731384,
      "learning_rate": 0.00016347498655190962,
      "loss": 0.7706,
      "step": 730
    },
    {
      "epoch": 0.5894066109119872,
      "grad_norm": 0.49697065353393555,
      "learning_rate": 0.00016293706293706294,
      "loss": 0.7611,
      "step": 740
    },
    {
      "epoch": 0.5973715651135006,
      "grad_norm": 0.514756977558136,
      "learning_rate": 0.00016239913932221625,
      "loss": 0.7573,
      "step": 750
    },
    {
      "epoch": 0.605336519315014,
      "grad_norm": 0.5491628646850586,
      "learning_rate": 0.00016186121570736957,
      "loss": 0.786,
      "step": 760
    },
    {
      "epoch": 0.6133014735165273,
      "grad_norm": 0.5565916299819946,
      "learning_rate": 0.00016132329209252286,
      "loss": 0.7502,
      "step": 770
    },
    {
      "epoch": 0.6212664277180406,
      "grad_norm": 0.5392376184463501,
      "learning_rate": 0.00016078536847767617,
      "loss": 0.7493,
      "step": 780
    },
    {
      "epoch": 0.629231381919554,
      "grad_norm": 0.5479769706726074,
      "learning_rate": 0.0001602474448628295,
      "loss": 0.7957,
      "step": 790
    },
    {
      "epoch": 0.6371963361210673,
      "grad_norm": 0.5240273475646973,
      "learning_rate": 0.0001597095212479828,
      "loss": 0.7792,
      "step": 800
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 0.4773210883140564,
      "learning_rate": 0.0001591715976331361,
      "loss": 0.7606,
      "step": 810
    },
    {
      "epoch": 0.653126244524094,
      "grad_norm": 0.44444289803504944,
      "learning_rate": 0.0001586336740182894,
      "loss": 0.7673,
      "step": 820
    },
    {
      "epoch": 0.6610911987256073,
      "grad_norm": 0.5055608153343201,
      "learning_rate": 0.00015809575040344272,
      "loss": 0.77,
      "step": 830
    },
    {
      "epoch": 0.6690561529271206,
      "grad_norm": 0.46055611968040466,
      "learning_rate": 0.00015755782678859604,
      "loss": 0.7545,
      "step": 840
    },
    {
      "epoch": 0.6770211071286341,
      "grad_norm": 0.497326523065567,
      "learning_rate": 0.00015701990317374933,
      "loss": 0.7276,
      "step": 850
    },
    {
      "epoch": 0.6849860613301474,
      "grad_norm": 0.4432331323623657,
      "learning_rate": 0.00015648197955890264,
      "loss": 0.7483,
      "step": 860
    },
    {
      "epoch": 0.6929510155316607,
      "grad_norm": 0.4714212417602539,
      "learning_rate": 0.00015594405594405596,
      "loss": 0.757,
      "step": 870
    },
    {
      "epoch": 0.700915969733174,
      "grad_norm": 0.4514620006084442,
      "learning_rate": 0.00015540613232920927,
      "loss": 0.726,
      "step": 880
    },
    {
      "epoch": 0.7088809239346874,
      "grad_norm": 0.4436425268650055,
      "learning_rate": 0.00015486820871436256,
      "loss": 0.7244,
      "step": 890
    },
    {
      "epoch": 0.7168458781362007,
      "grad_norm": 0.4528445303440094,
      "learning_rate": 0.00015433028509951588,
      "loss": 0.7403,
      "step": 900
    },
    {
      "epoch": 0.724810832337714,
      "grad_norm": 0.4918217957019806,
      "learning_rate": 0.0001537923614846692,
      "loss": 0.7304,
      "step": 910
    },
    {
      "epoch": 0.7327757865392274,
      "grad_norm": 0.4754583239555359,
      "learning_rate": 0.0001532544378698225,
      "loss": 0.7506,
      "step": 920
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.5018563270568848,
      "learning_rate": 0.0001527165142549758,
      "loss": 0.7681,
      "step": 930
    },
    {
      "epoch": 0.748705694942254,
      "grad_norm": 0.49649712443351746,
      "learning_rate": 0.0001521785906401291,
      "loss": 0.7465,
      "step": 940
    },
    {
      "epoch": 0.7566706491437675,
      "grad_norm": 0.4681350290775299,
      "learning_rate": 0.00015164066702528243,
      "loss": 0.7273,
      "step": 950
    },
    {
      "epoch": 0.7646356033452808,
      "grad_norm": 0.4947724938392639,
      "learning_rate": 0.00015110274341043574,
      "loss": 0.7755,
      "step": 960
    },
    {
      "epoch": 0.7726005575467941,
      "grad_norm": 0.4668620526790619,
      "learning_rate": 0.00015056481979558903,
      "loss": 0.7154,
      "step": 970
    },
    {
      "epoch": 0.7805655117483075,
      "grad_norm": 0.4795647859573364,
      "learning_rate": 0.00015002689618074234,
      "loss": 0.7368,
      "step": 980
    },
    {
      "epoch": 0.7885304659498208,
      "grad_norm": 0.4544738829135895,
      "learning_rate": 0.00014948897256589566,
      "loss": 0.7445,
      "step": 990
    },
    {
      "epoch": 0.7964954201513341,
      "grad_norm": 0.4597337543964386,
      "learning_rate": 0.00014895104895104897,
      "loss": 0.772,
      "step": 1000
    },
    {
      "epoch": 0.8044603743528475,
      "grad_norm": 0.4793347120285034,
      "learning_rate": 0.00014841312533620226,
      "loss": 0.7226,
      "step": 1010
    },
    {
      "epoch": 0.8124253285543608,
      "grad_norm": 0.46236521005630493,
      "learning_rate": 0.00014787520172135558,
      "loss": 0.7293,
      "step": 1020
    },
    {
      "epoch": 0.8203902827558741,
      "grad_norm": 0.4980211853981018,
      "learning_rate": 0.0001473372781065089,
      "loss": 0.7556,
      "step": 1030
    },
    {
      "epoch": 0.8283552369573874,
      "grad_norm": 0.45580118894577026,
      "learning_rate": 0.0001467993544916622,
      "loss": 0.7821,
      "step": 1040
    },
    {
      "epoch": 0.8363201911589009,
      "grad_norm": 0.47210371494293213,
      "learning_rate": 0.0001462614308768155,
      "loss": 0.7815,
      "step": 1050
    },
    {
      "epoch": 0.8442851453604142,
      "grad_norm": 0.47734859585762024,
      "learning_rate": 0.0001457235072619688,
      "loss": 0.7624,
      "step": 1060
    },
    {
      "epoch": 0.8522500995619275,
      "grad_norm": 0.45166775584220886,
      "learning_rate": 0.0001451855836471221,
      "loss": 0.7601,
      "step": 1070
    },
    {
      "epoch": 0.8602150537634409,
      "grad_norm": 0.4564398527145386,
      "learning_rate": 0.00014464766003227544,
      "loss": 0.7276,
      "step": 1080
    },
    {
      "epoch": 0.8681800079649542,
      "grad_norm": 0.4579167366027832,
      "learning_rate": 0.00014410973641742873,
      "loss": 0.7516,
      "step": 1090
    },
    {
      "epoch": 0.8761449621664675,
      "grad_norm": 0.45872241258621216,
      "learning_rate": 0.00014357181280258205,
      "loss": 0.7868,
      "step": 1100
    },
    {
      "epoch": 0.8841099163679809,
      "grad_norm": 0.549182116985321,
      "learning_rate": 0.00014303388918773533,
      "loss": 0.7316,
      "step": 1110
    },
    {
      "epoch": 0.8920748705694942,
      "grad_norm": 0.5116949081420898,
      "learning_rate": 0.00014249596557288865,
      "loss": 0.7851,
      "step": 1120
    },
    {
      "epoch": 0.9000398247710075,
      "grad_norm": 0.45142361521720886,
      "learning_rate": 0.00014195804195804197,
      "loss": 0.757,
      "step": 1130
    },
    {
      "epoch": 0.9080047789725209,
      "grad_norm": 0.47317203879356384,
      "learning_rate": 0.00014142011834319525,
      "loss": 0.7527,
      "step": 1140
    },
    {
      "epoch": 0.9159697331740343,
      "grad_norm": 0.45667991042137146,
      "learning_rate": 0.00014088219472834857,
      "loss": 0.7343,
      "step": 1150
    },
    {
      "epoch": 0.9239346873755476,
      "grad_norm": 0.4944150149822235,
      "learning_rate": 0.00014034427111350188,
      "loss": 0.7533,
      "step": 1160
    },
    {
      "epoch": 0.931899641577061,
      "grad_norm": 0.4322386682033539,
      "learning_rate": 0.0001398063474986552,
      "loss": 0.7407,
      "step": 1170
    },
    {
      "epoch": 0.9398645957785743,
      "grad_norm": 0.4452013671398163,
      "learning_rate": 0.0001392684238838085,
      "loss": 0.7574,
      "step": 1180
    },
    {
      "epoch": 0.9478295499800876,
      "grad_norm": 0.519768476486206,
      "learning_rate": 0.0001387305002689618,
      "loss": 0.7405,
      "step": 1190
    },
    {
      "epoch": 0.955794504181601,
      "grad_norm": 0.4438643455505371,
      "learning_rate": 0.00013819257665411512,
      "loss": 0.7489,
      "step": 1200
    },
    {
      "epoch": 0.9637594583831143,
      "grad_norm": 0.46488696336746216,
      "learning_rate": 0.00013765465303926843,
      "loss": 0.7641,
      "step": 1210
    },
    {
      "epoch": 0.9717244125846276,
      "grad_norm": 0.4657285511493683,
      "learning_rate": 0.00013711672942442172,
      "loss": 0.7335,
      "step": 1220
    },
    {
      "epoch": 0.9796893667861409,
      "grad_norm": 0.5356078147888184,
      "learning_rate": 0.00013657880580957504,
      "loss": 0.7374,
      "step": 1230
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 0.5349792838096619,
      "learning_rate": 0.00013604088219472835,
      "loss": 0.7201,
      "step": 1240
    },
    {
      "epoch": 0.9956192751891677,
      "grad_norm": 0.45736873149871826,
      "learning_rate": 0.00013550295857988167,
      "loss": 0.7562,
      "step": 1250
    },
    {
      "epoch": 1.0031859816806052,
      "grad_norm": 0.4625971019268036,
      "learning_rate": 0.00013496503496503496,
      "loss": 0.7673,
      "step": 1260
    },
    {
      "epoch": 1.0111509358821187,
      "grad_norm": 0.4288552701473236,
      "learning_rate": 0.00013442711135018827,
      "loss": 0.7597,
      "step": 1270
    },
    {
      "epoch": 1.0191158900836321,
      "grad_norm": 0.4733198285102844,
      "learning_rate": 0.0001338891877353416,
      "loss": 0.7344,
      "step": 1280
    },
    {
      "epoch": 1.0270808442851453,
      "grad_norm": 0.4954827129840851,
      "learning_rate": 0.0001333512641204949,
      "loss": 0.7387,
      "step": 1290
    },
    {
      "epoch": 1.0350457984866588,
      "grad_norm": 0.520415723323822,
      "learning_rate": 0.0001328133405056482,
      "loss": 0.7091,
      "step": 1300
    },
    {
      "epoch": 1.043010752688172,
      "grad_norm": 0.4539541006088257,
      "learning_rate": 0.0001322754168908015,
      "loss": 0.7184,
      "step": 1310
    },
    {
      "epoch": 1.0509757068896854,
      "grad_norm": 0.4909593462944031,
      "learning_rate": 0.00013173749327595482,
      "loss": 0.7257,
      "step": 1320
    },
    {
      "epoch": 1.0589406610911987,
      "grad_norm": 0.44648057222366333,
      "learning_rate": 0.00013119956966110814,
      "loss": 0.7291,
      "step": 1330
    },
    {
      "epoch": 1.066905615292712,
      "grad_norm": 0.5058598518371582,
      "learning_rate": 0.00013066164604626142,
      "loss": 0.7511,
      "step": 1340
    },
    {
      "epoch": 1.0748705694942253,
      "grad_norm": 0.5058706998825073,
      "learning_rate": 0.00013012372243141474,
      "loss": 0.6728,
      "step": 1350
    },
    {
      "epoch": 1.0828355236957388,
      "grad_norm": 0.5570462942123413,
      "learning_rate": 0.00012958579881656806,
      "loss": 0.7404,
      "step": 1360
    },
    {
      "epoch": 1.0908004778972522,
      "grad_norm": 0.5753740072250366,
      "learning_rate": 0.00012904787520172137,
      "loss": 0.7354,
      "step": 1370
    },
    {
      "epoch": 1.0987654320987654,
      "grad_norm": 0.5565868020057678,
      "learning_rate": 0.00012850995158687466,
      "loss": 0.7111,
      "step": 1380
    },
    {
      "epoch": 1.1067303863002789,
      "grad_norm": 0.528270423412323,
      "learning_rate": 0.00012797202797202797,
      "loss": 0.7916,
      "step": 1390
    },
    {
      "epoch": 1.114695340501792,
      "grad_norm": 0.5695818066596985,
      "learning_rate": 0.0001274341043571813,
      "loss": 0.7574,
      "step": 1400
    },
    {
      "epoch": 1.1226602947033055,
      "grad_norm": 0.47674667835235596,
      "learning_rate": 0.0001268961807423346,
      "loss": 0.7077,
      "step": 1410
    },
    {
      "epoch": 1.1306252489048187,
      "grad_norm": 0.5483880639076233,
      "learning_rate": 0.0001263582571274879,
      "loss": 0.7432,
      "step": 1420
    },
    {
      "epoch": 1.1385902031063322,
      "grad_norm": 0.4990485608577728,
      "learning_rate": 0.0001258203335126412,
      "loss": 0.7255,
      "step": 1430
    },
    {
      "epoch": 1.1465551573078454,
      "grad_norm": 0.46087712049484253,
      "learning_rate": 0.00012528240989779452,
      "loss": 0.7268,
      "step": 1440
    },
    {
      "epoch": 1.1545201115093588,
      "grad_norm": 0.47685572504997253,
      "learning_rate": 0.00012474448628294784,
      "loss": 0.6883,
      "step": 1450
    },
    {
      "epoch": 1.162485065710872,
      "grad_norm": 0.5443675518035889,
      "learning_rate": 0.00012420656266810113,
      "loss": 0.7317,
      "step": 1460
    },
    {
      "epoch": 1.1704500199123855,
      "grad_norm": 0.4945436418056488,
      "learning_rate": 0.00012366863905325444,
      "loss": 0.7094,
      "step": 1470
    },
    {
      "epoch": 1.178414974113899,
      "grad_norm": 0.4786890745162964,
      "learning_rate": 0.00012313071543840776,
      "loss": 0.7156,
      "step": 1480
    },
    {
      "epoch": 1.1863799283154122,
      "grad_norm": 0.5114238262176514,
      "learning_rate": 0.00012259279182356107,
      "loss": 0.7351,
      "step": 1490
    },
    {
      "epoch": 1.1943448825169256,
      "grad_norm": 0.4729917347431183,
      "learning_rate": 0.00012205486820871436,
      "loss": 0.7134,
      "step": 1500
    },
    {
      "epoch": 1.2023098367184388,
      "grad_norm": 0.4813857972621918,
      "learning_rate": 0.00012151694459386769,
      "loss": 0.7245,
      "step": 1510
    },
    {
      "epoch": 1.2102747909199523,
      "grad_norm": 0.5106756687164307,
      "learning_rate": 0.00012097902097902098,
      "loss": 0.718,
      "step": 1520
    },
    {
      "epoch": 1.2182397451214655,
      "grad_norm": 0.5027218461036682,
      "learning_rate": 0.00012044109736417428,
      "loss": 0.7467,
      "step": 1530
    },
    {
      "epoch": 1.226204699322979,
      "grad_norm": 0.5211023688316345,
      "learning_rate": 0.0001199031737493276,
      "loss": 0.6958,
      "step": 1540
    },
    {
      "epoch": 1.2341696535244921,
      "grad_norm": 0.5364963412284851,
      "learning_rate": 0.0001193652501344809,
      "loss": 0.7184,
      "step": 1550
    },
    {
      "epoch": 1.2421346077260056,
      "grad_norm": 0.48435521125793457,
      "learning_rate": 0.00011882732651963421,
      "loss": 0.7349,
      "step": 1560
    },
    {
      "epoch": 1.2500995619275188,
      "grad_norm": 0.5540192127227783,
      "learning_rate": 0.00011828940290478751,
      "loss": 0.7384,
      "step": 1570
    },
    {
      "epoch": 1.2580645161290323,
      "grad_norm": 0.4760480523109436,
      "learning_rate": 0.00011775147928994083,
      "loss": 0.7265,
      "step": 1580
    },
    {
      "epoch": 1.2660294703305457,
      "grad_norm": 0.4919531047344208,
      "learning_rate": 0.00011721355567509413,
      "loss": 0.7095,
      "step": 1590
    },
    {
      "epoch": 1.273994424532059,
      "grad_norm": 0.5661686062812805,
      "learning_rate": 0.00011667563206024745,
      "loss": 0.6682,
      "step": 1600
    },
    {
      "epoch": 1.2819593787335724,
      "grad_norm": 0.507229745388031,
      "learning_rate": 0.00011613770844540075,
      "loss": 0.7645,
      "step": 1610
    },
    {
      "epoch": 1.2899243329350856,
      "grad_norm": 0.5442198514938354,
      "learning_rate": 0.00011559978483055406,
      "loss": 0.7404,
      "step": 1620
    },
    {
      "epoch": 1.297889287136599,
      "grad_norm": 0.5415163040161133,
      "learning_rate": 0.00011506186121570737,
      "loss": 0.7218,
      "step": 1630
    },
    {
      "epoch": 1.3058542413381122,
      "grad_norm": 0.45485225319862366,
      "learning_rate": 0.00011452393760086068,
      "loss": 0.7137,
      "step": 1640
    },
    {
      "epoch": 1.3138191955396257,
      "grad_norm": 0.5133085250854492,
      "learning_rate": 0.00011398601398601398,
      "loss": 0.7502,
      "step": 1650
    },
    {
      "epoch": 1.3217841497411391,
      "grad_norm": 0.4626544117927551,
      "learning_rate": 0.0001134480903711673,
      "loss": 0.7129,
      "step": 1660
    },
    {
      "epoch": 1.3297491039426523,
      "grad_norm": 0.5139325857162476,
      "learning_rate": 0.0001129101667563206,
      "loss": 0.7039,
      "step": 1670
    },
    {
      "epoch": 1.3377140581441656,
      "grad_norm": 0.4753801226615906,
      "learning_rate": 0.00011237224314147392,
      "loss": 0.7095,
      "step": 1680
    },
    {
      "epoch": 1.345679012345679,
      "grad_norm": 0.49914097785949707,
      "learning_rate": 0.00011183431952662722,
      "loss": 0.7196,
      "step": 1690
    },
    {
      "epoch": 1.3536439665471924,
      "grad_norm": 0.47906267642974854,
      "learning_rate": 0.00011129639591178053,
      "loss": 0.7155,
      "step": 1700
    },
    {
      "epoch": 1.3616089207487057,
      "grad_norm": 0.5311673283576965,
      "learning_rate": 0.00011075847229693383,
      "loss": 0.7377,
      "step": 1710
    },
    {
      "epoch": 1.369573874950219,
      "grad_norm": 0.4906010627746582,
      "learning_rate": 0.00011022054868208715,
      "loss": 0.7172,
      "step": 1720
    },
    {
      "epoch": 1.3775388291517323,
      "grad_norm": 0.5275512933731079,
      "learning_rate": 0.00010968262506724045,
      "loss": 0.7382,
      "step": 1730
    },
    {
      "epoch": 1.3855037833532458,
      "grad_norm": 0.47829827666282654,
      "learning_rate": 0.00010914470145239377,
      "loss": 0.719,
      "step": 1740
    },
    {
      "epoch": 1.393468737554759,
      "grad_norm": 0.5060559511184692,
      "learning_rate": 0.00010860677783754707,
      "loss": 0.725,
      "step": 1750
    },
    {
      "epoch": 1.4014336917562724,
      "grad_norm": 0.5723638534545898,
      "learning_rate": 0.00010806885422270038,
      "loss": 0.7258,
      "step": 1760
    },
    {
      "epoch": 1.4093986459577859,
      "grad_norm": 0.5221225023269653,
      "learning_rate": 0.00010753093060785369,
      "loss": 0.7284,
      "step": 1770
    },
    {
      "epoch": 1.417363600159299,
      "grad_norm": 0.5279989242553711,
      "learning_rate": 0.000106993006993007,
      "loss": 0.7477,
      "step": 1780
    },
    {
      "epoch": 1.4253285543608123,
      "grad_norm": 0.5157337784767151,
      "learning_rate": 0.0001064550833781603,
      "loss": 0.7225,
      "step": 1790
    },
    {
      "epoch": 1.4332935085623257,
      "grad_norm": 0.5328108072280884,
      "learning_rate": 0.00010591715976331362,
      "loss": 0.7112,
      "step": 1800
    },
    {
      "epoch": 1.4412584627638392,
      "grad_norm": 0.4593004584312439,
      "learning_rate": 0.00010537923614846692,
      "loss": 0.7255,
      "step": 1810
    },
    {
      "epoch": 1.4492234169653524,
      "grad_norm": 0.51335608959198,
      "learning_rate": 0.00010484131253362023,
      "loss": 0.7232,
      "step": 1820
    },
    {
      "epoch": 1.4571883711668658,
      "grad_norm": 0.508044958114624,
      "learning_rate": 0.00010430338891877354,
      "loss": 0.7449,
      "step": 1830
    },
    {
      "epoch": 1.465153325368379,
      "grad_norm": 0.5165323615074158,
      "learning_rate": 0.00010376546530392685,
      "loss": 0.7224,
      "step": 1840
    },
    {
      "epoch": 1.4731182795698925,
      "grad_norm": 0.48717552423477173,
      "learning_rate": 0.00010322754168908015,
      "loss": 0.71,
      "step": 1850
    },
    {
      "epoch": 1.4810832337714057,
      "grad_norm": 0.5158607363700867,
      "learning_rate": 0.00010268961807423347,
      "loss": 0.7436,
      "step": 1860
    },
    {
      "epoch": 1.4890481879729192,
      "grad_norm": 0.5024345517158508,
      "learning_rate": 0.00010215169445938677,
      "loss": 0.7244,
      "step": 1870
    },
    {
      "epoch": 1.4970131421744326,
      "grad_norm": 0.5138691067695618,
      "learning_rate": 0.00010161377084454009,
      "loss": 0.6842,
      "step": 1880
    },
    {
      "epoch": 1.5049780963759458,
      "grad_norm": 0.5467655658721924,
      "learning_rate": 0.00010107584722969339,
      "loss": 0.723,
      "step": 1890
    },
    {
      "epoch": 1.512943050577459,
      "grad_norm": 0.4740504324436188,
      "learning_rate": 0.0001005379236148467,
      "loss": 0.708,
      "step": 1900
    },
    {
      "epoch": 1.5209080047789725,
      "grad_norm": 0.5175864696502686,
      "learning_rate": 0.0001,
      "loss": 0.7196,
      "step": 1910
    },
    {
      "epoch": 1.528872958980486,
      "grad_norm": 0.4771402180194855,
      "learning_rate": 9.94620763851533e-05,
      "loss": 0.7151,
      "step": 1920
    },
    {
      "epoch": 1.5368379131819991,
      "grad_norm": 0.5163642764091492,
      "learning_rate": 9.892415277030662e-05,
      "loss": 0.7353,
      "step": 1930
    },
    {
      "epoch": 1.5448028673835126,
      "grad_norm": 0.555423378944397,
      "learning_rate": 9.838622915545992e-05,
      "loss": 0.7287,
      "step": 1940
    },
    {
      "epoch": 1.552767821585026,
      "grad_norm": 0.5214229822158813,
      "learning_rate": 9.784830554061324e-05,
      "loss": 0.7151,
      "step": 1950
    },
    {
      "epoch": 1.5607327757865392,
      "grad_norm": 0.5850131511688232,
      "learning_rate": 9.731038192576654e-05,
      "loss": 0.7686,
      "step": 1960
    },
    {
      "epoch": 1.5686977299880525,
      "grad_norm": 0.5416763424873352,
      "learning_rate": 9.677245831091986e-05,
      "loss": 0.7532,
      "step": 1970
    },
    {
      "epoch": 1.576662684189566,
      "grad_norm": 0.5511633157730103,
      "learning_rate": 9.623453469607316e-05,
      "loss": 0.728,
      "step": 1980
    },
    {
      "epoch": 1.5846276383910793,
      "grad_norm": 0.5364885330200195,
      "learning_rate": 9.569661108122647e-05,
      "loss": 0.783,
      "step": 1990
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 0.4735919237136841,
      "learning_rate": 9.515868746637977e-05,
      "loss": 0.6912,
      "step": 2000
    },
    {
      "epoch": 1.6005575467941058,
      "grad_norm": 0.5241736173629761,
      "learning_rate": 9.462076385153309e-05,
      "loss": 0.7267,
      "step": 2010
    },
    {
      "epoch": 1.6085225009956192,
      "grad_norm": 0.4514029324054718,
      "learning_rate": 9.408284023668639e-05,
      "loss": 0.7321,
      "step": 2020
    },
    {
      "epoch": 1.6164874551971327,
      "grad_norm": 0.5544889569282532,
      "learning_rate": 9.354491662183971e-05,
      "loss": 0.7516,
      "step": 2030
    },
    {
      "epoch": 1.6244524093986459,
      "grad_norm": 0.5061807632446289,
      "learning_rate": 9.300699300699301e-05,
      "loss": 0.7139,
      "step": 2040
    },
    {
      "epoch": 1.6324173636001593,
      "grad_norm": 0.487118124961853,
      "learning_rate": 9.246906939214632e-05,
      "loss": 0.7307,
      "step": 2050
    },
    {
      "epoch": 1.6403823178016728,
      "grad_norm": 0.5057170987129211,
      "learning_rate": 9.193114577729963e-05,
      "loss": 0.7021,
      "step": 2060
    },
    {
      "epoch": 1.648347272003186,
      "grad_norm": 0.5394274592399597,
      "learning_rate": 9.139322216245294e-05,
      "loss": 0.7319,
      "step": 2070
    },
    {
      "epoch": 1.6563122262046992,
      "grad_norm": 0.5435377359390259,
      "learning_rate": 9.085529854760624e-05,
      "loss": 0.7404,
      "step": 2080
    },
    {
      "epoch": 1.6642771804062126,
      "grad_norm": 0.44337692856788635,
      "learning_rate": 9.031737493275956e-05,
      "loss": 0.6873,
      "step": 2090
    },
    {
      "epoch": 1.672242134607726,
      "grad_norm": 0.5030319094657898,
      "learning_rate": 8.977945131791286e-05,
      "loss": 0.7466,
      "step": 2100
    },
    {
      "epoch": 1.6802070888092393,
      "grad_norm": 0.5560398101806641,
      "learning_rate": 8.924152770306616e-05,
      "loss": 0.7392,
      "step": 2110
    },
    {
      "epoch": 1.6881720430107527,
      "grad_norm": 0.46371927857398987,
      "learning_rate": 8.870360408821948e-05,
      "loss": 0.7301,
      "step": 2120
    },
    {
      "epoch": 1.6961369972122662,
      "grad_norm": 0.48637527227401733,
      "learning_rate": 8.816568047337278e-05,
      "loss": 0.7395,
      "step": 2130
    },
    {
      "epoch": 1.7041019514137794,
      "grad_norm": 0.5033164024353027,
      "learning_rate": 8.76277568585261e-05,
      "loss": 0.687,
      "step": 2140
    },
    {
      "epoch": 1.7120669056152926,
      "grad_norm": 0.5034162402153015,
      "learning_rate": 8.70898332436794e-05,
      "loss": 0.7131,
      "step": 2150
    },
    {
      "epoch": 1.720031859816806,
      "grad_norm": 0.5034641623497009,
      "learning_rate": 8.655190962883271e-05,
      "loss": 0.7267,
      "step": 2160
    },
    {
      "epoch": 1.7279968140183195,
      "grad_norm": 0.5509538650512695,
      "learning_rate": 8.601398601398601e-05,
      "loss": 0.7008,
      "step": 2170
    },
    {
      "epoch": 1.7359617682198327,
      "grad_norm": 0.5422916412353516,
      "learning_rate": 8.547606239913933e-05,
      "loss": 0.758,
      "step": 2180
    },
    {
      "epoch": 1.743926722421346,
      "grad_norm": 0.47149571776390076,
      "learning_rate": 8.493813878429263e-05,
      "loss": 0.7125,
      "step": 2190
    },
    {
      "epoch": 1.7518916766228594,
      "grad_norm": 0.5824500918388367,
      "learning_rate": 8.440021516944595e-05,
      "loss": 0.73,
      "step": 2200
    },
    {
      "epoch": 1.7598566308243728,
      "grad_norm": 0.49923044443130493,
      "learning_rate": 8.386229155459925e-05,
      "loss": 0.7182,
      "step": 2210
    },
    {
      "epoch": 1.767821585025886,
      "grad_norm": 0.5198145508766174,
      "learning_rate": 8.332436793975256e-05,
      "loss": 0.7351,
      "step": 2220
    },
    {
      "epoch": 1.7757865392273995,
      "grad_norm": 0.5441893339157104,
      "learning_rate": 8.278644432490586e-05,
      "loss": 0.718,
      "step": 2230
    },
    {
      "epoch": 1.783751493428913,
      "grad_norm": 0.4886374771595001,
      "learning_rate": 8.224852071005918e-05,
      "loss": 0.7032,
      "step": 2240
    },
    {
      "epoch": 1.7917164476304261,
      "grad_norm": 0.4992038309574127,
      "learning_rate": 8.171059709521248e-05,
      "loss": 0.7397,
      "step": 2250
    },
    {
      "epoch": 1.7996814018319394,
      "grad_norm": 0.4685755670070648,
      "learning_rate": 8.11726734803658e-05,
      "loss": 0.7272,
      "step": 2260
    },
    {
      "epoch": 1.8076463560334528,
      "grad_norm": 0.5154479742050171,
      "learning_rate": 8.06347498655191e-05,
      "loss": 0.7594,
      "step": 2270
    },
    {
      "epoch": 1.8156113102349662,
      "grad_norm": 0.47591331601142883,
      "learning_rate": 8.009682625067241e-05,
      "loss": 0.725,
      "step": 2280
    },
    {
      "epoch": 1.8235762644364795,
      "grad_norm": 0.5525245070457458,
      "learning_rate": 7.955890263582572e-05,
      "loss": 0.7281,
      "step": 2290
    },
    {
      "epoch": 1.8315412186379927,
      "grad_norm": 0.5060536861419678,
      "learning_rate": 7.902097902097903e-05,
      "loss": 0.7478,
      "step": 2300
    },
    {
      "epoch": 1.8395061728395061,
      "grad_norm": 0.4930019676685333,
      "learning_rate": 7.848305540613233e-05,
      "loss": 0.7522,
      "step": 2310
    },
    {
      "epoch": 1.8474711270410196,
      "grad_norm": 0.5212144255638123,
      "learning_rate": 7.794513179128565e-05,
      "loss": 0.7217,
      "step": 2320
    },
    {
      "epoch": 1.8554360812425328,
      "grad_norm": 0.48041462898254395,
      "learning_rate": 7.740720817643895e-05,
      "loss": 0.737,
      "step": 2330
    },
    {
      "epoch": 1.8634010354440462,
      "grad_norm": 0.49279695749282837,
      "learning_rate": 7.686928456159225e-05,
      "loss": 0.7308,
      "step": 2340
    },
    {
      "epoch": 1.8713659896455597,
      "grad_norm": 0.5309919714927673,
      "learning_rate": 7.633136094674557e-05,
      "loss": 0.6768,
      "step": 2350
    },
    {
      "epoch": 1.8793309438470729,
      "grad_norm": 0.5160070657730103,
      "learning_rate": 7.579343733189887e-05,
      "loss": 0.6959,
      "step": 2360
    },
    {
      "epoch": 1.887295898048586,
      "grad_norm": 0.47561001777648926,
      "learning_rate": 7.525551371705218e-05,
      "loss": 0.7374,
      "step": 2370
    },
    {
      "epoch": 1.8952608522500995,
      "grad_norm": 0.5049702525138855,
      "learning_rate": 7.471759010220549e-05,
      "loss": 0.6987,
      "step": 2380
    },
    {
      "epoch": 1.903225806451613,
      "grad_norm": 0.5691946148872375,
      "learning_rate": 7.41796664873588e-05,
      "loss": 0.7264,
      "step": 2390
    },
    {
      "epoch": 1.9111907606531262,
      "grad_norm": 0.5225602388381958,
      "learning_rate": 7.36417428725121e-05,
      "loss": 0.7128,
      "step": 2400
    },
    {
      "epoch": 1.9191557148546394,
      "grad_norm": 0.5281886458396912,
      "learning_rate": 7.310381925766542e-05,
      "loss": 0.7262,
      "step": 2410
    },
    {
      "epoch": 1.927120669056153,
      "grad_norm": 0.601759672164917,
      "learning_rate": 7.256589564281872e-05,
      "loss": 0.7302,
      "step": 2420
    },
    {
      "epoch": 1.9350856232576663,
      "grad_norm": 0.5215563774108887,
      "learning_rate": 7.202797202797204e-05,
      "loss": 0.7604,
      "step": 2430
    },
    {
      "epoch": 1.9430505774591795,
      "grad_norm": 0.5226988196372986,
      "learning_rate": 7.149004841312534e-05,
      "loss": 0.707,
      "step": 2440
    },
    {
      "epoch": 1.951015531660693,
      "grad_norm": 0.5164101719856262,
      "learning_rate": 7.095212479827865e-05,
      "loss": 0.7029,
      "step": 2450
    },
    {
      "epoch": 1.9589804858622064,
      "grad_norm": 0.5570307970046997,
      "learning_rate": 7.041420118343195e-05,
      "loss": 0.7354,
      "step": 2460
    },
    {
      "epoch": 1.9669454400637196,
      "grad_norm": 0.5072523355484009,
      "learning_rate": 6.987627756858527e-05,
      "loss": 0.724,
      "step": 2470
    },
    {
      "epoch": 1.9749103942652328,
      "grad_norm": 0.534125030040741,
      "learning_rate": 6.933835395373857e-05,
      "loss": 0.7134,
      "step": 2480
    },
    {
      "epoch": 1.9828753484667463,
      "grad_norm": 0.48700380325317383,
      "learning_rate": 6.880043033889189e-05,
      "loss": 0.7018,
      "step": 2490
    },
    {
      "epoch": 1.9908403026682597,
      "grad_norm": 0.545971155166626,
      "learning_rate": 6.826250672404519e-05,
      "loss": 0.7039,
      "step": 2500
    },
    {
      "epoch": 1.998805256869773,
      "grad_norm": 0.5028773546218872,
      "learning_rate": 6.772458310919849e-05,
      "loss": 0.7293,
      "step": 2510
    }
  ],
  "logging_steps": 10,
  "max_steps": 3768,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5994781964435456e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
