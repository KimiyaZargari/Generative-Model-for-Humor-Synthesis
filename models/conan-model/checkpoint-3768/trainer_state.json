{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3768,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00796495420151334,
      "grad_norm": 133.13047790527344,
      "learning_rate": 3.6e-05,
      "loss": 8.5937,
      "step": 10
    },
    {
      "epoch": 0.01592990840302668,
      "grad_norm": 15.227502822875977,
      "learning_rate": 7.6e-05,
      "loss": 4.6764,
      "step": 20
    },
    {
      "epoch": 0.023894862604540025,
      "grad_norm": 4.609999179840088,
      "learning_rate": 0.000116,
      "loss": 2.0707,
      "step": 30
    },
    {
      "epoch": 0.03185981680605336,
      "grad_norm": 0.696156919002533,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.0088,
      "step": 40
    },
    {
      "epoch": 0.039824771007566706,
      "grad_norm": 0.6791530847549438,
      "learning_rate": 0.000196,
      "loss": 0.9243,
      "step": 50
    },
    {
      "epoch": 0.04778972520908005,
      "grad_norm": 0.5873997807502747,
      "learning_rate": 0.00019951586874663799,
      "loss": 0.8584,
      "step": 60
    },
    {
      "epoch": 0.05575467941059339,
      "grad_norm": 0.5680383443832397,
      "learning_rate": 0.0001989779451317913,
      "loss": 0.8108,
      "step": 70
    },
    {
      "epoch": 0.06371963361210672,
      "grad_norm": 0.5462983250617981,
      "learning_rate": 0.00019844002151694462,
      "loss": 0.8032,
      "step": 80
    },
    {
      "epoch": 0.07168458781362007,
      "grad_norm": 0.5560625791549683,
      "learning_rate": 0.0001979020979020979,
      "loss": 0.7863,
      "step": 90
    },
    {
      "epoch": 0.07964954201513341,
      "grad_norm": 0.5551924705505371,
      "learning_rate": 0.00019736417428725122,
      "loss": 0.7818,
      "step": 100
    },
    {
      "epoch": 0.08761449621664676,
      "grad_norm": 0.5794554948806763,
      "learning_rate": 0.00019682625067240453,
      "loss": 0.7911,
      "step": 110
    },
    {
      "epoch": 0.0955794504181601,
      "grad_norm": 0.6070852875709534,
      "learning_rate": 0.00019628832705755785,
      "loss": 0.7937,
      "step": 120
    },
    {
      "epoch": 0.10354440461967343,
      "grad_norm": 0.5872889161109924,
      "learning_rate": 0.00019575040344271114,
      "loss": 0.7998,
      "step": 130
    },
    {
      "epoch": 0.11150935882118677,
      "grad_norm": 0.5939893126487732,
      "learning_rate": 0.00019521247982786445,
      "loss": 0.7531,
      "step": 140
    },
    {
      "epoch": 0.11947431302270012,
      "grad_norm": 0.5536476969718933,
      "learning_rate": 0.00019467455621301777,
      "loss": 0.7962,
      "step": 150
    },
    {
      "epoch": 0.12743926722421345,
      "grad_norm": 0.5763424634933472,
      "learning_rate": 0.00019413663259817108,
      "loss": 0.7622,
      "step": 160
    },
    {
      "epoch": 0.1354042214257268,
      "grad_norm": 0.5685245990753174,
      "learning_rate": 0.00019359870898332437,
      "loss": 0.7692,
      "step": 170
    },
    {
      "epoch": 0.14336917562724014,
      "grad_norm": 0.5661225318908691,
      "learning_rate": 0.0001930607853684777,
      "loss": 0.7409,
      "step": 180
    },
    {
      "epoch": 0.1513341298287535,
      "grad_norm": 0.5888197422027588,
      "learning_rate": 0.000192522861753631,
      "loss": 0.7779,
      "step": 190
    },
    {
      "epoch": 0.15929908403026682,
      "grad_norm": 0.5563907623291016,
      "learning_rate": 0.00019198493813878432,
      "loss": 0.7681,
      "step": 200
    },
    {
      "epoch": 0.16726403823178015,
      "grad_norm": 0.5455360412597656,
      "learning_rate": 0.0001914470145239376,
      "loss": 0.7614,
      "step": 210
    },
    {
      "epoch": 0.1752289924332935,
      "grad_norm": 0.5019347667694092,
      "learning_rate": 0.00019090909090909092,
      "loss": 0.7718,
      "step": 220
    },
    {
      "epoch": 0.18319394663480684,
      "grad_norm": 0.553898274898529,
      "learning_rate": 0.0001903711672942442,
      "loss": 0.7723,
      "step": 230
    },
    {
      "epoch": 0.1911589008363202,
      "grad_norm": 0.5552647113800049,
      "learning_rate": 0.00018983324367939755,
      "loss": 0.762,
      "step": 240
    },
    {
      "epoch": 0.19912385503783353,
      "grad_norm": 0.5925323367118835,
      "learning_rate": 0.00018929532006455084,
      "loss": 0.7665,
      "step": 250
    },
    {
      "epoch": 0.20708880923934686,
      "grad_norm": 0.4991273283958435,
      "learning_rate": 0.00018875739644970416,
      "loss": 0.799,
      "step": 260
    },
    {
      "epoch": 0.21505376344086022,
      "grad_norm": 0.5987207293510437,
      "learning_rate": 0.00018821947283485744,
      "loss": 0.81,
      "step": 270
    },
    {
      "epoch": 0.22301871764237355,
      "grad_norm": 0.5108638405799866,
      "learning_rate": 0.0001876815492200108,
      "loss": 0.7694,
      "step": 280
    },
    {
      "epoch": 0.2309836718438869,
      "grad_norm": 0.4778127074241638,
      "learning_rate": 0.00018714362560516407,
      "loss": 0.769,
      "step": 290
    },
    {
      "epoch": 0.23894862604540024,
      "grad_norm": 0.49839282035827637,
      "learning_rate": 0.0001866057019903174,
      "loss": 0.7449,
      "step": 300
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 0.5170665383338928,
      "learning_rate": 0.00018606777837547068,
      "loss": 0.7877,
      "step": 310
    },
    {
      "epoch": 0.2548785344484269,
      "grad_norm": 0.5087386965751648,
      "learning_rate": 0.00018552985476062402,
      "loss": 0.7811,
      "step": 320
    },
    {
      "epoch": 0.2628434886499403,
      "grad_norm": 0.5170137882232666,
      "learning_rate": 0.0001849919311457773,
      "loss": 0.8107,
      "step": 330
    },
    {
      "epoch": 0.2708084428514536,
      "grad_norm": 0.5296459794044495,
      "learning_rate": 0.0001844540075309306,
      "loss": 0.7449,
      "step": 340
    },
    {
      "epoch": 0.27877339705296694,
      "grad_norm": 0.490922212600708,
      "learning_rate": 0.0001839160839160839,
      "loss": 0.7663,
      "step": 350
    },
    {
      "epoch": 0.2867383512544803,
      "grad_norm": 0.519218921661377,
      "learning_rate": 0.00018337816030123723,
      "loss": 0.7249,
      "step": 360
    },
    {
      "epoch": 0.2947033054559936,
      "grad_norm": 0.549088716506958,
      "learning_rate": 0.00018284023668639054,
      "loss": 0.7524,
      "step": 370
    },
    {
      "epoch": 0.302668259657507,
      "grad_norm": 0.4940321743488312,
      "learning_rate": 0.00018230231307154383,
      "loss": 0.7297,
      "step": 380
    },
    {
      "epoch": 0.3106332138590203,
      "grad_norm": 0.46705177426338196,
      "learning_rate": 0.00018176438945669715,
      "loss": 0.7748,
      "step": 390
    },
    {
      "epoch": 0.31859816806053365,
      "grad_norm": 0.539527952671051,
      "learning_rate": 0.00018122646584185046,
      "loss": 0.7599,
      "step": 400
    },
    {
      "epoch": 0.326563122262047,
      "grad_norm": 0.5246395468711853,
      "learning_rate": 0.00018068854222700378,
      "loss": 0.7363,
      "step": 410
    },
    {
      "epoch": 0.3345280764635603,
      "grad_norm": 0.5225979685783386,
      "learning_rate": 0.00018015061861215707,
      "loss": 0.779,
      "step": 420
    },
    {
      "epoch": 0.3424930306650737,
      "grad_norm": 0.5296785235404968,
      "learning_rate": 0.00017961269499731038,
      "loss": 0.7487,
      "step": 430
    },
    {
      "epoch": 0.350457984866587,
      "grad_norm": 0.49650877714157104,
      "learning_rate": 0.0001790747713824637,
      "loss": 0.7392,
      "step": 440
    },
    {
      "epoch": 0.35842293906810035,
      "grad_norm": 0.4982154965400696,
      "learning_rate": 0.000178536847767617,
      "loss": 0.7535,
      "step": 450
    },
    {
      "epoch": 0.3663878932696137,
      "grad_norm": 0.5009126663208008,
      "learning_rate": 0.0001779989241527703,
      "loss": 0.7537,
      "step": 460
    },
    {
      "epoch": 0.374352847471127,
      "grad_norm": 0.494118869304657,
      "learning_rate": 0.00017746100053792362,
      "loss": 0.7849,
      "step": 470
    },
    {
      "epoch": 0.3823178016726404,
      "grad_norm": 0.5648855566978455,
      "learning_rate": 0.00017692307692307693,
      "loss": 0.7457,
      "step": 480
    },
    {
      "epoch": 0.39028275587415373,
      "grad_norm": 0.4557546377182007,
      "learning_rate": 0.00017638515330823025,
      "loss": 0.7284,
      "step": 490
    },
    {
      "epoch": 0.39824771007566706,
      "grad_norm": 0.5763673782348633,
      "learning_rate": 0.00017584722969338353,
      "loss": 0.7501,
      "step": 500
    },
    {
      "epoch": 0.4062126642771804,
      "grad_norm": 0.5994834303855896,
      "learning_rate": 0.00017530930607853685,
      "loss": 0.7547,
      "step": 510
    },
    {
      "epoch": 0.4141776184786937,
      "grad_norm": 0.511115312576294,
      "learning_rate": 0.00017477138246369016,
      "loss": 0.7666,
      "step": 520
    },
    {
      "epoch": 0.4221425726802071,
      "grad_norm": 0.4372538924217224,
      "learning_rate": 0.00017423345884884348,
      "loss": 0.744,
      "step": 530
    },
    {
      "epoch": 0.43010752688172044,
      "grad_norm": 0.5074840188026428,
      "learning_rate": 0.00017369553523399677,
      "loss": 0.7809,
      "step": 540
    },
    {
      "epoch": 0.43807248108323377,
      "grad_norm": 0.5417396426200867,
      "learning_rate": 0.00017315761161915008,
      "loss": 0.7411,
      "step": 550
    },
    {
      "epoch": 0.4460374352847471,
      "grad_norm": 0.5052236914634705,
      "learning_rate": 0.0001726196880043034,
      "loss": 0.7448,
      "step": 560
    },
    {
      "epoch": 0.4540023894862604,
      "grad_norm": 0.5071873664855957,
      "learning_rate": 0.00017208176438945671,
      "loss": 0.771,
      "step": 570
    },
    {
      "epoch": 0.4619673436877738,
      "grad_norm": 0.5701971054077148,
      "learning_rate": 0.00017154384077461,
      "loss": 0.765,
      "step": 580
    },
    {
      "epoch": 0.46993229788928714,
      "grad_norm": 0.522875964641571,
      "learning_rate": 0.00017100591715976332,
      "loss": 0.7561,
      "step": 590
    },
    {
      "epoch": 0.4778972520908005,
      "grad_norm": 0.5036452412605286,
      "learning_rate": 0.00017046799354491663,
      "loss": 0.7974,
      "step": 600
    },
    {
      "epoch": 0.4858622062923138,
      "grad_norm": 0.517601728439331,
      "learning_rate": 0.00016993006993006995,
      "loss": 0.7614,
      "step": 610
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 0.5130582451820374,
      "learning_rate": 0.00016939214631522324,
      "loss": 0.7422,
      "step": 620
    },
    {
      "epoch": 0.5017921146953405,
      "grad_norm": 0.46496492624282837,
      "learning_rate": 0.00016885422270037655,
      "loss": 0.7675,
      "step": 630
    },
    {
      "epoch": 0.5097570688968538,
      "grad_norm": 0.46519261598587036,
      "learning_rate": 0.00016831629908552987,
      "loss": 0.7535,
      "step": 640
    },
    {
      "epoch": 0.5177220230983672,
      "grad_norm": 0.4690667390823364,
      "learning_rate": 0.00016777837547068318,
      "loss": 0.7376,
      "step": 650
    },
    {
      "epoch": 0.5256869772998806,
      "grad_norm": 0.47642776370048523,
      "learning_rate": 0.00016724045185583647,
      "loss": 0.7435,
      "step": 660
    },
    {
      "epoch": 0.5336519315013939,
      "grad_norm": 0.4922437071800232,
      "learning_rate": 0.00016670252824098979,
      "loss": 0.7878,
      "step": 670
    },
    {
      "epoch": 0.5416168857029072,
      "grad_norm": 0.427765429019928,
      "learning_rate": 0.0001661646046261431,
      "loss": 0.7383,
      "step": 680
    },
    {
      "epoch": 0.5495818399044206,
      "grad_norm": 0.5159996151924133,
      "learning_rate": 0.00016562668101129642,
      "loss": 0.7415,
      "step": 690
    },
    {
      "epoch": 0.5575467941059339,
      "grad_norm": 0.4753890931606293,
      "learning_rate": 0.0001650887573964497,
      "loss": 0.7647,
      "step": 700
    },
    {
      "epoch": 0.5655117483074472,
      "grad_norm": 0.5033360123634338,
      "learning_rate": 0.00016455083378160302,
      "loss": 0.7617,
      "step": 710
    },
    {
      "epoch": 0.5734767025089605,
      "grad_norm": 0.5544610619544983,
      "learning_rate": 0.00016401291016675634,
      "loss": 0.7655,
      "step": 720
    },
    {
      "epoch": 0.5814416567104739,
      "grad_norm": 0.4957584738731384,
      "learning_rate": 0.00016347498655190962,
      "loss": 0.7706,
      "step": 730
    },
    {
      "epoch": 0.5894066109119872,
      "grad_norm": 0.49697065353393555,
      "learning_rate": 0.00016293706293706294,
      "loss": 0.7611,
      "step": 740
    },
    {
      "epoch": 0.5973715651135006,
      "grad_norm": 0.514756977558136,
      "learning_rate": 0.00016239913932221625,
      "loss": 0.7573,
      "step": 750
    },
    {
      "epoch": 0.605336519315014,
      "grad_norm": 0.5491628646850586,
      "learning_rate": 0.00016186121570736957,
      "loss": 0.786,
      "step": 760
    },
    {
      "epoch": 0.6133014735165273,
      "grad_norm": 0.5565916299819946,
      "learning_rate": 0.00016132329209252286,
      "loss": 0.7502,
      "step": 770
    },
    {
      "epoch": 0.6212664277180406,
      "grad_norm": 0.5392376184463501,
      "learning_rate": 0.00016078536847767617,
      "loss": 0.7493,
      "step": 780
    },
    {
      "epoch": 0.629231381919554,
      "grad_norm": 0.5479769706726074,
      "learning_rate": 0.0001602474448628295,
      "loss": 0.7957,
      "step": 790
    },
    {
      "epoch": 0.6371963361210673,
      "grad_norm": 0.5240273475646973,
      "learning_rate": 0.0001597095212479828,
      "loss": 0.7792,
      "step": 800
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 0.4773210883140564,
      "learning_rate": 0.0001591715976331361,
      "loss": 0.7606,
      "step": 810
    },
    {
      "epoch": 0.653126244524094,
      "grad_norm": 0.44444289803504944,
      "learning_rate": 0.0001586336740182894,
      "loss": 0.7673,
      "step": 820
    },
    {
      "epoch": 0.6610911987256073,
      "grad_norm": 0.5055608153343201,
      "learning_rate": 0.00015809575040344272,
      "loss": 0.77,
      "step": 830
    },
    {
      "epoch": 0.6690561529271206,
      "grad_norm": 0.46055611968040466,
      "learning_rate": 0.00015755782678859604,
      "loss": 0.7545,
      "step": 840
    },
    {
      "epoch": 0.6770211071286341,
      "grad_norm": 0.497326523065567,
      "learning_rate": 0.00015701990317374933,
      "loss": 0.7276,
      "step": 850
    },
    {
      "epoch": 0.6849860613301474,
      "grad_norm": 0.4432331323623657,
      "learning_rate": 0.00015648197955890264,
      "loss": 0.7483,
      "step": 860
    },
    {
      "epoch": 0.6929510155316607,
      "grad_norm": 0.4714212417602539,
      "learning_rate": 0.00015594405594405596,
      "loss": 0.757,
      "step": 870
    },
    {
      "epoch": 0.700915969733174,
      "grad_norm": 0.4514620006084442,
      "learning_rate": 0.00015540613232920927,
      "loss": 0.726,
      "step": 880
    },
    {
      "epoch": 0.7088809239346874,
      "grad_norm": 0.4436425268650055,
      "learning_rate": 0.00015486820871436256,
      "loss": 0.7244,
      "step": 890
    },
    {
      "epoch": 0.7168458781362007,
      "grad_norm": 0.4528445303440094,
      "learning_rate": 0.00015433028509951588,
      "loss": 0.7403,
      "step": 900
    },
    {
      "epoch": 0.724810832337714,
      "grad_norm": 0.4918217957019806,
      "learning_rate": 0.0001537923614846692,
      "loss": 0.7304,
      "step": 910
    },
    {
      "epoch": 0.7327757865392274,
      "grad_norm": 0.4754583239555359,
      "learning_rate": 0.0001532544378698225,
      "loss": 0.7506,
      "step": 920
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.5018563270568848,
      "learning_rate": 0.0001527165142549758,
      "loss": 0.7681,
      "step": 930
    },
    {
      "epoch": 0.748705694942254,
      "grad_norm": 0.49649712443351746,
      "learning_rate": 0.0001521785906401291,
      "loss": 0.7465,
      "step": 940
    },
    {
      "epoch": 0.7566706491437675,
      "grad_norm": 0.4681350290775299,
      "learning_rate": 0.00015164066702528243,
      "loss": 0.7273,
      "step": 950
    },
    {
      "epoch": 0.7646356033452808,
      "grad_norm": 0.4947724938392639,
      "learning_rate": 0.00015110274341043574,
      "loss": 0.7755,
      "step": 960
    },
    {
      "epoch": 0.7726005575467941,
      "grad_norm": 0.4668620526790619,
      "learning_rate": 0.00015056481979558903,
      "loss": 0.7154,
      "step": 970
    },
    {
      "epoch": 0.7805655117483075,
      "grad_norm": 0.4795647859573364,
      "learning_rate": 0.00015002689618074234,
      "loss": 0.7368,
      "step": 980
    },
    {
      "epoch": 0.7885304659498208,
      "grad_norm": 0.4544738829135895,
      "learning_rate": 0.00014948897256589566,
      "loss": 0.7445,
      "step": 990
    },
    {
      "epoch": 0.7964954201513341,
      "grad_norm": 0.4597337543964386,
      "learning_rate": 0.00014895104895104897,
      "loss": 0.772,
      "step": 1000
    },
    {
      "epoch": 0.8044603743528475,
      "grad_norm": 0.4793347120285034,
      "learning_rate": 0.00014841312533620226,
      "loss": 0.7226,
      "step": 1010
    },
    {
      "epoch": 0.8124253285543608,
      "grad_norm": 0.46236521005630493,
      "learning_rate": 0.00014787520172135558,
      "loss": 0.7293,
      "step": 1020
    },
    {
      "epoch": 0.8203902827558741,
      "grad_norm": 0.4980211853981018,
      "learning_rate": 0.0001473372781065089,
      "loss": 0.7556,
      "step": 1030
    },
    {
      "epoch": 0.8283552369573874,
      "grad_norm": 0.45580118894577026,
      "learning_rate": 0.0001467993544916622,
      "loss": 0.7821,
      "step": 1040
    },
    {
      "epoch": 0.8363201911589009,
      "grad_norm": 0.47210371494293213,
      "learning_rate": 0.0001462614308768155,
      "loss": 0.7815,
      "step": 1050
    },
    {
      "epoch": 0.8442851453604142,
      "grad_norm": 0.47734859585762024,
      "learning_rate": 0.0001457235072619688,
      "loss": 0.7624,
      "step": 1060
    },
    {
      "epoch": 0.8522500995619275,
      "grad_norm": 0.45166775584220886,
      "learning_rate": 0.0001451855836471221,
      "loss": 0.7601,
      "step": 1070
    },
    {
      "epoch": 0.8602150537634409,
      "grad_norm": 0.4564398527145386,
      "learning_rate": 0.00014464766003227544,
      "loss": 0.7276,
      "step": 1080
    },
    {
      "epoch": 0.8681800079649542,
      "grad_norm": 0.4579167366027832,
      "learning_rate": 0.00014410973641742873,
      "loss": 0.7516,
      "step": 1090
    },
    {
      "epoch": 0.8761449621664675,
      "grad_norm": 0.45872241258621216,
      "learning_rate": 0.00014357181280258205,
      "loss": 0.7868,
      "step": 1100
    },
    {
      "epoch": 0.8841099163679809,
      "grad_norm": 0.549182116985321,
      "learning_rate": 0.00014303388918773533,
      "loss": 0.7316,
      "step": 1110
    },
    {
      "epoch": 0.8920748705694942,
      "grad_norm": 0.5116949081420898,
      "learning_rate": 0.00014249596557288865,
      "loss": 0.7851,
      "step": 1120
    },
    {
      "epoch": 0.9000398247710075,
      "grad_norm": 0.45142361521720886,
      "learning_rate": 0.00014195804195804197,
      "loss": 0.757,
      "step": 1130
    },
    {
      "epoch": 0.9080047789725209,
      "grad_norm": 0.47317203879356384,
      "learning_rate": 0.00014142011834319525,
      "loss": 0.7527,
      "step": 1140
    },
    {
      "epoch": 0.9159697331740343,
      "grad_norm": 0.45667991042137146,
      "learning_rate": 0.00014088219472834857,
      "loss": 0.7343,
      "step": 1150
    },
    {
      "epoch": 0.9239346873755476,
      "grad_norm": 0.4944150149822235,
      "learning_rate": 0.00014034427111350188,
      "loss": 0.7533,
      "step": 1160
    },
    {
      "epoch": 0.931899641577061,
      "grad_norm": 0.4322386682033539,
      "learning_rate": 0.0001398063474986552,
      "loss": 0.7407,
      "step": 1170
    },
    {
      "epoch": 0.9398645957785743,
      "grad_norm": 0.4452013671398163,
      "learning_rate": 0.0001392684238838085,
      "loss": 0.7574,
      "step": 1180
    },
    {
      "epoch": 0.9478295499800876,
      "grad_norm": 0.519768476486206,
      "learning_rate": 0.0001387305002689618,
      "loss": 0.7405,
      "step": 1190
    },
    {
      "epoch": 0.955794504181601,
      "grad_norm": 0.4438643455505371,
      "learning_rate": 0.00013819257665411512,
      "loss": 0.7489,
      "step": 1200
    },
    {
      "epoch": 0.9637594583831143,
      "grad_norm": 0.46488696336746216,
      "learning_rate": 0.00013765465303926843,
      "loss": 0.7641,
      "step": 1210
    },
    {
      "epoch": 0.9717244125846276,
      "grad_norm": 0.4657285511493683,
      "learning_rate": 0.00013711672942442172,
      "loss": 0.7335,
      "step": 1220
    },
    {
      "epoch": 0.9796893667861409,
      "grad_norm": 0.5356078147888184,
      "learning_rate": 0.00013657880580957504,
      "loss": 0.7374,
      "step": 1230
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 0.5349792838096619,
      "learning_rate": 0.00013604088219472835,
      "loss": 0.7201,
      "step": 1240
    },
    {
      "epoch": 0.9956192751891677,
      "grad_norm": 0.45736873149871826,
      "learning_rate": 0.00013550295857988167,
      "loss": 0.7562,
      "step": 1250
    },
    {
      "epoch": 1.0031859816806052,
      "grad_norm": 0.4625971019268036,
      "learning_rate": 0.00013496503496503496,
      "loss": 0.7673,
      "step": 1260
    },
    {
      "epoch": 1.0111509358821187,
      "grad_norm": 0.4288552701473236,
      "learning_rate": 0.00013442711135018827,
      "loss": 0.7597,
      "step": 1270
    },
    {
      "epoch": 1.0191158900836321,
      "grad_norm": 0.4733198285102844,
      "learning_rate": 0.0001338891877353416,
      "loss": 0.7344,
      "step": 1280
    },
    {
      "epoch": 1.0270808442851453,
      "grad_norm": 0.4954827129840851,
      "learning_rate": 0.0001333512641204949,
      "loss": 0.7387,
      "step": 1290
    },
    {
      "epoch": 1.0350457984866588,
      "grad_norm": 0.520415723323822,
      "learning_rate": 0.0001328133405056482,
      "loss": 0.7091,
      "step": 1300
    },
    {
      "epoch": 1.043010752688172,
      "grad_norm": 0.4539541006088257,
      "learning_rate": 0.0001322754168908015,
      "loss": 0.7184,
      "step": 1310
    },
    {
      "epoch": 1.0509757068896854,
      "grad_norm": 0.4909593462944031,
      "learning_rate": 0.00013173749327595482,
      "loss": 0.7257,
      "step": 1320
    },
    {
      "epoch": 1.0589406610911987,
      "grad_norm": 0.44648057222366333,
      "learning_rate": 0.00013119956966110814,
      "loss": 0.7291,
      "step": 1330
    },
    {
      "epoch": 1.066905615292712,
      "grad_norm": 0.5058598518371582,
      "learning_rate": 0.00013066164604626142,
      "loss": 0.7511,
      "step": 1340
    },
    {
      "epoch": 1.0748705694942253,
      "grad_norm": 0.5058706998825073,
      "learning_rate": 0.00013012372243141474,
      "loss": 0.6728,
      "step": 1350
    },
    {
      "epoch": 1.0828355236957388,
      "grad_norm": 0.5570462942123413,
      "learning_rate": 0.00012958579881656806,
      "loss": 0.7404,
      "step": 1360
    },
    {
      "epoch": 1.0908004778972522,
      "grad_norm": 0.5753740072250366,
      "learning_rate": 0.00012904787520172137,
      "loss": 0.7354,
      "step": 1370
    },
    {
      "epoch": 1.0987654320987654,
      "grad_norm": 0.5565868020057678,
      "learning_rate": 0.00012850995158687466,
      "loss": 0.7111,
      "step": 1380
    },
    {
      "epoch": 1.1067303863002789,
      "grad_norm": 0.528270423412323,
      "learning_rate": 0.00012797202797202797,
      "loss": 0.7916,
      "step": 1390
    },
    {
      "epoch": 1.114695340501792,
      "grad_norm": 0.5695818066596985,
      "learning_rate": 0.0001274341043571813,
      "loss": 0.7574,
      "step": 1400
    },
    {
      "epoch": 1.1226602947033055,
      "grad_norm": 0.47674667835235596,
      "learning_rate": 0.0001268961807423346,
      "loss": 0.7077,
      "step": 1410
    },
    {
      "epoch": 1.1306252489048187,
      "grad_norm": 0.5483880639076233,
      "learning_rate": 0.0001263582571274879,
      "loss": 0.7432,
      "step": 1420
    },
    {
      "epoch": 1.1385902031063322,
      "grad_norm": 0.4990485608577728,
      "learning_rate": 0.0001258203335126412,
      "loss": 0.7255,
      "step": 1430
    },
    {
      "epoch": 1.1465551573078454,
      "grad_norm": 0.46087712049484253,
      "learning_rate": 0.00012528240989779452,
      "loss": 0.7268,
      "step": 1440
    },
    {
      "epoch": 1.1545201115093588,
      "grad_norm": 0.47685572504997253,
      "learning_rate": 0.00012474448628294784,
      "loss": 0.6883,
      "step": 1450
    },
    {
      "epoch": 1.162485065710872,
      "grad_norm": 0.5443675518035889,
      "learning_rate": 0.00012420656266810113,
      "loss": 0.7317,
      "step": 1460
    },
    {
      "epoch": 1.1704500199123855,
      "grad_norm": 0.4945436418056488,
      "learning_rate": 0.00012366863905325444,
      "loss": 0.7094,
      "step": 1470
    },
    {
      "epoch": 1.178414974113899,
      "grad_norm": 0.4786890745162964,
      "learning_rate": 0.00012313071543840776,
      "loss": 0.7156,
      "step": 1480
    },
    {
      "epoch": 1.1863799283154122,
      "grad_norm": 0.5114238262176514,
      "learning_rate": 0.00012259279182356107,
      "loss": 0.7351,
      "step": 1490
    },
    {
      "epoch": 1.1943448825169256,
      "grad_norm": 0.4729917347431183,
      "learning_rate": 0.00012205486820871436,
      "loss": 0.7134,
      "step": 1500
    },
    {
      "epoch": 1.2023098367184388,
      "grad_norm": 0.4813857972621918,
      "learning_rate": 0.00012151694459386769,
      "loss": 0.7245,
      "step": 1510
    },
    {
      "epoch": 1.2102747909199523,
      "grad_norm": 0.5106756687164307,
      "learning_rate": 0.00012097902097902098,
      "loss": 0.718,
      "step": 1520
    },
    {
      "epoch": 1.2182397451214655,
      "grad_norm": 0.5027218461036682,
      "learning_rate": 0.00012044109736417428,
      "loss": 0.7467,
      "step": 1530
    },
    {
      "epoch": 1.226204699322979,
      "grad_norm": 0.5211023688316345,
      "learning_rate": 0.0001199031737493276,
      "loss": 0.6958,
      "step": 1540
    },
    {
      "epoch": 1.2341696535244921,
      "grad_norm": 0.5364963412284851,
      "learning_rate": 0.0001193652501344809,
      "loss": 0.7184,
      "step": 1550
    },
    {
      "epoch": 1.2421346077260056,
      "grad_norm": 0.48435521125793457,
      "learning_rate": 0.00011882732651963421,
      "loss": 0.7349,
      "step": 1560
    },
    {
      "epoch": 1.2500995619275188,
      "grad_norm": 0.5540192127227783,
      "learning_rate": 0.00011828940290478751,
      "loss": 0.7384,
      "step": 1570
    },
    {
      "epoch": 1.2580645161290323,
      "grad_norm": 0.4760480523109436,
      "learning_rate": 0.00011775147928994083,
      "loss": 0.7265,
      "step": 1580
    },
    {
      "epoch": 1.2660294703305457,
      "grad_norm": 0.4919531047344208,
      "learning_rate": 0.00011721355567509413,
      "loss": 0.7095,
      "step": 1590
    },
    {
      "epoch": 1.273994424532059,
      "grad_norm": 0.5661686062812805,
      "learning_rate": 0.00011667563206024745,
      "loss": 0.6682,
      "step": 1600
    },
    {
      "epoch": 1.2819593787335724,
      "grad_norm": 0.507229745388031,
      "learning_rate": 0.00011613770844540075,
      "loss": 0.7645,
      "step": 1610
    },
    {
      "epoch": 1.2899243329350856,
      "grad_norm": 0.5442198514938354,
      "learning_rate": 0.00011559978483055406,
      "loss": 0.7404,
      "step": 1620
    },
    {
      "epoch": 1.297889287136599,
      "grad_norm": 0.5415163040161133,
      "learning_rate": 0.00011506186121570737,
      "loss": 0.7218,
      "step": 1630
    },
    {
      "epoch": 1.3058542413381122,
      "grad_norm": 0.45485225319862366,
      "learning_rate": 0.00011452393760086068,
      "loss": 0.7137,
      "step": 1640
    },
    {
      "epoch": 1.3138191955396257,
      "grad_norm": 0.5133085250854492,
      "learning_rate": 0.00011398601398601398,
      "loss": 0.7502,
      "step": 1650
    },
    {
      "epoch": 1.3217841497411391,
      "grad_norm": 0.4626544117927551,
      "learning_rate": 0.0001134480903711673,
      "loss": 0.7129,
      "step": 1660
    },
    {
      "epoch": 1.3297491039426523,
      "grad_norm": 0.5139325857162476,
      "learning_rate": 0.0001129101667563206,
      "loss": 0.7039,
      "step": 1670
    },
    {
      "epoch": 1.3377140581441656,
      "grad_norm": 0.4753801226615906,
      "learning_rate": 0.00011237224314147392,
      "loss": 0.7095,
      "step": 1680
    },
    {
      "epoch": 1.345679012345679,
      "grad_norm": 0.49914097785949707,
      "learning_rate": 0.00011183431952662722,
      "loss": 0.7196,
      "step": 1690
    },
    {
      "epoch": 1.3536439665471924,
      "grad_norm": 0.47906267642974854,
      "learning_rate": 0.00011129639591178053,
      "loss": 0.7155,
      "step": 1700
    },
    {
      "epoch": 1.3616089207487057,
      "grad_norm": 0.5311673283576965,
      "learning_rate": 0.00011075847229693383,
      "loss": 0.7377,
      "step": 1710
    },
    {
      "epoch": 1.369573874950219,
      "grad_norm": 0.4906010627746582,
      "learning_rate": 0.00011022054868208715,
      "loss": 0.7172,
      "step": 1720
    },
    {
      "epoch": 1.3775388291517323,
      "grad_norm": 0.5275512933731079,
      "learning_rate": 0.00010968262506724045,
      "loss": 0.7382,
      "step": 1730
    },
    {
      "epoch": 1.3855037833532458,
      "grad_norm": 0.47829827666282654,
      "learning_rate": 0.00010914470145239377,
      "loss": 0.719,
      "step": 1740
    },
    {
      "epoch": 1.393468737554759,
      "grad_norm": 0.5060559511184692,
      "learning_rate": 0.00010860677783754707,
      "loss": 0.725,
      "step": 1750
    },
    {
      "epoch": 1.4014336917562724,
      "grad_norm": 0.5723638534545898,
      "learning_rate": 0.00010806885422270038,
      "loss": 0.7258,
      "step": 1760
    },
    {
      "epoch": 1.4093986459577859,
      "grad_norm": 0.5221225023269653,
      "learning_rate": 0.00010753093060785369,
      "loss": 0.7284,
      "step": 1770
    },
    {
      "epoch": 1.417363600159299,
      "grad_norm": 0.5279989242553711,
      "learning_rate": 0.000106993006993007,
      "loss": 0.7477,
      "step": 1780
    },
    {
      "epoch": 1.4253285543608123,
      "grad_norm": 0.5157337784767151,
      "learning_rate": 0.0001064550833781603,
      "loss": 0.7225,
      "step": 1790
    },
    {
      "epoch": 1.4332935085623257,
      "grad_norm": 0.5328108072280884,
      "learning_rate": 0.00010591715976331362,
      "loss": 0.7112,
      "step": 1800
    },
    {
      "epoch": 1.4412584627638392,
      "grad_norm": 0.4593004584312439,
      "learning_rate": 0.00010537923614846692,
      "loss": 0.7255,
      "step": 1810
    },
    {
      "epoch": 1.4492234169653524,
      "grad_norm": 0.51335608959198,
      "learning_rate": 0.00010484131253362023,
      "loss": 0.7232,
      "step": 1820
    },
    {
      "epoch": 1.4571883711668658,
      "grad_norm": 0.508044958114624,
      "learning_rate": 0.00010430338891877354,
      "loss": 0.7449,
      "step": 1830
    },
    {
      "epoch": 1.465153325368379,
      "grad_norm": 0.5165323615074158,
      "learning_rate": 0.00010376546530392685,
      "loss": 0.7224,
      "step": 1840
    },
    {
      "epoch": 1.4731182795698925,
      "grad_norm": 0.48717552423477173,
      "learning_rate": 0.00010322754168908015,
      "loss": 0.71,
      "step": 1850
    },
    {
      "epoch": 1.4810832337714057,
      "grad_norm": 0.5158607363700867,
      "learning_rate": 0.00010268961807423347,
      "loss": 0.7436,
      "step": 1860
    },
    {
      "epoch": 1.4890481879729192,
      "grad_norm": 0.5024345517158508,
      "learning_rate": 0.00010215169445938677,
      "loss": 0.7244,
      "step": 1870
    },
    {
      "epoch": 1.4970131421744326,
      "grad_norm": 0.5138691067695618,
      "learning_rate": 0.00010161377084454009,
      "loss": 0.6842,
      "step": 1880
    },
    {
      "epoch": 1.5049780963759458,
      "grad_norm": 0.5467655658721924,
      "learning_rate": 0.00010107584722969339,
      "loss": 0.723,
      "step": 1890
    },
    {
      "epoch": 1.512943050577459,
      "grad_norm": 0.4740504324436188,
      "learning_rate": 0.0001005379236148467,
      "loss": 0.708,
      "step": 1900
    },
    {
      "epoch": 1.5209080047789725,
      "grad_norm": 0.5175864696502686,
      "learning_rate": 0.0001,
      "loss": 0.7196,
      "step": 1910
    },
    {
      "epoch": 1.528872958980486,
      "grad_norm": 0.4771402180194855,
      "learning_rate": 9.94620763851533e-05,
      "loss": 0.7151,
      "step": 1920
    },
    {
      "epoch": 1.5368379131819991,
      "grad_norm": 0.5163642764091492,
      "learning_rate": 9.892415277030662e-05,
      "loss": 0.7353,
      "step": 1930
    },
    {
      "epoch": 1.5448028673835126,
      "grad_norm": 0.555423378944397,
      "learning_rate": 9.838622915545992e-05,
      "loss": 0.7287,
      "step": 1940
    },
    {
      "epoch": 1.552767821585026,
      "grad_norm": 0.5214229822158813,
      "learning_rate": 9.784830554061324e-05,
      "loss": 0.7151,
      "step": 1950
    },
    {
      "epoch": 1.5607327757865392,
      "grad_norm": 0.5850131511688232,
      "learning_rate": 9.731038192576654e-05,
      "loss": 0.7686,
      "step": 1960
    },
    {
      "epoch": 1.5686977299880525,
      "grad_norm": 0.5416763424873352,
      "learning_rate": 9.677245831091986e-05,
      "loss": 0.7532,
      "step": 1970
    },
    {
      "epoch": 1.576662684189566,
      "grad_norm": 0.5511633157730103,
      "learning_rate": 9.623453469607316e-05,
      "loss": 0.728,
      "step": 1980
    },
    {
      "epoch": 1.5846276383910793,
      "grad_norm": 0.5364885330200195,
      "learning_rate": 9.569661108122647e-05,
      "loss": 0.783,
      "step": 1990
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 0.4735919237136841,
      "learning_rate": 9.515868746637977e-05,
      "loss": 0.6912,
      "step": 2000
    },
    {
      "epoch": 1.6005575467941058,
      "grad_norm": 0.5241736173629761,
      "learning_rate": 9.462076385153309e-05,
      "loss": 0.7267,
      "step": 2010
    },
    {
      "epoch": 1.6085225009956192,
      "grad_norm": 0.4514029324054718,
      "learning_rate": 9.408284023668639e-05,
      "loss": 0.7321,
      "step": 2020
    },
    {
      "epoch": 1.6164874551971327,
      "grad_norm": 0.5544889569282532,
      "learning_rate": 9.354491662183971e-05,
      "loss": 0.7516,
      "step": 2030
    },
    {
      "epoch": 1.6244524093986459,
      "grad_norm": 0.5061807632446289,
      "learning_rate": 9.300699300699301e-05,
      "loss": 0.7139,
      "step": 2040
    },
    {
      "epoch": 1.6324173636001593,
      "grad_norm": 0.487118124961853,
      "learning_rate": 9.246906939214632e-05,
      "loss": 0.7307,
      "step": 2050
    },
    {
      "epoch": 1.6403823178016728,
      "grad_norm": 0.5057170987129211,
      "learning_rate": 9.193114577729963e-05,
      "loss": 0.7021,
      "step": 2060
    },
    {
      "epoch": 1.648347272003186,
      "grad_norm": 0.5394274592399597,
      "learning_rate": 9.139322216245294e-05,
      "loss": 0.7319,
      "step": 2070
    },
    {
      "epoch": 1.6563122262046992,
      "grad_norm": 0.5435377359390259,
      "learning_rate": 9.085529854760624e-05,
      "loss": 0.7404,
      "step": 2080
    },
    {
      "epoch": 1.6642771804062126,
      "grad_norm": 0.44337692856788635,
      "learning_rate": 9.031737493275956e-05,
      "loss": 0.6873,
      "step": 2090
    },
    {
      "epoch": 1.672242134607726,
      "grad_norm": 0.5030319094657898,
      "learning_rate": 8.977945131791286e-05,
      "loss": 0.7466,
      "step": 2100
    },
    {
      "epoch": 1.6802070888092393,
      "grad_norm": 0.5560398101806641,
      "learning_rate": 8.924152770306616e-05,
      "loss": 0.7392,
      "step": 2110
    },
    {
      "epoch": 1.6881720430107527,
      "grad_norm": 0.46371927857398987,
      "learning_rate": 8.870360408821948e-05,
      "loss": 0.7301,
      "step": 2120
    },
    {
      "epoch": 1.6961369972122662,
      "grad_norm": 0.48637527227401733,
      "learning_rate": 8.816568047337278e-05,
      "loss": 0.7395,
      "step": 2130
    },
    {
      "epoch": 1.7041019514137794,
      "grad_norm": 0.5033164024353027,
      "learning_rate": 8.76277568585261e-05,
      "loss": 0.687,
      "step": 2140
    },
    {
      "epoch": 1.7120669056152926,
      "grad_norm": 0.5034162402153015,
      "learning_rate": 8.70898332436794e-05,
      "loss": 0.7131,
      "step": 2150
    },
    {
      "epoch": 1.720031859816806,
      "grad_norm": 0.5034641623497009,
      "learning_rate": 8.655190962883271e-05,
      "loss": 0.7267,
      "step": 2160
    },
    {
      "epoch": 1.7279968140183195,
      "grad_norm": 0.5509538650512695,
      "learning_rate": 8.601398601398601e-05,
      "loss": 0.7008,
      "step": 2170
    },
    {
      "epoch": 1.7359617682198327,
      "grad_norm": 0.5422916412353516,
      "learning_rate": 8.547606239913933e-05,
      "loss": 0.758,
      "step": 2180
    },
    {
      "epoch": 1.743926722421346,
      "grad_norm": 0.47149571776390076,
      "learning_rate": 8.493813878429263e-05,
      "loss": 0.7125,
      "step": 2190
    },
    {
      "epoch": 1.7518916766228594,
      "grad_norm": 0.5824500918388367,
      "learning_rate": 8.440021516944595e-05,
      "loss": 0.73,
      "step": 2200
    },
    {
      "epoch": 1.7598566308243728,
      "grad_norm": 0.49923044443130493,
      "learning_rate": 8.386229155459925e-05,
      "loss": 0.7182,
      "step": 2210
    },
    {
      "epoch": 1.767821585025886,
      "grad_norm": 0.5198145508766174,
      "learning_rate": 8.332436793975256e-05,
      "loss": 0.7351,
      "step": 2220
    },
    {
      "epoch": 1.7757865392273995,
      "grad_norm": 0.5441893339157104,
      "learning_rate": 8.278644432490586e-05,
      "loss": 0.718,
      "step": 2230
    },
    {
      "epoch": 1.783751493428913,
      "grad_norm": 0.4886374771595001,
      "learning_rate": 8.224852071005918e-05,
      "loss": 0.7032,
      "step": 2240
    },
    {
      "epoch": 1.7917164476304261,
      "grad_norm": 0.4992038309574127,
      "learning_rate": 8.171059709521248e-05,
      "loss": 0.7397,
      "step": 2250
    },
    {
      "epoch": 1.7996814018319394,
      "grad_norm": 0.4685755670070648,
      "learning_rate": 8.11726734803658e-05,
      "loss": 0.7272,
      "step": 2260
    },
    {
      "epoch": 1.8076463560334528,
      "grad_norm": 0.5154479742050171,
      "learning_rate": 8.06347498655191e-05,
      "loss": 0.7594,
      "step": 2270
    },
    {
      "epoch": 1.8156113102349662,
      "grad_norm": 0.47591331601142883,
      "learning_rate": 8.009682625067241e-05,
      "loss": 0.725,
      "step": 2280
    },
    {
      "epoch": 1.8235762644364795,
      "grad_norm": 0.5525245070457458,
      "learning_rate": 7.955890263582572e-05,
      "loss": 0.7281,
      "step": 2290
    },
    {
      "epoch": 1.8315412186379927,
      "grad_norm": 0.5060536861419678,
      "learning_rate": 7.902097902097903e-05,
      "loss": 0.7478,
      "step": 2300
    },
    {
      "epoch": 1.8395061728395061,
      "grad_norm": 0.4930019676685333,
      "learning_rate": 7.848305540613233e-05,
      "loss": 0.7522,
      "step": 2310
    },
    {
      "epoch": 1.8474711270410196,
      "grad_norm": 0.5212144255638123,
      "learning_rate": 7.794513179128565e-05,
      "loss": 0.7217,
      "step": 2320
    },
    {
      "epoch": 1.8554360812425328,
      "grad_norm": 0.48041462898254395,
      "learning_rate": 7.740720817643895e-05,
      "loss": 0.737,
      "step": 2330
    },
    {
      "epoch": 1.8634010354440462,
      "grad_norm": 0.49279695749282837,
      "learning_rate": 7.686928456159225e-05,
      "loss": 0.7308,
      "step": 2340
    },
    {
      "epoch": 1.8713659896455597,
      "grad_norm": 0.5309919714927673,
      "learning_rate": 7.633136094674557e-05,
      "loss": 0.6768,
      "step": 2350
    },
    {
      "epoch": 1.8793309438470729,
      "grad_norm": 0.5160070657730103,
      "learning_rate": 7.579343733189887e-05,
      "loss": 0.6959,
      "step": 2360
    },
    {
      "epoch": 1.887295898048586,
      "grad_norm": 0.47561001777648926,
      "learning_rate": 7.525551371705218e-05,
      "loss": 0.7374,
      "step": 2370
    },
    {
      "epoch": 1.8952608522500995,
      "grad_norm": 0.5049702525138855,
      "learning_rate": 7.471759010220549e-05,
      "loss": 0.6987,
      "step": 2380
    },
    {
      "epoch": 1.903225806451613,
      "grad_norm": 0.5691946148872375,
      "learning_rate": 7.41796664873588e-05,
      "loss": 0.7264,
      "step": 2390
    },
    {
      "epoch": 1.9111907606531262,
      "grad_norm": 0.5225602388381958,
      "learning_rate": 7.36417428725121e-05,
      "loss": 0.7128,
      "step": 2400
    },
    {
      "epoch": 1.9191557148546394,
      "grad_norm": 0.5281886458396912,
      "learning_rate": 7.310381925766542e-05,
      "loss": 0.7262,
      "step": 2410
    },
    {
      "epoch": 1.927120669056153,
      "grad_norm": 0.601759672164917,
      "learning_rate": 7.256589564281872e-05,
      "loss": 0.7302,
      "step": 2420
    },
    {
      "epoch": 1.9350856232576663,
      "grad_norm": 0.5215563774108887,
      "learning_rate": 7.202797202797204e-05,
      "loss": 0.7604,
      "step": 2430
    },
    {
      "epoch": 1.9430505774591795,
      "grad_norm": 0.5226988196372986,
      "learning_rate": 7.149004841312534e-05,
      "loss": 0.707,
      "step": 2440
    },
    {
      "epoch": 1.951015531660693,
      "grad_norm": 0.5164101719856262,
      "learning_rate": 7.095212479827865e-05,
      "loss": 0.7029,
      "step": 2450
    },
    {
      "epoch": 1.9589804858622064,
      "grad_norm": 0.5570307970046997,
      "learning_rate": 7.041420118343195e-05,
      "loss": 0.7354,
      "step": 2460
    },
    {
      "epoch": 1.9669454400637196,
      "grad_norm": 0.5072523355484009,
      "learning_rate": 6.987627756858527e-05,
      "loss": 0.724,
      "step": 2470
    },
    {
      "epoch": 1.9749103942652328,
      "grad_norm": 0.534125030040741,
      "learning_rate": 6.933835395373857e-05,
      "loss": 0.7134,
      "step": 2480
    },
    {
      "epoch": 1.9828753484667463,
      "grad_norm": 0.48700380325317383,
      "learning_rate": 6.880043033889189e-05,
      "loss": 0.7018,
      "step": 2490
    },
    {
      "epoch": 1.9908403026682597,
      "grad_norm": 0.545971155166626,
      "learning_rate": 6.826250672404519e-05,
      "loss": 0.7039,
      "step": 2500
    },
    {
      "epoch": 1.998805256869773,
      "grad_norm": 0.5028773546218872,
      "learning_rate": 6.772458310919849e-05,
      "loss": 0.7293,
      "step": 2510
    },
    {
      "epoch": 2.0063719633612105,
      "grad_norm": 0.5146816372871399,
      "learning_rate": 6.71866594943518e-05,
      "loss": 0.6957,
      "step": 2520
    },
    {
      "epoch": 2.014336917562724,
      "grad_norm": 0.4964713156223297,
      "learning_rate": 6.664873587950511e-05,
      "loss": 0.6806,
      "step": 2530
    },
    {
      "epoch": 2.0223018717642374,
      "grad_norm": 0.5546703934669495,
      "learning_rate": 6.611081226465842e-05,
      "loss": 0.7268,
      "step": 2540
    },
    {
      "epoch": 2.0302668259657506,
      "grad_norm": 0.5097230076789856,
      "learning_rate": 6.557288864981172e-05,
      "loss": 0.7015,
      "step": 2550
    },
    {
      "epoch": 2.0382317801672643,
      "grad_norm": 0.5684093832969666,
      "learning_rate": 6.503496503496504e-05,
      "loss": 0.695,
      "step": 2560
    },
    {
      "epoch": 2.0461967343687775,
      "grad_norm": 0.539786696434021,
      "learning_rate": 6.449704142011834e-05,
      "loss": 0.6886,
      "step": 2570
    },
    {
      "epoch": 2.0541616885702907,
      "grad_norm": 0.5099940299987793,
      "learning_rate": 6.395911780527166e-05,
      "loss": 0.6747,
      "step": 2580
    },
    {
      "epoch": 2.062126642771804,
      "grad_norm": 0.48336535692214966,
      "learning_rate": 6.342119419042496e-05,
      "loss": 0.6695,
      "step": 2590
    },
    {
      "epoch": 2.0700915969733176,
      "grad_norm": 0.5233086943626404,
      "learning_rate": 6.288327057557827e-05,
      "loss": 0.6749,
      "step": 2600
    },
    {
      "epoch": 2.078056551174831,
      "grad_norm": 0.45826295018196106,
      "learning_rate": 6.234534696073158e-05,
      "loss": 0.6805,
      "step": 2610
    },
    {
      "epoch": 2.086021505376344,
      "grad_norm": 0.5217341184616089,
      "learning_rate": 6.180742334588489e-05,
      "loss": 0.6958,
      "step": 2620
    },
    {
      "epoch": 2.0939864595778572,
      "grad_norm": 0.5156152844429016,
      "learning_rate": 6.126949973103819e-05,
      "loss": 0.7326,
      "step": 2630
    },
    {
      "epoch": 2.101951413779371,
      "grad_norm": 0.5481261014938354,
      "learning_rate": 6.07315761161915e-05,
      "loss": 0.7048,
      "step": 2640
    },
    {
      "epoch": 2.109916367980884,
      "grad_norm": 0.5459609031677246,
      "learning_rate": 6.019365250134481e-05,
      "loss": 0.7295,
      "step": 2650
    },
    {
      "epoch": 2.1178813221823973,
      "grad_norm": 0.4921671152114868,
      "learning_rate": 5.965572888649812e-05,
      "loss": 0.6964,
      "step": 2660
    },
    {
      "epoch": 2.125846276383911,
      "grad_norm": 0.5091741681098938,
      "learning_rate": 5.911780527165143e-05,
      "loss": 0.6805,
      "step": 2670
    },
    {
      "epoch": 2.133811230585424,
      "grad_norm": 0.5775667428970337,
      "learning_rate": 5.8579881656804736e-05,
      "loss": 0.7226,
      "step": 2680
    },
    {
      "epoch": 2.1417761847869374,
      "grad_norm": 0.5007416605949402,
      "learning_rate": 5.8041958041958044e-05,
      "loss": 0.7124,
      "step": 2690
    },
    {
      "epoch": 2.1497411389884507,
      "grad_norm": 0.540289580821991,
      "learning_rate": 5.750403442711135e-05,
      "loss": 0.7001,
      "step": 2700
    },
    {
      "epoch": 2.1577060931899643,
      "grad_norm": 0.6314790844917297,
      "learning_rate": 5.696611081226466e-05,
      "loss": 0.7259,
      "step": 2710
    },
    {
      "epoch": 2.1656710473914775,
      "grad_norm": 0.5589468479156494,
      "learning_rate": 5.642818719741797e-05,
      "loss": 0.6977,
      "step": 2720
    },
    {
      "epoch": 2.1736360015929908,
      "grad_norm": 0.5088843107223511,
      "learning_rate": 5.589026358257128e-05,
      "loss": 0.6877,
      "step": 2730
    },
    {
      "epoch": 2.1816009557945044,
      "grad_norm": 0.5505317449569702,
      "learning_rate": 5.535233996772459e-05,
      "loss": 0.6827,
      "step": 2740
    },
    {
      "epoch": 2.1895659099960176,
      "grad_norm": 0.5908107757568359,
      "learning_rate": 5.4814416352877895e-05,
      "loss": 0.6968,
      "step": 2750
    },
    {
      "epoch": 2.197530864197531,
      "grad_norm": 0.5801724791526794,
      "learning_rate": 5.4276492738031204e-05,
      "loss": 0.7091,
      "step": 2760
    },
    {
      "epoch": 2.205495818399044,
      "grad_norm": 0.5325687527656555,
      "learning_rate": 5.373856912318451e-05,
      "loss": 0.7052,
      "step": 2770
    },
    {
      "epoch": 2.2134607726005577,
      "grad_norm": 0.4998343586921692,
      "learning_rate": 5.320064550833782e-05,
      "loss": 0.7067,
      "step": 2780
    },
    {
      "epoch": 2.221425726802071,
      "grad_norm": 0.5750508308410645,
      "learning_rate": 5.266272189349113e-05,
      "loss": 0.7321,
      "step": 2790
    },
    {
      "epoch": 2.229390681003584,
      "grad_norm": 0.5719403028488159,
      "learning_rate": 5.212479827864444e-05,
      "loss": 0.7073,
      "step": 2800
    },
    {
      "epoch": 2.2373556352050974,
      "grad_norm": 0.8225753307342529,
      "learning_rate": 5.1586874663797747e-05,
      "loss": 0.7315,
      "step": 2810
    },
    {
      "epoch": 2.245320589406611,
      "grad_norm": 0.5344606637954712,
      "learning_rate": 5.1048951048951055e-05,
      "loss": 0.6946,
      "step": 2820
    },
    {
      "epoch": 2.2532855436081243,
      "grad_norm": 0.5052036643028259,
      "learning_rate": 5.0511027434104364e-05,
      "loss": 0.7199,
      "step": 2830
    },
    {
      "epoch": 2.2612504978096375,
      "grad_norm": 0.5796772241592407,
      "learning_rate": 4.9973103819257665e-05,
      "loss": 0.7023,
      "step": 2840
    },
    {
      "epoch": 2.269215452011151,
      "grad_norm": 0.5326991677284241,
      "learning_rate": 4.9435180204410974e-05,
      "loss": 0.7012,
      "step": 2850
    },
    {
      "epoch": 2.2771804062126644,
      "grad_norm": 0.6012923717498779,
      "learning_rate": 4.889725658956428e-05,
      "loss": 0.7367,
      "step": 2860
    },
    {
      "epoch": 2.2851453604141776,
      "grad_norm": 0.5279554128646851,
      "learning_rate": 4.835933297471759e-05,
      "loss": 0.7034,
      "step": 2870
    },
    {
      "epoch": 2.293110314615691,
      "grad_norm": 0.5787526369094849,
      "learning_rate": 4.78214093598709e-05,
      "loss": 0.718,
      "step": 2880
    },
    {
      "epoch": 2.3010752688172045,
      "grad_norm": 0.545978844165802,
      "learning_rate": 4.728348574502421e-05,
      "loss": 0.6895,
      "step": 2890
    },
    {
      "epoch": 2.3090402230187177,
      "grad_norm": 0.5528902411460876,
      "learning_rate": 4.674556213017752e-05,
      "loss": 0.7227,
      "step": 2900
    },
    {
      "epoch": 2.317005177220231,
      "grad_norm": 0.5790509581565857,
      "learning_rate": 4.6207638515330825e-05,
      "loss": 0.6777,
      "step": 2910
    },
    {
      "epoch": 2.324970131421744,
      "grad_norm": 0.5527275800704956,
      "learning_rate": 4.5669714900484134e-05,
      "loss": 0.7332,
      "step": 2920
    },
    {
      "epoch": 2.332935085623258,
      "grad_norm": 0.5016792416572571,
      "learning_rate": 4.513179128563744e-05,
      "loss": 0.6848,
      "step": 2930
    },
    {
      "epoch": 2.340900039824771,
      "grad_norm": 0.5079123377799988,
      "learning_rate": 4.459386767079075e-05,
      "loss": 0.6896,
      "step": 2940
    },
    {
      "epoch": 2.3488649940262842,
      "grad_norm": 0.5804486274719238,
      "learning_rate": 4.405594405594406e-05,
      "loss": 0.7366,
      "step": 2950
    },
    {
      "epoch": 2.356829948227798,
      "grad_norm": 0.5144789814949036,
      "learning_rate": 4.351802044109737e-05,
      "loss": 0.6971,
      "step": 2960
    },
    {
      "epoch": 2.364794902429311,
      "grad_norm": 0.544153094291687,
      "learning_rate": 4.2980096826250677e-05,
      "loss": 0.6578,
      "step": 2970
    },
    {
      "epoch": 2.3727598566308243,
      "grad_norm": 0.5699613690376282,
      "learning_rate": 4.2442173211403985e-05,
      "loss": 0.7109,
      "step": 2980
    },
    {
      "epoch": 2.3807248108323376,
      "grad_norm": 0.5060641169548035,
      "learning_rate": 4.1904249596557294e-05,
      "loss": 0.7051,
      "step": 2990
    },
    {
      "epoch": 2.388689765033851,
      "grad_norm": 0.5277009010314941,
      "learning_rate": 4.1366325981710595e-05,
      "loss": 0.699,
      "step": 3000
    },
    {
      "epoch": 2.3966547192353644,
      "grad_norm": 0.5735759735107422,
      "learning_rate": 4.0828402366863904e-05,
      "loss": 0.7162,
      "step": 3010
    },
    {
      "epoch": 2.4046196734368777,
      "grad_norm": 0.5395981073379517,
      "learning_rate": 4.029047875201721e-05,
      "loss": 0.7161,
      "step": 3020
    },
    {
      "epoch": 2.412584627638391,
      "grad_norm": 0.4709540009498596,
      "learning_rate": 3.975255513717052e-05,
      "loss": 0.6518,
      "step": 3030
    },
    {
      "epoch": 2.4205495818399045,
      "grad_norm": 0.478793203830719,
      "learning_rate": 3.921463152232383e-05,
      "loss": 0.711,
      "step": 3040
    },
    {
      "epoch": 2.4285145360414178,
      "grad_norm": 0.5895249247550964,
      "learning_rate": 3.867670790747714e-05,
      "loss": 0.6846,
      "step": 3050
    },
    {
      "epoch": 2.436479490242931,
      "grad_norm": 0.507109522819519,
      "learning_rate": 3.813878429263045e-05,
      "loss": 0.685,
      "step": 3060
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.5473280549049377,
      "learning_rate": 3.7600860677783755e-05,
      "loss": 0.7201,
      "step": 3070
    },
    {
      "epoch": 2.452409398645958,
      "grad_norm": 0.5722809433937073,
      "learning_rate": 3.7062937062937064e-05,
      "loss": 0.7117,
      "step": 3080
    },
    {
      "epoch": 2.460374352847471,
      "grad_norm": 0.5330430269241333,
      "learning_rate": 3.652501344809037e-05,
      "loss": 0.7183,
      "step": 3090
    },
    {
      "epoch": 2.4683393070489843,
      "grad_norm": 0.5052078366279602,
      "learning_rate": 3.598708983324368e-05,
      "loss": 0.7071,
      "step": 3100
    },
    {
      "epoch": 2.476304261250498,
      "grad_norm": 0.5484607219696045,
      "learning_rate": 3.544916621839699e-05,
      "loss": 0.6892,
      "step": 3110
    },
    {
      "epoch": 2.484269215452011,
      "grad_norm": 0.5669679641723633,
      "learning_rate": 3.49112426035503e-05,
      "loss": 0.6963,
      "step": 3120
    },
    {
      "epoch": 2.4922341696535244,
      "grad_norm": 0.5860273241996765,
      "learning_rate": 3.4373318988703606e-05,
      "loss": 0.6921,
      "step": 3130
    },
    {
      "epoch": 2.5001991238550376,
      "grad_norm": 0.5657526254653931,
      "learning_rate": 3.3835395373856915e-05,
      "loss": 0.7364,
      "step": 3140
    },
    {
      "epoch": 2.5081640780565513,
      "grad_norm": 0.5174479484558105,
      "learning_rate": 3.3297471759010224e-05,
      "loss": 0.6959,
      "step": 3150
    },
    {
      "epoch": 2.5161290322580645,
      "grad_norm": 0.5422989726066589,
      "learning_rate": 3.275954814416353e-05,
      "loss": 0.7163,
      "step": 3160
    },
    {
      "epoch": 2.5240939864595777,
      "grad_norm": 0.5155618190765381,
      "learning_rate": 3.222162452931684e-05,
      "loss": 0.7179,
      "step": 3170
    },
    {
      "epoch": 2.5320589406610914,
      "grad_norm": 0.5701838135719299,
      "learning_rate": 3.168370091447015e-05,
      "loss": 0.6868,
      "step": 3180
    },
    {
      "epoch": 2.5400238948626046,
      "grad_norm": 0.5255002975463867,
      "learning_rate": 3.114577729962346e-05,
      "loss": 0.7148,
      "step": 3190
    },
    {
      "epoch": 2.547988849064118,
      "grad_norm": 0.5619814991950989,
      "learning_rate": 3.060785368477676e-05,
      "loss": 0.7181,
      "step": 3200
    },
    {
      "epoch": 2.5559538032656315,
      "grad_norm": 0.5414488911628723,
      "learning_rate": 3.0069930069930068e-05,
      "loss": 0.6665,
      "step": 3210
    },
    {
      "epoch": 2.5639187574671447,
      "grad_norm": 0.5838732719421387,
      "learning_rate": 2.9532006455083377e-05,
      "loss": 0.7134,
      "step": 3220
    },
    {
      "epoch": 2.571883711668658,
      "grad_norm": 0.6129980683326721,
      "learning_rate": 2.8994082840236685e-05,
      "loss": 0.7298,
      "step": 3230
    },
    {
      "epoch": 2.579848665870171,
      "grad_norm": 0.5383477807044983,
      "learning_rate": 2.8456159225389994e-05,
      "loss": 0.7127,
      "step": 3240
    },
    {
      "epoch": 2.5878136200716844,
      "grad_norm": 0.6075318455696106,
      "learning_rate": 2.7918235610543302e-05,
      "loss": 0.7071,
      "step": 3250
    },
    {
      "epoch": 2.595778574273198,
      "grad_norm": 0.5382643342018127,
      "learning_rate": 2.738031199569661e-05,
      "loss": 0.712,
      "step": 3260
    },
    {
      "epoch": 2.6037435284747112,
      "grad_norm": 0.6086379289627075,
      "learning_rate": 2.684238838084992e-05,
      "loss": 0.7046,
      "step": 3270
    },
    {
      "epoch": 2.6117084826762245,
      "grad_norm": 0.5800758004188538,
      "learning_rate": 2.6304464766003228e-05,
      "loss": 0.7046,
      "step": 3280
    },
    {
      "epoch": 2.619673436877738,
      "grad_norm": 0.5691474676132202,
      "learning_rate": 2.5766541151156536e-05,
      "loss": 0.7113,
      "step": 3290
    },
    {
      "epoch": 2.6276383910792513,
      "grad_norm": 0.576819896697998,
      "learning_rate": 2.5228617536309845e-05,
      "loss": 0.6846,
      "step": 3300
    },
    {
      "epoch": 2.6356033452807646,
      "grad_norm": 0.5761575698852539,
      "learning_rate": 2.4690693921463153e-05,
      "loss": 0.6784,
      "step": 3310
    },
    {
      "epoch": 2.6435682994822782,
      "grad_norm": 0.5302213430404663,
      "learning_rate": 2.4152770306616462e-05,
      "loss": 0.7327,
      "step": 3320
    },
    {
      "epoch": 2.6515332536837914,
      "grad_norm": 0.5900525450706482,
      "learning_rate": 2.361484669176977e-05,
      "loss": 0.722,
      "step": 3330
    },
    {
      "epoch": 2.6594982078853047,
      "grad_norm": 0.538966715335846,
      "learning_rate": 2.307692307692308e-05,
      "loss": 0.6823,
      "step": 3340
    },
    {
      "epoch": 2.667463162086818,
      "grad_norm": 0.6026687026023865,
      "learning_rate": 2.2538999462076384e-05,
      "loss": 0.7137,
      "step": 3350
    },
    {
      "epoch": 2.675428116288331,
      "grad_norm": 0.6426318287849426,
      "learning_rate": 2.2001075847229693e-05,
      "loss": 0.6847,
      "step": 3360
    },
    {
      "epoch": 2.6833930704898448,
      "grad_norm": 0.5818213224411011,
      "learning_rate": 2.1463152232383e-05,
      "loss": 0.7219,
      "step": 3370
    },
    {
      "epoch": 2.691358024691358,
      "grad_norm": 0.5365744233131409,
      "learning_rate": 2.092522861753631e-05,
      "loss": 0.6887,
      "step": 3380
    },
    {
      "epoch": 2.699322978892871,
      "grad_norm": 0.5480359196662903,
      "learning_rate": 2.038730500268962e-05,
      "loss": 0.7089,
      "step": 3390
    },
    {
      "epoch": 2.707287933094385,
      "grad_norm": 0.5666966438293457,
      "learning_rate": 1.9849381387842927e-05,
      "loss": 0.6885,
      "step": 3400
    },
    {
      "epoch": 2.715252887295898,
      "grad_norm": 0.5086397528648376,
      "learning_rate": 1.9311457772996235e-05,
      "loss": 0.7059,
      "step": 3410
    },
    {
      "epoch": 2.7232178414974113,
      "grad_norm": 0.5928812623023987,
      "learning_rate": 1.8773534158149544e-05,
      "loss": 0.7236,
      "step": 3420
    },
    {
      "epoch": 2.731182795698925,
      "grad_norm": 0.6126912236213684,
      "learning_rate": 1.8235610543302853e-05,
      "loss": 0.6882,
      "step": 3430
    },
    {
      "epoch": 2.739147749900438,
      "grad_norm": 0.5971669554710388,
      "learning_rate": 1.769768692845616e-05,
      "loss": 0.7057,
      "step": 3440
    },
    {
      "epoch": 2.7471127041019514,
      "grad_norm": 0.54996258020401,
      "learning_rate": 1.7159763313609466e-05,
      "loss": 0.7006,
      "step": 3450
    },
    {
      "epoch": 2.7550776583034646,
      "grad_norm": 0.5770092010498047,
      "learning_rate": 1.6621839698762775e-05,
      "loss": 0.6966,
      "step": 3460
    },
    {
      "epoch": 2.763042612504978,
      "grad_norm": 0.582775354385376,
      "learning_rate": 1.6083916083916083e-05,
      "loss": 0.7248,
      "step": 3470
    },
    {
      "epoch": 2.7710075667064915,
      "grad_norm": 0.49920782446861267,
      "learning_rate": 1.5545992469069392e-05,
      "loss": 0.6903,
      "step": 3480
    },
    {
      "epoch": 2.7789725209080047,
      "grad_norm": 0.5506903529167175,
      "learning_rate": 1.50080688542227e-05,
      "loss": 0.6895,
      "step": 3490
    },
    {
      "epoch": 2.786937475109518,
      "grad_norm": 0.5528154969215393,
      "learning_rate": 1.4470145239376009e-05,
      "loss": 0.7166,
      "step": 3500
    },
    {
      "epoch": 2.7949024293110316,
      "grad_norm": 0.5971211791038513,
      "learning_rate": 1.3932221624529318e-05,
      "loss": 0.684,
      "step": 3510
    },
    {
      "epoch": 2.802867383512545,
      "grad_norm": 0.4879603683948517,
      "learning_rate": 1.3394298009682626e-05,
      "loss": 0.7006,
      "step": 3520
    },
    {
      "epoch": 2.810832337714058,
      "grad_norm": 0.5354431867599487,
      "learning_rate": 1.2856374394835935e-05,
      "loss": 0.6898,
      "step": 3530
    },
    {
      "epoch": 2.8187972919155717,
      "grad_norm": 0.5352184772491455,
      "learning_rate": 1.2318450779989241e-05,
      "loss": 0.7058,
      "step": 3540
    },
    {
      "epoch": 2.826762246117085,
      "grad_norm": 0.5314378142356873,
      "learning_rate": 1.178052716514255e-05,
      "loss": 0.6885,
      "step": 3550
    },
    {
      "epoch": 2.834727200318598,
      "grad_norm": 0.5487663149833679,
      "learning_rate": 1.1242603550295859e-05,
      "loss": 0.7328,
      "step": 3560
    },
    {
      "epoch": 2.8426921545201114,
      "grad_norm": 0.5307224988937378,
      "learning_rate": 1.0704679935449165e-05,
      "loss": 0.6913,
      "step": 3570
    },
    {
      "epoch": 2.8506571087216246,
      "grad_norm": 0.5410717129707336,
      "learning_rate": 1.0166756320602474e-05,
      "loss": 0.7089,
      "step": 3580
    },
    {
      "epoch": 2.8586220629231383,
      "grad_norm": 0.5336647033691406,
      "learning_rate": 9.628832705755782e-06,
      "loss": 0.7072,
      "step": 3590
    },
    {
      "epoch": 2.8665870171246515,
      "grad_norm": 0.5437268614768982,
      "learning_rate": 9.090909090909091e-06,
      "loss": 0.6803,
      "step": 3600
    },
    {
      "epoch": 2.8745519713261647,
      "grad_norm": 0.5826005935668945,
      "learning_rate": 8.5529854760624e-06,
      "loss": 0.6824,
      "step": 3610
    },
    {
      "epoch": 2.8825169255276784,
      "grad_norm": 0.5286666750907898,
      "learning_rate": 8.015061861215706e-06,
      "loss": 0.6967,
      "step": 3620
    },
    {
      "epoch": 2.8904818797291916,
      "grad_norm": 0.6445249319076538,
      "learning_rate": 7.477138246369016e-06,
      "loss": 0.7164,
      "step": 3630
    },
    {
      "epoch": 2.898446833930705,
      "grad_norm": 0.5282170176506042,
      "learning_rate": 6.939214631522324e-06,
      "loss": 0.687,
      "step": 3640
    },
    {
      "epoch": 2.9064117881322185,
      "grad_norm": 0.5837377905845642,
      "learning_rate": 6.401291016675633e-06,
      "loss": 0.7058,
      "step": 3650
    },
    {
      "epoch": 2.9143767423337317,
      "grad_norm": 0.5324959754943848,
      "learning_rate": 5.863367401828941e-06,
      "loss": 0.6757,
      "step": 3660
    },
    {
      "epoch": 2.922341696535245,
      "grad_norm": 0.6124046444892883,
      "learning_rate": 5.325443786982249e-06,
      "loss": 0.7022,
      "step": 3670
    },
    {
      "epoch": 2.930306650736758,
      "grad_norm": 0.547589123249054,
      "learning_rate": 4.787520172135557e-06,
      "loss": 0.7166,
      "step": 3680
    },
    {
      "epoch": 2.9382716049382713,
      "grad_norm": 0.5376645922660828,
      "learning_rate": 4.249596557288865e-06,
      "loss": 0.6994,
      "step": 3690
    },
    {
      "epoch": 2.946236559139785,
      "grad_norm": 0.5922972559928894,
      "learning_rate": 3.7116729424421735e-06,
      "loss": 0.7241,
      "step": 3700
    },
    {
      "epoch": 2.954201513341298,
      "grad_norm": 0.6280102133750916,
      "learning_rate": 3.1737493275954816e-06,
      "loss": 0.6798,
      "step": 3710
    },
    {
      "epoch": 2.9621664675428114,
      "grad_norm": 0.5498873591423035,
      "learning_rate": 2.6358257127487897e-06,
      "loss": 0.6928,
      "step": 3720
    },
    {
      "epoch": 2.970131421744325,
      "grad_norm": 0.5770207047462463,
      "learning_rate": 2.0979020979020983e-06,
      "loss": 0.6972,
      "step": 3730
    },
    {
      "epoch": 2.9780963759458383,
      "grad_norm": 0.5093324780464172,
      "learning_rate": 1.5599784830554062e-06,
      "loss": 0.6843,
      "step": 3740
    },
    {
      "epoch": 2.9860613301473515,
      "grad_norm": 0.5692604184150696,
      "learning_rate": 1.0220548682087143e-06,
      "loss": 0.7106,
      "step": 3750
    },
    {
      "epoch": 2.994026284348865,
      "grad_norm": 0.647711455821991,
      "learning_rate": 4.841312533620225e-07,
      "loss": 0.7193,
      "step": 3760
    }
  ],
  "logging_steps": 10,
  "max_steps": 3768,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3992172946653184e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
