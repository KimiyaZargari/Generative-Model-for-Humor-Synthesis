{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1256,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00796495420151334,
      "grad_norm": 133.13047790527344,
      "learning_rate": 3.6e-05,
      "loss": 8.5937,
      "step": 10
    },
    {
      "epoch": 0.01592990840302668,
      "grad_norm": 15.227502822875977,
      "learning_rate": 7.6e-05,
      "loss": 4.6764,
      "step": 20
    },
    {
      "epoch": 0.023894862604540025,
      "grad_norm": 4.609999179840088,
      "learning_rate": 0.000116,
      "loss": 2.0707,
      "step": 30
    },
    {
      "epoch": 0.03185981680605336,
      "grad_norm": 0.696156919002533,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.0088,
      "step": 40
    },
    {
      "epoch": 0.039824771007566706,
      "grad_norm": 0.6791530847549438,
      "learning_rate": 0.000196,
      "loss": 0.9243,
      "step": 50
    },
    {
      "epoch": 0.04778972520908005,
      "grad_norm": 0.5873997807502747,
      "learning_rate": 0.00019951586874663799,
      "loss": 0.8584,
      "step": 60
    },
    {
      "epoch": 0.05575467941059339,
      "grad_norm": 0.5680383443832397,
      "learning_rate": 0.0001989779451317913,
      "loss": 0.8108,
      "step": 70
    },
    {
      "epoch": 0.06371963361210672,
      "grad_norm": 0.5462983250617981,
      "learning_rate": 0.00019844002151694462,
      "loss": 0.8032,
      "step": 80
    },
    {
      "epoch": 0.07168458781362007,
      "grad_norm": 0.5560625791549683,
      "learning_rate": 0.0001979020979020979,
      "loss": 0.7863,
      "step": 90
    },
    {
      "epoch": 0.07964954201513341,
      "grad_norm": 0.5551924705505371,
      "learning_rate": 0.00019736417428725122,
      "loss": 0.7818,
      "step": 100
    },
    {
      "epoch": 0.08761449621664676,
      "grad_norm": 0.5794554948806763,
      "learning_rate": 0.00019682625067240453,
      "loss": 0.7911,
      "step": 110
    },
    {
      "epoch": 0.0955794504181601,
      "grad_norm": 0.6070852875709534,
      "learning_rate": 0.00019628832705755785,
      "loss": 0.7937,
      "step": 120
    },
    {
      "epoch": 0.10354440461967343,
      "grad_norm": 0.5872889161109924,
      "learning_rate": 0.00019575040344271114,
      "loss": 0.7998,
      "step": 130
    },
    {
      "epoch": 0.11150935882118677,
      "grad_norm": 0.5939893126487732,
      "learning_rate": 0.00019521247982786445,
      "loss": 0.7531,
      "step": 140
    },
    {
      "epoch": 0.11947431302270012,
      "grad_norm": 0.5536476969718933,
      "learning_rate": 0.00019467455621301777,
      "loss": 0.7962,
      "step": 150
    },
    {
      "epoch": 0.12743926722421345,
      "grad_norm": 0.5763424634933472,
      "learning_rate": 0.00019413663259817108,
      "loss": 0.7622,
      "step": 160
    },
    {
      "epoch": 0.1354042214257268,
      "grad_norm": 0.5685245990753174,
      "learning_rate": 0.00019359870898332437,
      "loss": 0.7692,
      "step": 170
    },
    {
      "epoch": 0.14336917562724014,
      "grad_norm": 0.5661225318908691,
      "learning_rate": 0.0001930607853684777,
      "loss": 0.7409,
      "step": 180
    },
    {
      "epoch": 0.1513341298287535,
      "grad_norm": 0.5888197422027588,
      "learning_rate": 0.000192522861753631,
      "loss": 0.7779,
      "step": 190
    },
    {
      "epoch": 0.15929908403026682,
      "grad_norm": 0.5563907623291016,
      "learning_rate": 0.00019198493813878432,
      "loss": 0.7681,
      "step": 200
    },
    {
      "epoch": 0.16726403823178015,
      "grad_norm": 0.5455360412597656,
      "learning_rate": 0.0001914470145239376,
      "loss": 0.7614,
      "step": 210
    },
    {
      "epoch": 0.1752289924332935,
      "grad_norm": 0.5019347667694092,
      "learning_rate": 0.00019090909090909092,
      "loss": 0.7718,
      "step": 220
    },
    {
      "epoch": 0.18319394663480684,
      "grad_norm": 0.553898274898529,
      "learning_rate": 0.0001903711672942442,
      "loss": 0.7723,
      "step": 230
    },
    {
      "epoch": 0.1911589008363202,
      "grad_norm": 0.5552647113800049,
      "learning_rate": 0.00018983324367939755,
      "loss": 0.762,
      "step": 240
    },
    {
      "epoch": 0.19912385503783353,
      "grad_norm": 0.5925323367118835,
      "learning_rate": 0.00018929532006455084,
      "loss": 0.7665,
      "step": 250
    },
    {
      "epoch": 0.20708880923934686,
      "grad_norm": 0.4991273283958435,
      "learning_rate": 0.00018875739644970416,
      "loss": 0.799,
      "step": 260
    },
    {
      "epoch": 0.21505376344086022,
      "grad_norm": 0.5987207293510437,
      "learning_rate": 0.00018821947283485744,
      "loss": 0.81,
      "step": 270
    },
    {
      "epoch": 0.22301871764237355,
      "grad_norm": 0.5108638405799866,
      "learning_rate": 0.0001876815492200108,
      "loss": 0.7694,
      "step": 280
    },
    {
      "epoch": 0.2309836718438869,
      "grad_norm": 0.4778127074241638,
      "learning_rate": 0.00018714362560516407,
      "loss": 0.769,
      "step": 290
    },
    {
      "epoch": 0.23894862604540024,
      "grad_norm": 0.49839282035827637,
      "learning_rate": 0.0001866057019903174,
      "loss": 0.7449,
      "step": 300
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 0.5170665383338928,
      "learning_rate": 0.00018606777837547068,
      "loss": 0.7877,
      "step": 310
    },
    {
      "epoch": 0.2548785344484269,
      "grad_norm": 0.5087386965751648,
      "learning_rate": 0.00018552985476062402,
      "loss": 0.7811,
      "step": 320
    },
    {
      "epoch": 0.2628434886499403,
      "grad_norm": 0.5170137882232666,
      "learning_rate": 0.0001849919311457773,
      "loss": 0.8107,
      "step": 330
    },
    {
      "epoch": 0.2708084428514536,
      "grad_norm": 0.5296459794044495,
      "learning_rate": 0.0001844540075309306,
      "loss": 0.7449,
      "step": 340
    },
    {
      "epoch": 0.27877339705296694,
      "grad_norm": 0.490922212600708,
      "learning_rate": 0.0001839160839160839,
      "loss": 0.7663,
      "step": 350
    },
    {
      "epoch": 0.2867383512544803,
      "grad_norm": 0.519218921661377,
      "learning_rate": 0.00018337816030123723,
      "loss": 0.7249,
      "step": 360
    },
    {
      "epoch": 0.2947033054559936,
      "grad_norm": 0.549088716506958,
      "learning_rate": 0.00018284023668639054,
      "loss": 0.7524,
      "step": 370
    },
    {
      "epoch": 0.302668259657507,
      "grad_norm": 0.4940321743488312,
      "learning_rate": 0.00018230231307154383,
      "loss": 0.7297,
      "step": 380
    },
    {
      "epoch": 0.3106332138590203,
      "grad_norm": 0.46705177426338196,
      "learning_rate": 0.00018176438945669715,
      "loss": 0.7748,
      "step": 390
    },
    {
      "epoch": 0.31859816806053365,
      "grad_norm": 0.539527952671051,
      "learning_rate": 0.00018122646584185046,
      "loss": 0.7599,
      "step": 400
    },
    {
      "epoch": 0.326563122262047,
      "grad_norm": 0.5246395468711853,
      "learning_rate": 0.00018068854222700378,
      "loss": 0.7363,
      "step": 410
    },
    {
      "epoch": 0.3345280764635603,
      "grad_norm": 0.5225979685783386,
      "learning_rate": 0.00018015061861215707,
      "loss": 0.779,
      "step": 420
    },
    {
      "epoch": 0.3424930306650737,
      "grad_norm": 0.5296785235404968,
      "learning_rate": 0.00017961269499731038,
      "loss": 0.7487,
      "step": 430
    },
    {
      "epoch": 0.350457984866587,
      "grad_norm": 0.49650877714157104,
      "learning_rate": 0.0001790747713824637,
      "loss": 0.7392,
      "step": 440
    },
    {
      "epoch": 0.35842293906810035,
      "grad_norm": 0.4982154965400696,
      "learning_rate": 0.000178536847767617,
      "loss": 0.7535,
      "step": 450
    },
    {
      "epoch": 0.3663878932696137,
      "grad_norm": 0.5009126663208008,
      "learning_rate": 0.0001779989241527703,
      "loss": 0.7537,
      "step": 460
    },
    {
      "epoch": 0.374352847471127,
      "grad_norm": 0.494118869304657,
      "learning_rate": 0.00017746100053792362,
      "loss": 0.7849,
      "step": 470
    },
    {
      "epoch": 0.3823178016726404,
      "grad_norm": 0.5648855566978455,
      "learning_rate": 0.00017692307692307693,
      "loss": 0.7457,
      "step": 480
    },
    {
      "epoch": 0.39028275587415373,
      "grad_norm": 0.4557546377182007,
      "learning_rate": 0.00017638515330823025,
      "loss": 0.7284,
      "step": 490
    },
    {
      "epoch": 0.39824771007566706,
      "grad_norm": 0.5763673782348633,
      "learning_rate": 0.00017584722969338353,
      "loss": 0.7501,
      "step": 500
    },
    {
      "epoch": 0.4062126642771804,
      "grad_norm": 0.5994834303855896,
      "learning_rate": 0.00017530930607853685,
      "loss": 0.7547,
      "step": 510
    },
    {
      "epoch": 0.4141776184786937,
      "grad_norm": 0.511115312576294,
      "learning_rate": 0.00017477138246369016,
      "loss": 0.7666,
      "step": 520
    },
    {
      "epoch": 0.4221425726802071,
      "grad_norm": 0.4372538924217224,
      "learning_rate": 0.00017423345884884348,
      "loss": 0.744,
      "step": 530
    },
    {
      "epoch": 0.43010752688172044,
      "grad_norm": 0.5074840188026428,
      "learning_rate": 0.00017369553523399677,
      "loss": 0.7809,
      "step": 540
    },
    {
      "epoch": 0.43807248108323377,
      "grad_norm": 0.5417396426200867,
      "learning_rate": 0.00017315761161915008,
      "loss": 0.7411,
      "step": 550
    },
    {
      "epoch": 0.4460374352847471,
      "grad_norm": 0.5052236914634705,
      "learning_rate": 0.0001726196880043034,
      "loss": 0.7448,
      "step": 560
    },
    {
      "epoch": 0.4540023894862604,
      "grad_norm": 0.5071873664855957,
      "learning_rate": 0.00017208176438945671,
      "loss": 0.771,
      "step": 570
    },
    {
      "epoch": 0.4619673436877738,
      "grad_norm": 0.5701971054077148,
      "learning_rate": 0.00017154384077461,
      "loss": 0.765,
      "step": 580
    },
    {
      "epoch": 0.46993229788928714,
      "grad_norm": 0.522875964641571,
      "learning_rate": 0.00017100591715976332,
      "loss": 0.7561,
      "step": 590
    },
    {
      "epoch": 0.4778972520908005,
      "grad_norm": 0.5036452412605286,
      "learning_rate": 0.00017046799354491663,
      "loss": 0.7974,
      "step": 600
    },
    {
      "epoch": 0.4858622062923138,
      "grad_norm": 0.517601728439331,
      "learning_rate": 0.00016993006993006995,
      "loss": 0.7614,
      "step": 610
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 0.5130582451820374,
      "learning_rate": 0.00016939214631522324,
      "loss": 0.7422,
      "step": 620
    },
    {
      "epoch": 0.5017921146953405,
      "grad_norm": 0.46496492624282837,
      "learning_rate": 0.00016885422270037655,
      "loss": 0.7675,
      "step": 630
    },
    {
      "epoch": 0.5097570688968538,
      "grad_norm": 0.46519261598587036,
      "learning_rate": 0.00016831629908552987,
      "loss": 0.7535,
      "step": 640
    },
    {
      "epoch": 0.5177220230983672,
      "grad_norm": 0.4690667390823364,
      "learning_rate": 0.00016777837547068318,
      "loss": 0.7376,
      "step": 650
    },
    {
      "epoch": 0.5256869772998806,
      "grad_norm": 0.47642776370048523,
      "learning_rate": 0.00016724045185583647,
      "loss": 0.7435,
      "step": 660
    },
    {
      "epoch": 0.5336519315013939,
      "grad_norm": 0.4922437071800232,
      "learning_rate": 0.00016670252824098979,
      "loss": 0.7878,
      "step": 670
    },
    {
      "epoch": 0.5416168857029072,
      "grad_norm": 0.427765429019928,
      "learning_rate": 0.0001661646046261431,
      "loss": 0.7383,
      "step": 680
    },
    {
      "epoch": 0.5495818399044206,
      "grad_norm": 0.5159996151924133,
      "learning_rate": 0.00016562668101129642,
      "loss": 0.7415,
      "step": 690
    },
    {
      "epoch": 0.5575467941059339,
      "grad_norm": 0.4753890931606293,
      "learning_rate": 0.0001650887573964497,
      "loss": 0.7647,
      "step": 700
    },
    {
      "epoch": 0.5655117483074472,
      "grad_norm": 0.5033360123634338,
      "learning_rate": 0.00016455083378160302,
      "loss": 0.7617,
      "step": 710
    },
    {
      "epoch": 0.5734767025089605,
      "grad_norm": 0.5544610619544983,
      "learning_rate": 0.00016401291016675634,
      "loss": 0.7655,
      "step": 720
    },
    {
      "epoch": 0.5814416567104739,
      "grad_norm": 0.4957584738731384,
      "learning_rate": 0.00016347498655190962,
      "loss": 0.7706,
      "step": 730
    },
    {
      "epoch": 0.5894066109119872,
      "grad_norm": 0.49697065353393555,
      "learning_rate": 0.00016293706293706294,
      "loss": 0.7611,
      "step": 740
    },
    {
      "epoch": 0.5973715651135006,
      "grad_norm": 0.514756977558136,
      "learning_rate": 0.00016239913932221625,
      "loss": 0.7573,
      "step": 750
    },
    {
      "epoch": 0.605336519315014,
      "grad_norm": 0.5491628646850586,
      "learning_rate": 0.00016186121570736957,
      "loss": 0.786,
      "step": 760
    },
    {
      "epoch": 0.6133014735165273,
      "grad_norm": 0.5565916299819946,
      "learning_rate": 0.00016132329209252286,
      "loss": 0.7502,
      "step": 770
    },
    {
      "epoch": 0.6212664277180406,
      "grad_norm": 0.5392376184463501,
      "learning_rate": 0.00016078536847767617,
      "loss": 0.7493,
      "step": 780
    },
    {
      "epoch": 0.629231381919554,
      "grad_norm": 0.5479769706726074,
      "learning_rate": 0.0001602474448628295,
      "loss": 0.7957,
      "step": 790
    },
    {
      "epoch": 0.6371963361210673,
      "grad_norm": 0.5240273475646973,
      "learning_rate": 0.0001597095212479828,
      "loss": 0.7792,
      "step": 800
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 0.4773210883140564,
      "learning_rate": 0.0001591715976331361,
      "loss": 0.7606,
      "step": 810
    },
    {
      "epoch": 0.653126244524094,
      "grad_norm": 0.44444289803504944,
      "learning_rate": 0.0001586336740182894,
      "loss": 0.7673,
      "step": 820
    },
    {
      "epoch": 0.6610911987256073,
      "grad_norm": 0.5055608153343201,
      "learning_rate": 0.00015809575040344272,
      "loss": 0.77,
      "step": 830
    },
    {
      "epoch": 0.6690561529271206,
      "grad_norm": 0.46055611968040466,
      "learning_rate": 0.00015755782678859604,
      "loss": 0.7545,
      "step": 840
    },
    {
      "epoch": 0.6770211071286341,
      "grad_norm": 0.497326523065567,
      "learning_rate": 0.00015701990317374933,
      "loss": 0.7276,
      "step": 850
    },
    {
      "epoch": 0.6849860613301474,
      "grad_norm": 0.4432331323623657,
      "learning_rate": 0.00015648197955890264,
      "loss": 0.7483,
      "step": 860
    },
    {
      "epoch": 0.6929510155316607,
      "grad_norm": 0.4714212417602539,
      "learning_rate": 0.00015594405594405596,
      "loss": 0.757,
      "step": 870
    },
    {
      "epoch": 0.700915969733174,
      "grad_norm": 0.4514620006084442,
      "learning_rate": 0.00015540613232920927,
      "loss": 0.726,
      "step": 880
    },
    {
      "epoch": 0.7088809239346874,
      "grad_norm": 0.4436425268650055,
      "learning_rate": 0.00015486820871436256,
      "loss": 0.7244,
      "step": 890
    },
    {
      "epoch": 0.7168458781362007,
      "grad_norm": 0.4528445303440094,
      "learning_rate": 0.00015433028509951588,
      "loss": 0.7403,
      "step": 900
    },
    {
      "epoch": 0.724810832337714,
      "grad_norm": 0.4918217957019806,
      "learning_rate": 0.0001537923614846692,
      "loss": 0.7304,
      "step": 910
    },
    {
      "epoch": 0.7327757865392274,
      "grad_norm": 0.4754583239555359,
      "learning_rate": 0.0001532544378698225,
      "loss": 0.7506,
      "step": 920
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.5018563270568848,
      "learning_rate": 0.0001527165142549758,
      "loss": 0.7681,
      "step": 930
    },
    {
      "epoch": 0.748705694942254,
      "grad_norm": 0.49649712443351746,
      "learning_rate": 0.0001521785906401291,
      "loss": 0.7465,
      "step": 940
    },
    {
      "epoch": 0.7566706491437675,
      "grad_norm": 0.4681350290775299,
      "learning_rate": 0.00015164066702528243,
      "loss": 0.7273,
      "step": 950
    },
    {
      "epoch": 0.7646356033452808,
      "grad_norm": 0.4947724938392639,
      "learning_rate": 0.00015110274341043574,
      "loss": 0.7755,
      "step": 960
    },
    {
      "epoch": 0.7726005575467941,
      "grad_norm": 0.4668620526790619,
      "learning_rate": 0.00015056481979558903,
      "loss": 0.7154,
      "step": 970
    },
    {
      "epoch": 0.7805655117483075,
      "grad_norm": 0.4795647859573364,
      "learning_rate": 0.00015002689618074234,
      "loss": 0.7368,
      "step": 980
    },
    {
      "epoch": 0.7885304659498208,
      "grad_norm": 0.4544738829135895,
      "learning_rate": 0.00014948897256589566,
      "loss": 0.7445,
      "step": 990
    },
    {
      "epoch": 0.7964954201513341,
      "grad_norm": 0.4597337543964386,
      "learning_rate": 0.00014895104895104897,
      "loss": 0.772,
      "step": 1000
    },
    {
      "epoch": 0.8044603743528475,
      "grad_norm": 0.4793347120285034,
      "learning_rate": 0.00014841312533620226,
      "loss": 0.7226,
      "step": 1010
    },
    {
      "epoch": 0.8124253285543608,
      "grad_norm": 0.46236521005630493,
      "learning_rate": 0.00014787520172135558,
      "loss": 0.7293,
      "step": 1020
    },
    {
      "epoch": 0.8203902827558741,
      "grad_norm": 0.4980211853981018,
      "learning_rate": 0.0001473372781065089,
      "loss": 0.7556,
      "step": 1030
    },
    {
      "epoch": 0.8283552369573874,
      "grad_norm": 0.45580118894577026,
      "learning_rate": 0.0001467993544916622,
      "loss": 0.7821,
      "step": 1040
    },
    {
      "epoch": 0.8363201911589009,
      "grad_norm": 0.47210371494293213,
      "learning_rate": 0.0001462614308768155,
      "loss": 0.7815,
      "step": 1050
    },
    {
      "epoch": 0.8442851453604142,
      "grad_norm": 0.47734859585762024,
      "learning_rate": 0.0001457235072619688,
      "loss": 0.7624,
      "step": 1060
    },
    {
      "epoch": 0.8522500995619275,
      "grad_norm": 0.45166775584220886,
      "learning_rate": 0.0001451855836471221,
      "loss": 0.7601,
      "step": 1070
    },
    {
      "epoch": 0.8602150537634409,
      "grad_norm": 0.4564398527145386,
      "learning_rate": 0.00014464766003227544,
      "loss": 0.7276,
      "step": 1080
    },
    {
      "epoch": 0.8681800079649542,
      "grad_norm": 0.4579167366027832,
      "learning_rate": 0.00014410973641742873,
      "loss": 0.7516,
      "step": 1090
    },
    {
      "epoch": 0.8761449621664675,
      "grad_norm": 0.45872241258621216,
      "learning_rate": 0.00014357181280258205,
      "loss": 0.7868,
      "step": 1100
    },
    {
      "epoch": 0.8841099163679809,
      "grad_norm": 0.549182116985321,
      "learning_rate": 0.00014303388918773533,
      "loss": 0.7316,
      "step": 1110
    },
    {
      "epoch": 0.8920748705694942,
      "grad_norm": 0.5116949081420898,
      "learning_rate": 0.00014249596557288865,
      "loss": 0.7851,
      "step": 1120
    },
    {
      "epoch": 0.9000398247710075,
      "grad_norm": 0.45142361521720886,
      "learning_rate": 0.00014195804195804197,
      "loss": 0.757,
      "step": 1130
    },
    {
      "epoch": 0.9080047789725209,
      "grad_norm": 0.47317203879356384,
      "learning_rate": 0.00014142011834319525,
      "loss": 0.7527,
      "step": 1140
    },
    {
      "epoch": 0.9159697331740343,
      "grad_norm": 0.45667991042137146,
      "learning_rate": 0.00014088219472834857,
      "loss": 0.7343,
      "step": 1150
    },
    {
      "epoch": 0.9239346873755476,
      "grad_norm": 0.4944150149822235,
      "learning_rate": 0.00014034427111350188,
      "loss": 0.7533,
      "step": 1160
    },
    {
      "epoch": 0.931899641577061,
      "grad_norm": 0.4322386682033539,
      "learning_rate": 0.0001398063474986552,
      "loss": 0.7407,
      "step": 1170
    },
    {
      "epoch": 0.9398645957785743,
      "grad_norm": 0.4452013671398163,
      "learning_rate": 0.0001392684238838085,
      "loss": 0.7574,
      "step": 1180
    },
    {
      "epoch": 0.9478295499800876,
      "grad_norm": 0.519768476486206,
      "learning_rate": 0.0001387305002689618,
      "loss": 0.7405,
      "step": 1190
    },
    {
      "epoch": 0.955794504181601,
      "grad_norm": 0.4438643455505371,
      "learning_rate": 0.00013819257665411512,
      "loss": 0.7489,
      "step": 1200
    },
    {
      "epoch": 0.9637594583831143,
      "grad_norm": 0.46488696336746216,
      "learning_rate": 0.00013765465303926843,
      "loss": 0.7641,
      "step": 1210
    },
    {
      "epoch": 0.9717244125846276,
      "grad_norm": 0.4657285511493683,
      "learning_rate": 0.00013711672942442172,
      "loss": 0.7335,
      "step": 1220
    },
    {
      "epoch": 0.9796893667861409,
      "grad_norm": 0.5356078147888184,
      "learning_rate": 0.00013657880580957504,
      "loss": 0.7374,
      "step": 1230
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 0.5349792838096619,
      "learning_rate": 0.00013604088219472835,
      "loss": 0.7201,
      "step": 1240
    },
    {
      "epoch": 0.9956192751891677,
      "grad_norm": 0.45736873149871826,
      "learning_rate": 0.00013550295857988167,
      "loss": 0.7562,
      "step": 1250
    }
  ],
  "logging_steps": 10,
  "max_steps": 3768,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7997390982217728.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
