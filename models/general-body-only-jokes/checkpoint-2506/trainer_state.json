{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2506,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00798482882523206,
      "grad_norm": 30.482675552368164,
      "learning_rate": 3.6e-05,
      "loss": 5.3446,
      "step": 10
    },
    {
      "epoch": 0.01596965765046412,
      "grad_norm": 36.363956451416016,
      "learning_rate": 7.6e-05,
      "loss": 5.6861,
      "step": 20
    },
    {
      "epoch": 0.023954486475696177,
      "grad_norm": 15.69612979888916,
      "learning_rate": 0.000116,
      "loss": 3.0738,
      "step": 30
    },
    {
      "epoch": 0.03193931530092824,
      "grad_norm": 1.5338035821914673,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.859,
      "step": 40
    },
    {
      "epoch": 0.03992414412616029,
      "grad_norm": 1.0143561363220215,
      "learning_rate": 0.000196,
      "loss": 1.8205,
      "step": 50
    },
    {
      "epoch": 0.047908972951392355,
      "grad_norm": 0.7006070613861084,
      "learning_rate": 0.00019951469398759775,
      "loss": 1.3436,
      "step": 60
    },
    {
      "epoch": 0.05589380177662441,
      "grad_norm": 0.7801810503005981,
      "learning_rate": 0.00019897546508492858,
      "loss": 1.3531,
      "step": 70
    },
    {
      "epoch": 0.06387863060185647,
      "grad_norm": 0.8036137819290161,
      "learning_rate": 0.00019843623618225937,
      "loss": 1.5215,
      "step": 80
    },
    {
      "epoch": 0.07186345942708854,
      "grad_norm": 0.7702972888946533,
      "learning_rate": 0.0001978970072795902,
      "loss": 1.4136,
      "step": 90
    },
    {
      "epoch": 0.07984828825232058,
      "grad_norm": 0.7577479481697083,
      "learning_rate": 0.00019735777837692102,
      "loss": 1.486,
      "step": 100
    },
    {
      "epoch": 0.08783311707755265,
      "grad_norm": 0.6865746974945068,
      "learning_rate": 0.00019681854947425182,
      "loss": 1.4652,
      "step": 110
    },
    {
      "epoch": 0.09581794590278471,
      "grad_norm": 0.8489285111427307,
      "learning_rate": 0.00019627932057158265,
      "loss": 1.5717,
      "step": 120
    },
    {
      "epoch": 0.10380277472801677,
      "grad_norm": 0.7449133396148682,
      "learning_rate": 0.00019574009166891347,
      "loss": 1.4764,
      "step": 130
    },
    {
      "epoch": 0.11178760355324882,
      "grad_norm": 0.637741208076477,
      "learning_rate": 0.00019520086276624427,
      "loss": 1.3293,
      "step": 140
    },
    {
      "epoch": 0.11977243237848088,
      "grad_norm": 0.6205766797065735,
      "learning_rate": 0.0001946616338635751,
      "loss": 1.4435,
      "step": 150
    },
    {
      "epoch": 0.12775726120371295,
      "grad_norm": 0.7272225022315979,
      "learning_rate": 0.00019412240496090592,
      "loss": 1.5667,
      "step": 160
    },
    {
      "epoch": 0.135742090028945,
      "grad_norm": 0.7306421995162964,
      "learning_rate": 0.00019358317605823672,
      "loss": 1.3924,
      "step": 170
    },
    {
      "epoch": 0.14372691885417707,
      "grad_norm": 0.5935525894165039,
      "learning_rate": 0.00019304394715556755,
      "loss": 1.4195,
      "step": 180
    },
    {
      "epoch": 0.15171174767940912,
      "grad_norm": 0.6170304417610168,
      "learning_rate": 0.00019250471825289837,
      "loss": 1.3778,
      "step": 190
    },
    {
      "epoch": 0.15969657650464117,
      "grad_norm": 0.7554738521575928,
      "learning_rate": 0.00019196548935022917,
      "loss": 1.5051,
      "step": 200
    },
    {
      "epoch": 0.16768140532987325,
      "grad_norm": 0.5735598206520081,
      "learning_rate": 0.00019142626044756,
      "loss": 1.3098,
      "step": 210
    },
    {
      "epoch": 0.1756662341551053,
      "grad_norm": 0.7177059054374695,
      "learning_rate": 0.0001908870315448908,
      "loss": 1.4712,
      "step": 220
    },
    {
      "epoch": 0.18365106298033737,
      "grad_norm": 0.6213260293006897,
      "learning_rate": 0.00019034780264222162,
      "loss": 1.6159,
      "step": 230
    },
    {
      "epoch": 0.19163589180556942,
      "grad_norm": 0.707767903804779,
      "learning_rate": 0.00018980857373955245,
      "loss": 1.4745,
      "step": 240
    },
    {
      "epoch": 0.19962072063080147,
      "grad_norm": 0.6198323369026184,
      "learning_rate": 0.00018926934483688324,
      "loss": 1.4939,
      "step": 250
    },
    {
      "epoch": 0.20760554945603354,
      "grad_norm": 0.642883837223053,
      "learning_rate": 0.00018873011593421407,
      "loss": 1.2962,
      "step": 260
    },
    {
      "epoch": 0.2155903782812656,
      "grad_norm": 0.5987836122512817,
      "learning_rate": 0.0001881908870315449,
      "loss": 1.2986,
      "step": 270
    },
    {
      "epoch": 0.22357520710649764,
      "grad_norm": 0.5673917531967163,
      "learning_rate": 0.00018765165812887572,
      "loss": 1.2967,
      "step": 280
    },
    {
      "epoch": 0.23156003593172972,
      "grad_norm": 0.8044225573539734,
      "learning_rate": 0.00018711242922620652,
      "loss": 1.5361,
      "step": 290
    },
    {
      "epoch": 0.23954486475696177,
      "grad_norm": 0.6505784392356873,
      "learning_rate": 0.00018657320032353734,
      "loss": 1.3479,
      "step": 300
    },
    {
      "epoch": 0.24752969358219384,
      "grad_norm": 0.7528983950614929,
      "learning_rate": 0.00018603397142086817,
      "loss": 1.1948,
      "step": 310
    },
    {
      "epoch": 0.2555145224074259,
      "grad_norm": 0.5303647518157959,
      "learning_rate": 0.000185494742518199,
      "loss": 1.3601,
      "step": 320
    },
    {
      "epoch": 0.26349935123265794,
      "grad_norm": 0.6215204000473022,
      "learning_rate": 0.00018495551361552982,
      "loss": 1.3789,
      "step": 330
    },
    {
      "epoch": 0.27148418005789,
      "grad_norm": 0.6296535730361938,
      "learning_rate": 0.00018441628471286062,
      "loss": 1.4543,
      "step": 340
    },
    {
      "epoch": 0.2794690088831221,
      "grad_norm": 0.6368131041526794,
      "learning_rate": 0.00018387705581019144,
      "loss": 1.5253,
      "step": 350
    },
    {
      "epoch": 0.28745383770835414,
      "grad_norm": 0.6374208331108093,
      "learning_rate": 0.00018333782690752227,
      "loss": 1.3088,
      "step": 360
    },
    {
      "epoch": 0.2954386665335862,
      "grad_norm": 0.7112879753112793,
      "learning_rate": 0.00018279859800485307,
      "loss": 1.4524,
      "step": 370
    },
    {
      "epoch": 0.30342349535881824,
      "grad_norm": 0.6349444389343262,
      "learning_rate": 0.0001822593691021839,
      "loss": 1.4364,
      "step": 380
    },
    {
      "epoch": 0.3114083241840503,
      "grad_norm": 4.251534938812256,
      "learning_rate": 0.00018172014019951472,
      "loss": 1.3136,
      "step": 390
    },
    {
      "epoch": 0.31939315300928234,
      "grad_norm": 0.6353300213813782,
      "learning_rate": 0.00018118091129684552,
      "loss": 1.4955,
      "step": 400
    },
    {
      "epoch": 0.32737798183451444,
      "grad_norm": 0.7864438891410828,
      "learning_rate": 0.00018064168239417634,
      "loss": 1.3771,
      "step": 410
    },
    {
      "epoch": 0.3353628106597465,
      "grad_norm": 0.6768373847007751,
      "learning_rate": 0.00018010245349150717,
      "loss": 1.3705,
      "step": 420
    },
    {
      "epoch": 0.34334763948497854,
      "grad_norm": 0.6065288782119751,
      "learning_rate": 0.00017956322458883797,
      "loss": 1.324,
      "step": 430
    },
    {
      "epoch": 0.3513324683102106,
      "grad_norm": 0.8063865900039673,
      "learning_rate": 0.0001790239956861688,
      "loss": 1.3827,
      "step": 440
    },
    {
      "epoch": 0.35931729713544264,
      "grad_norm": 0.5352798700332642,
      "learning_rate": 0.00017848476678349962,
      "loss": 1.3518,
      "step": 450
    },
    {
      "epoch": 0.36730212596067474,
      "grad_norm": 0.6008937358856201,
      "learning_rate": 0.00017794553788083042,
      "loss": 1.4084,
      "step": 460
    },
    {
      "epoch": 0.3752869547859068,
      "grad_norm": 0.559150755405426,
      "learning_rate": 0.00017740630897816124,
      "loss": 1.3979,
      "step": 470
    },
    {
      "epoch": 0.38327178361113884,
      "grad_norm": 0.4672015905380249,
      "learning_rate": 0.00017686708007549207,
      "loss": 1.2824,
      "step": 480
    },
    {
      "epoch": 0.3912566124363709,
      "grad_norm": 0.5719656944274902,
      "learning_rate": 0.00017632785117282287,
      "loss": 1.3949,
      "step": 490
    },
    {
      "epoch": 0.39924144126160294,
      "grad_norm": 0.6068591475486755,
      "learning_rate": 0.0001757886222701537,
      "loss": 1.3966,
      "step": 500
    },
    {
      "epoch": 0.40722627008683504,
      "grad_norm": 0.518182098865509,
      "learning_rate": 0.00017524939336748452,
      "loss": 1.3885,
      "step": 510
    },
    {
      "epoch": 0.4152110989120671,
      "grad_norm": 0.6265910267829895,
      "learning_rate": 0.00017471016446481531,
      "loss": 1.3725,
      "step": 520
    },
    {
      "epoch": 0.42319592773729914,
      "grad_norm": 0.6024556159973145,
      "learning_rate": 0.00017417093556214614,
      "loss": 1.2687,
      "step": 530
    },
    {
      "epoch": 0.4311807565625312,
      "grad_norm": 0.5678408145904541,
      "learning_rate": 0.00017363170665947697,
      "loss": 1.4154,
      "step": 540
    },
    {
      "epoch": 0.43916558538776324,
      "grad_norm": 0.6861778497695923,
      "learning_rate": 0.00017309247775680776,
      "loss": 1.4359,
      "step": 550
    },
    {
      "epoch": 0.4471504142129953,
      "grad_norm": 0.5948216915130615,
      "learning_rate": 0.0001725532488541386,
      "loss": 1.4136,
      "step": 560
    },
    {
      "epoch": 0.4551352430382274,
      "grad_norm": 0.5641587376594543,
      "learning_rate": 0.0001720140199514694,
      "loss": 1.3593,
      "step": 570
    },
    {
      "epoch": 0.46312007186345944,
      "grad_norm": 0.5326521396636963,
      "learning_rate": 0.0001714747910488002,
      "loss": 1.3766,
      "step": 580
    },
    {
      "epoch": 0.4711049006886915,
      "grad_norm": 0.6434705853462219,
      "learning_rate": 0.00017093556214613104,
      "loss": 1.4805,
      "step": 590
    },
    {
      "epoch": 0.47908972951392353,
      "grad_norm": 0.6152803897857666,
      "learning_rate": 0.00017039633324346184,
      "loss": 1.6189,
      "step": 600
    },
    {
      "epoch": 0.4870745583391556,
      "grad_norm": 0.5218241214752197,
      "learning_rate": 0.00016985710434079266,
      "loss": 1.396,
      "step": 610
    },
    {
      "epoch": 0.4950593871643877,
      "grad_norm": 0.4779745638370514,
      "learning_rate": 0.0001693178754381235,
      "loss": 1.228,
      "step": 620
    },
    {
      "epoch": 0.5030442159896197,
      "grad_norm": 0.6390764117240906,
      "learning_rate": 0.00016877864653545429,
      "loss": 1.4059,
      "step": 630
    },
    {
      "epoch": 0.5110290448148518,
      "grad_norm": 0.5837946534156799,
      "learning_rate": 0.0001682394176327851,
      "loss": 1.3067,
      "step": 640
    },
    {
      "epoch": 0.5190138736400839,
      "grad_norm": 0.6048617959022522,
      "learning_rate": 0.00016770018873011594,
      "loss": 1.4396,
      "step": 650
    },
    {
      "epoch": 0.5269987024653159,
      "grad_norm": 0.5870561003684998,
      "learning_rate": 0.00016716095982744676,
      "loss": 1.373,
      "step": 660
    },
    {
      "epoch": 0.534983531290548,
      "grad_norm": 0.6002352833747864,
      "learning_rate": 0.0001666217309247776,
      "loss": 1.2448,
      "step": 670
    },
    {
      "epoch": 0.54296836011578,
      "grad_norm": 0.6091067790985107,
      "learning_rate": 0.00016608250202210839,
      "loss": 1.4321,
      "step": 680
    },
    {
      "epoch": 0.5509531889410121,
      "grad_norm": 0.6509440541267395,
      "learning_rate": 0.0001655432731194392,
      "loss": 1.3774,
      "step": 690
    },
    {
      "epoch": 0.5589380177662442,
      "grad_norm": 0.5993853807449341,
      "learning_rate": 0.00016500404421677004,
      "loss": 1.4956,
      "step": 700
    },
    {
      "epoch": 0.5669228465914762,
      "grad_norm": 0.5307263731956482,
      "learning_rate": 0.00016446481531410086,
      "loss": 1.344,
      "step": 710
    },
    {
      "epoch": 0.5749076754167083,
      "grad_norm": 0.523870587348938,
      "learning_rate": 0.00016392558641143166,
      "loss": 1.3716,
      "step": 720
    },
    {
      "epoch": 0.5828925042419403,
      "grad_norm": 0.5324034690856934,
      "learning_rate": 0.0001633863575087625,
      "loss": 1.2638,
      "step": 730
    },
    {
      "epoch": 0.5908773330671724,
      "grad_norm": 0.5567993521690369,
      "learning_rate": 0.0001628471286060933,
      "loss": 1.3757,
      "step": 740
    },
    {
      "epoch": 0.5988621618924044,
      "grad_norm": 0.5553314685821533,
      "learning_rate": 0.0001623078997034241,
      "loss": 1.204,
      "step": 750
    },
    {
      "epoch": 0.6068469907176365,
      "grad_norm": 0.5847036242485046,
      "learning_rate": 0.00016176867080075494,
      "loss": 1.3961,
      "step": 760
    },
    {
      "epoch": 0.6148318195428686,
      "grad_norm": 0.5879939198493958,
      "learning_rate": 0.00016122944189808576,
      "loss": 1.241,
      "step": 770
    },
    {
      "epoch": 0.6228166483681006,
      "grad_norm": 0.6125561594963074,
      "learning_rate": 0.00016069021299541656,
      "loss": 1.4823,
      "step": 780
    },
    {
      "epoch": 0.6308014771933327,
      "grad_norm": 0.5341528058052063,
      "learning_rate": 0.00016015098409274739,
      "loss": 1.4964,
      "step": 790
    },
    {
      "epoch": 0.6387863060185647,
      "grad_norm": 0.569900393486023,
      "learning_rate": 0.0001596117551900782,
      "loss": 1.3134,
      "step": 800
    },
    {
      "epoch": 0.6467711348437968,
      "grad_norm": 0.6958159804344177,
      "learning_rate": 0.000159072526287409,
      "loss": 1.3937,
      "step": 810
    },
    {
      "epoch": 0.6547559636690289,
      "grad_norm": 0.6519637703895569,
      "learning_rate": 0.00015853329738473983,
      "loss": 1.2053,
      "step": 820
    },
    {
      "epoch": 0.6627407924942609,
      "grad_norm": 0.6284202337265015,
      "learning_rate": 0.00015799406848207066,
      "loss": 1.3381,
      "step": 830
    },
    {
      "epoch": 0.670725621319493,
      "grad_norm": 0.6377053260803223,
      "learning_rate": 0.00015745483957940146,
      "loss": 1.4078,
      "step": 840
    },
    {
      "epoch": 0.678710450144725,
      "grad_norm": 0.6328989863395691,
      "learning_rate": 0.00015691561067673228,
      "loss": 1.4839,
      "step": 850
    },
    {
      "epoch": 0.6866952789699571,
      "grad_norm": 0.5344203114509583,
      "learning_rate": 0.0001563763817740631,
      "loss": 1.2686,
      "step": 860
    },
    {
      "epoch": 0.6946801077951892,
      "grad_norm": 0.5249454975128174,
      "learning_rate": 0.0001558371528713939,
      "loss": 1.3581,
      "step": 870
    },
    {
      "epoch": 0.7026649366204212,
      "grad_norm": 0.6128615736961365,
      "learning_rate": 0.00015529792396872473,
      "loss": 1.25,
      "step": 880
    },
    {
      "epoch": 0.7106497654456533,
      "grad_norm": 0.5318364500999451,
      "learning_rate": 0.00015475869506605556,
      "loss": 1.3219,
      "step": 890
    },
    {
      "epoch": 0.7186345942708853,
      "grad_norm": 0.472660094499588,
      "learning_rate": 0.00015421946616338636,
      "loss": 1.3927,
      "step": 900
    },
    {
      "epoch": 0.7266194230961174,
      "grad_norm": 0.6231308579444885,
      "learning_rate": 0.00015368023726071718,
      "loss": 1.5223,
      "step": 910
    },
    {
      "epoch": 0.7346042519213495,
      "grad_norm": 0.5580579042434692,
      "learning_rate": 0.00015314100835804798,
      "loss": 1.4372,
      "step": 920
    },
    {
      "epoch": 0.7425890807465815,
      "grad_norm": 0.5752753615379333,
      "learning_rate": 0.0001526017794553788,
      "loss": 1.3683,
      "step": 930
    },
    {
      "epoch": 0.7505739095718136,
      "grad_norm": 0.5004637837409973,
      "learning_rate": 0.00015206255055270963,
      "loss": 1.4012,
      "step": 940
    },
    {
      "epoch": 0.7585587383970456,
      "grad_norm": 0.5533067584037781,
      "learning_rate": 0.00015152332165004043,
      "loss": 1.4606,
      "step": 950
    },
    {
      "epoch": 0.7665435672222777,
      "grad_norm": 0.5819045901298523,
      "learning_rate": 0.00015098409274737126,
      "loss": 1.4591,
      "step": 960
    },
    {
      "epoch": 0.7745283960475098,
      "grad_norm": 0.6172547936439514,
      "learning_rate": 0.00015044486384470208,
      "loss": 1.4335,
      "step": 970
    },
    {
      "epoch": 0.7825132248727418,
      "grad_norm": 0.6497629880905151,
      "learning_rate": 0.00014990563494203288,
      "loss": 1.3392,
      "step": 980
    },
    {
      "epoch": 0.7904980536979739,
      "grad_norm": 0.5011788010597229,
      "learning_rate": 0.0001493664060393637,
      "loss": 1.5313,
      "step": 990
    },
    {
      "epoch": 0.7984828825232059,
      "grad_norm": 0.516576886177063,
      "learning_rate": 0.00014882717713669453,
      "loss": 1.2808,
      "step": 1000
    },
    {
      "epoch": 0.806467711348438,
      "grad_norm": 0.6078372001647949,
      "learning_rate": 0.00014828794823402536,
      "loss": 1.5167,
      "step": 1010
    },
    {
      "epoch": 0.8144525401736701,
      "grad_norm": 0.6907067894935608,
      "learning_rate": 0.00014774871933135615,
      "loss": 1.3547,
      "step": 1020
    },
    {
      "epoch": 0.8224373689989021,
      "grad_norm": 0.5622528195381165,
      "learning_rate": 0.00014720949042868698,
      "loss": 1.5332,
      "step": 1030
    },
    {
      "epoch": 0.8304221978241342,
      "grad_norm": 0.6292944550514221,
      "learning_rate": 0.0001466702615260178,
      "loss": 1.3967,
      "step": 1040
    },
    {
      "epoch": 0.8384070266493662,
      "grad_norm": 0.5321279764175415,
      "learning_rate": 0.00014613103262334863,
      "loss": 1.4094,
      "step": 1050
    },
    {
      "epoch": 0.8463918554745983,
      "grad_norm": 0.5213555693626404,
      "learning_rate": 0.00014559180372067946,
      "loss": 1.3922,
      "step": 1060
    },
    {
      "epoch": 0.8543766842998303,
      "grad_norm": 0.5296475291252136,
      "learning_rate": 0.00014505257481801025,
      "loss": 1.2865,
      "step": 1070
    },
    {
      "epoch": 0.8623615131250624,
      "grad_norm": 8.505202293395996,
      "learning_rate": 0.00014451334591534108,
      "loss": 1.3887,
      "step": 1080
    },
    {
      "epoch": 0.8703463419502945,
      "grad_norm": 0.5957502722740173,
      "learning_rate": 0.0001439741170126719,
      "loss": 1.3651,
      "step": 1090
    },
    {
      "epoch": 0.8783311707755265,
      "grad_norm": 0.6026617884635925,
      "learning_rate": 0.0001434348881100027,
      "loss": 1.3288,
      "step": 1100
    },
    {
      "epoch": 0.8863159996007586,
      "grad_norm": 0.6780215501785278,
      "learning_rate": 0.00014289565920733353,
      "loss": 1.5329,
      "step": 1110
    },
    {
      "epoch": 0.8943008284259906,
      "grad_norm": 0.5692557692527771,
      "learning_rate": 0.00014235643030466435,
      "loss": 1.3283,
      "step": 1120
    },
    {
      "epoch": 0.9022856572512227,
      "grad_norm": 0.6560620069503784,
      "learning_rate": 0.00014181720140199515,
      "loss": 1.2733,
      "step": 1130
    },
    {
      "epoch": 0.9102704860764548,
      "grad_norm": 0.43386736512184143,
      "learning_rate": 0.00014127797249932598,
      "loss": 1.3017,
      "step": 1140
    },
    {
      "epoch": 0.9182553149016868,
      "grad_norm": 0.5525165796279907,
      "learning_rate": 0.0001407387435966568,
      "loss": 1.394,
      "step": 1150
    },
    {
      "epoch": 0.9262401437269189,
      "grad_norm": 0.5369125008583069,
      "learning_rate": 0.0001401995146939876,
      "loss": 1.3632,
      "step": 1160
    },
    {
      "epoch": 0.9342249725521509,
      "grad_norm": 0.5351528525352478,
      "learning_rate": 0.00013966028579131843,
      "loss": 1.3146,
      "step": 1170
    },
    {
      "epoch": 0.942209801377383,
      "grad_norm": 0.5853438973426819,
      "learning_rate": 0.00013912105688864925,
      "loss": 1.3736,
      "step": 1180
    },
    {
      "epoch": 0.9501946302026151,
      "grad_norm": 0.6114912629127502,
      "learning_rate": 0.00013858182798598005,
      "loss": 1.5406,
      "step": 1190
    },
    {
      "epoch": 0.9581794590278471,
      "grad_norm": 0.5585324168205261,
      "learning_rate": 0.00013804259908331088,
      "loss": 1.4754,
      "step": 1200
    },
    {
      "epoch": 0.9661642878530792,
      "grad_norm": 0.45689594745635986,
      "learning_rate": 0.0001375033701806417,
      "loss": 1.2081,
      "step": 1210
    },
    {
      "epoch": 0.9741491166783112,
      "grad_norm": 0.6063959002494812,
      "learning_rate": 0.0001369641412779725,
      "loss": 1.3598,
      "step": 1220
    },
    {
      "epoch": 0.9821339455035433,
      "grad_norm": 0.5570387244224548,
      "learning_rate": 0.00013642491237530333,
      "loss": 1.3172,
      "step": 1230
    },
    {
      "epoch": 0.9901187743287754,
      "grad_norm": 0.4581911861896515,
      "learning_rate": 0.00013588568347263412,
      "loss": 1.3649,
      "step": 1240
    },
    {
      "epoch": 0.9981036031540074,
      "grad_norm": 0.5677453875541687,
      "learning_rate": 0.00013534645456996495,
      "loss": 1.4041,
      "step": 1250
    },
    {
      "epoch": 1.0055893801776625,
      "grad_norm": 0.6095356941223145,
      "learning_rate": 0.00013480722566729578,
      "loss": 1.2775,
      "step": 1260
    },
    {
      "epoch": 1.0135742090028945,
      "grad_norm": 0.6120182871818542,
      "learning_rate": 0.00013426799676462657,
      "loss": 1.2553,
      "step": 1270
    },
    {
      "epoch": 1.0215590378281265,
      "grad_norm": 0.46213725209236145,
      "learning_rate": 0.0001337287678619574,
      "loss": 1.3845,
      "step": 1280
    },
    {
      "epoch": 1.0295438666533587,
      "grad_norm": 0.5179557800292969,
      "learning_rate": 0.00013318953895928822,
      "loss": 1.2771,
      "step": 1290
    },
    {
      "epoch": 1.0375286954785907,
      "grad_norm": 0.5412089824676514,
      "learning_rate": 0.00013265031005661902,
      "loss": 1.3922,
      "step": 1300
    },
    {
      "epoch": 1.0455135243038227,
      "grad_norm": 0.6219887733459473,
      "learning_rate": 0.00013211108115394985,
      "loss": 1.2108,
      "step": 1310
    },
    {
      "epoch": 1.0534983531290547,
      "grad_norm": 0.6621530652046204,
      "learning_rate": 0.00013157185225128067,
      "loss": 1.2697,
      "step": 1320
    },
    {
      "epoch": 1.061483181954287,
      "grad_norm": 0.5545846223831177,
      "learning_rate": 0.00013103262334861147,
      "loss": 1.2529,
      "step": 1330
    },
    {
      "epoch": 1.069468010779519,
      "grad_norm": 0.4894459843635559,
      "learning_rate": 0.0001304933944459423,
      "loss": 1.4021,
      "step": 1340
    },
    {
      "epoch": 1.077452839604751,
      "grad_norm": 0.5440306663513184,
      "learning_rate": 0.00012995416554327312,
      "loss": 1.2109,
      "step": 1350
    },
    {
      "epoch": 1.0854376684299831,
      "grad_norm": 0.5752737522125244,
      "learning_rate": 0.00012941493664060392,
      "loss": 1.2788,
      "step": 1360
    },
    {
      "epoch": 1.0934224972552151,
      "grad_norm": 0.5831282734870911,
      "learning_rate": 0.00012887570773793475,
      "loss": 1.42,
      "step": 1370
    },
    {
      "epoch": 1.1014073260804471,
      "grad_norm": 1.0200825929641724,
      "learning_rate": 0.00012833647883526557,
      "loss": 1.4548,
      "step": 1380
    },
    {
      "epoch": 1.109392154905679,
      "grad_norm": 0.6051325798034668,
      "learning_rate": 0.0001277972499325964,
      "loss": 1.2437,
      "step": 1390
    },
    {
      "epoch": 1.1173769837309113,
      "grad_norm": 9.018731117248535,
      "learning_rate": 0.00012725802102992722,
      "loss": 1.3685,
      "step": 1400
    },
    {
      "epoch": 1.1253618125561433,
      "grad_norm": 0.5370739102363586,
      "learning_rate": 0.00012671879212725802,
      "loss": 1.3581,
      "step": 1410
    },
    {
      "epoch": 1.1333466413813753,
      "grad_norm": 0.5509143471717834,
      "learning_rate": 0.00012617956322458885,
      "loss": 1.3644,
      "step": 1420
    },
    {
      "epoch": 1.1413314702066075,
      "grad_norm": 0.557280957698822,
      "learning_rate": 0.00012564033432191967,
      "loss": 1.4507,
      "step": 1430
    },
    {
      "epoch": 1.1493162990318395,
      "grad_norm": 0.4904331862926483,
      "learning_rate": 0.0001251011054192505,
      "loss": 1.3545,
      "step": 1440
    },
    {
      "epoch": 1.1573011278570715,
      "grad_norm": 0.4729349911212921,
      "learning_rate": 0.0001245618765165813,
      "loss": 1.2266,
      "step": 1450
    },
    {
      "epoch": 1.1652859566823035,
      "grad_norm": 0.7131232619285583,
      "learning_rate": 0.00012402264761391212,
      "loss": 1.4558,
      "step": 1460
    },
    {
      "epoch": 1.1732707855075357,
      "grad_norm": 0.6793272495269775,
      "learning_rate": 0.00012348341871124295,
      "loss": 1.2491,
      "step": 1470
    },
    {
      "epoch": 1.1812556143327677,
      "grad_norm": 0.5832497477531433,
      "learning_rate": 0.00012294418980857375,
      "loss": 1.1842,
      "step": 1480
    },
    {
      "epoch": 1.1892404431579997,
      "grad_norm": 0.6260777711868286,
      "learning_rate": 0.00012240496090590457,
      "loss": 1.4369,
      "step": 1490
    },
    {
      "epoch": 1.197225271983232,
      "grad_norm": 0.6669915914535522,
      "learning_rate": 0.00012186573200323538,
      "loss": 1.3779,
      "step": 1500
    },
    {
      "epoch": 1.205210100808464,
      "grad_norm": 0.6206994652748108,
      "learning_rate": 0.0001213265031005662,
      "loss": 1.284,
      "step": 1510
    },
    {
      "epoch": 1.213194929633696,
      "grad_norm": 0.6103357076644897,
      "learning_rate": 0.00012078727419789702,
      "loss": 1.3122,
      "step": 1520
    },
    {
      "epoch": 1.2211797584589281,
      "grad_norm": 0.6984562277793884,
      "learning_rate": 0.00012024804529522785,
      "loss": 1.4824,
      "step": 1530
    },
    {
      "epoch": 1.2291645872841601,
      "grad_norm": 0.5091553926467896,
      "learning_rate": 0.00011970881639255864,
      "loss": 1.3634,
      "step": 1540
    },
    {
      "epoch": 1.237149416109392,
      "grad_norm": 0.5289799571037292,
      "learning_rate": 0.00011916958748988947,
      "loss": 1.3415,
      "step": 1550
    },
    {
      "epoch": 1.2451342449346243,
      "grad_norm": 0.6743884086608887,
      "learning_rate": 0.0001186303585872203,
      "loss": 1.2947,
      "step": 1560
    },
    {
      "epoch": 1.2531190737598563,
      "grad_norm": 0.5726363062858582,
      "learning_rate": 0.0001180911296845511,
      "loss": 1.3247,
      "step": 1570
    },
    {
      "epoch": 1.2611039025850883,
      "grad_norm": 0.48835378885269165,
      "learning_rate": 0.00011755190078188192,
      "loss": 1.4096,
      "step": 1580
    },
    {
      "epoch": 1.2690887314103203,
      "grad_norm": 0.5842453837394714,
      "learning_rate": 0.00011701267187921272,
      "loss": 1.2433,
      "step": 1590
    },
    {
      "epoch": 1.2770735602355525,
      "grad_norm": 0.560939371585846,
      "learning_rate": 0.00011647344297654354,
      "loss": 1.3949,
      "step": 1600
    },
    {
      "epoch": 1.2850583890607845,
      "grad_norm": 0.5328729748725891,
      "learning_rate": 0.00011593421407387437,
      "loss": 1.2895,
      "step": 1610
    },
    {
      "epoch": 1.2930432178860165,
      "grad_norm": 0.5779713988304138,
      "learning_rate": 0.00011539498517120517,
      "loss": 1.3031,
      "step": 1620
    },
    {
      "epoch": 1.3010280467112487,
      "grad_norm": 0.6379626989364624,
      "learning_rate": 0.00011485575626853599,
      "loss": 1.5824,
      "step": 1630
    },
    {
      "epoch": 1.3090128755364807,
      "grad_norm": 0.5998775959014893,
      "learning_rate": 0.00011431652736586682,
      "loss": 1.4097,
      "step": 1640
    },
    {
      "epoch": 1.3169977043617127,
      "grad_norm": 0.4752112030982971,
      "learning_rate": 0.00011377729846319763,
      "loss": 1.2022,
      "step": 1650
    },
    {
      "epoch": 1.3249825331869447,
      "grad_norm": 0.6511373519897461,
      "learning_rate": 0.00011323806956052844,
      "loss": 1.4069,
      "step": 1660
    },
    {
      "epoch": 1.332967362012177,
      "grad_norm": 0.5803982615470886,
      "learning_rate": 0.00011269884065785927,
      "loss": 1.299,
      "step": 1670
    },
    {
      "epoch": 1.340952190837409,
      "grad_norm": 0.5963691473007202,
      "learning_rate": 0.00011215961175519008,
      "loss": 1.2688,
      "step": 1680
    },
    {
      "epoch": 1.348937019662641,
      "grad_norm": 0.6141477227210999,
      "learning_rate": 0.0001116203828525209,
      "loss": 1.2891,
      "step": 1690
    },
    {
      "epoch": 1.3569218484878731,
      "grad_norm": 0.7071518898010254,
      "learning_rate": 0.00011108115394985173,
      "loss": 1.4177,
      "step": 1700
    },
    {
      "epoch": 1.364906677313105,
      "grad_norm": 0.5811296105384827,
      "learning_rate": 0.00011054192504718253,
      "loss": 1.4208,
      "step": 1710
    },
    {
      "epoch": 1.372891506138337,
      "grad_norm": 0.5530140399932861,
      "learning_rate": 0.00011000269614451335,
      "loss": 1.4492,
      "step": 1720
    },
    {
      "epoch": 1.380876334963569,
      "grad_norm": 0.570385754108429,
      "learning_rate": 0.00010946346724184418,
      "loss": 1.2989,
      "step": 1730
    },
    {
      "epoch": 1.3888611637888013,
      "grad_norm": 0.6752719283103943,
      "learning_rate": 0.00010892423833917498,
      "loss": 1.3841,
      "step": 1740
    },
    {
      "epoch": 1.3968459926140333,
      "grad_norm": 0.6142017245292664,
      "learning_rate": 0.0001083850094365058,
      "loss": 1.5161,
      "step": 1750
    },
    {
      "epoch": 1.4048308214392655,
      "grad_norm": 0.6040313839912415,
      "learning_rate": 0.00010784578053383663,
      "loss": 1.303,
      "step": 1760
    },
    {
      "epoch": 1.4128156502644975,
      "grad_norm": 0.59196537733078,
      "learning_rate": 0.00010730655163116743,
      "loss": 1.5044,
      "step": 1770
    },
    {
      "epoch": 1.4208004790897295,
      "grad_norm": 0.566741943359375,
      "learning_rate": 0.00010676732272849825,
      "loss": 1.2814,
      "step": 1780
    },
    {
      "epoch": 1.4287853079149615,
      "grad_norm": 0.6441860198974609,
      "learning_rate": 0.00010622809382582908,
      "loss": 1.2562,
      "step": 1790
    },
    {
      "epoch": 1.4367701367401937,
      "grad_norm": 0.5717202425003052,
      "learning_rate": 0.00010568886492315988,
      "loss": 1.3298,
      "step": 1800
    },
    {
      "epoch": 1.4447549655654257,
      "grad_norm": 0.8148512840270996,
      "learning_rate": 0.0001051496360204907,
      "loss": 1.3851,
      "step": 1810
    },
    {
      "epoch": 1.4527397943906577,
      "grad_norm": 0.6749821305274963,
      "learning_rate": 0.00010461040711782153,
      "loss": 1.3824,
      "step": 1820
    },
    {
      "epoch": 1.46072462321589,
      "grad_norm": 0.7475818991661072,
      "learning_rate": 0.00010407117821515233,
      "loss": 1.334,
      "step": 1830
    },
    {
      "epoch": 1.468709452041122,
      "grad_norm": 0.5963310599327087,
      "learning_rate": 0.00010353194931248315,
      "loss": 1.2631,
      "step": 1840
    },
    {
      "epoch": 1.476694280866354,
      "grad_norm": 0.5797064304351807,
      "learning_rate": 0.00010299272040981398,
      "loss": 1.2296,
      "step": 1850
    },
    {
      "epoch": 1.484679109691586,
      "grad_norm": 0.5985116362571716,
      "learning_rate": 0.00010245349150714479,
      "loss": 1.4641,
      "step": 1860
    },
    {
      "epoch": 1.4926639385168181,
      "grad_norm": 0.5925047993659973,
      "learning_rate": 0.00010191426260447561,
      "loss": 1.3399,
      "step": 1870
    },
    {
      "epoch": 1.50064876734205,
      "grad_norm": 0.699285626411438,
      "learning_rate": 0.00010137503370180643,
      "loss": 1.4351,
      "step": 1880
    },
    {
      "epoch": 1.5086335961672823,
      "grad_norm": 0.5479611754417419,
      "learning_rate": 0.00010083580479913724,
      "loss": 1.2488,
      "step": 1890
    },
    {
      "epoch": 1.5166184249925143,
      "grad_norm": 1.2852829694747925,
      "learning_rate": 0.00010029657589646806,
      "loss": 1.3332,
      "step": 1900
    },
    {
      "epoch": 1.5246032538177463,
      "grad_norm": 0.6117525100708008,
      "learning_rate": 9.975734699379887e-05,
      "loss": 1.434,
      "step": 1910
    },
    {
      "epoch": 1.5325880826429783,
      "grad_norm": 0.5405614972114563,
      "learning_rate": 9.921811809112969e-05,
      "loss": 1.2369,
      "step": 1920
    },
    {
      "epoch": 1.5405729114682103,
      "grad_norm": 0.6774101853370667,
      "learning_rate": 9.867888918846051e-05,
      "loss": 1.4188,
      "step": 1930
    },
    {
      "epoch": 1.5485577402934425,
      "grad_norm": 0.6806389689445496,
      "learning_rate": 9.813966028579132e-05,
      "loss": 1.4828,
      "step": 1940
    },
    {
      "epoch": 1.5565425691186745,
      "grad_norm": 0.6367496848106384,
      "learning_rate": 9.760043138312214e-05,
      "loss": 1.4288,
      "step": 1950
    },
    {
      "epoch": 1.5645273979439067,
      "grad_norm": 0.6434332728385925,
      "learning_rate": 9.706120248045296e-05,
      "loss": 1.4145,
      "step": 1960
    },
    {
      "epoch": 1.5725122267691387,
      "grad_norm": 0.5477311015129089,
      "learning_rate": 9.652197357778377e-05,
      "loss": 1.2632,
      "step": 1970
    },
    {
      "epoch": 1.5804970555943707,
      "grad_norm": 0.5979363322257996,
      "learning_rate": 9.598274467511459e-05,
      "loss": 1.2671,
      "step": 1980
    },
    {
      "epoch": 1.5884818844196027,
      "grad_norm": 0.6557675004005432,
      "learning_rate": 9.54435157724454e-05,
      "loss": 1.3074,
      "step": 1990
    },
    {
      "epoch": 1.5964667132448347,
      "grad_norm": 0.5532064437866211,
      "learning_rate": 9.490428686977622e-05,
      "loss": 1.4764,
      "step": 2000
    },
    {
      "epoch": 1.604451542070067,
      "grad_norm": 0.5874671936035156,
      "learning_rate": 9.436505796710703e-05,
      "loss": 1.3399,
      "step": 2010
    },
    {
      "epoch": 1.612436370895299,
      "grad_norm": 0.691112220287323,
      "learning_rate": 9.382582906443786e-05,
      "loss": 1.4771,
      "step": 2020
    },
    {
      "epoch": 1.6204211997205311,
      "grad_norm": 0.649076521396637,
      "learning_rate": 9.328660016176867e-05,
      "loss": 1.3332,
      "step": 2030
    },
    {
      "epoch": 1.628406028545763,
      "grad_norm": 0.5597956776618958,
      "learning_rate": 9.27473712590995e-05,
      "loss": 1.4531,
      "step": 2040
    },
    {
      "epoch": 1.636390857370995,
      "grad_norm": 0.5729061961174011,
      "learning_rate": 9.220814235643031e-05,
      "loss": 1.354,
      "step": 2050
    },
    {
      "epoch": 1.644375686196227,
      "grad_norm": 0.6226245164871216,
      "learning_rate": 9.166891345376113e-05,
      "loss": 1.4194,
      "step": 2060
    },
    {
      "epoch": 1.652360515021459,
      "grad_norm": 0.5613691806793213,
      "learning_rate": 9.112968455109195e-05,
      "loss": 1.2133,
      "step": 2070
    },
    {
      "epoch": 1.6603453438466913,
      "grad_norm": 0.6045340895652771,
      "learning_rate": 9.059045564842276e-05,
      "loss": 1.3991,
      "step": 2080
    },
    {
      "epoch": 1.6683301726719233,
      "grad_norm": 0.5959174036979675,
      "learning_rate": 9.005122674575358e-05,
      "loss": 1.4871,
      "step": 2090
    },
    {
      "epoch": 1.6763150014971555,
      "grad_norm": 0.6648789644241333,
      "learning_rate": 8.95119978430844e-05,
      "loss": 1.4048,
      "step": 2100
    },
    {
      "epoch": 1.6842998303223875,
      "grad_norm": 0.688031017780304,
      "learning_rate": 8.897276894041521e-05,
      "loss": 1.3342,
      "step": 2110
    },
    {
      "epoch": 1.6922846591476195,
      "grad_norm": 0.5803268551826477,
      "learning_rate": 8.843354003774603e-05,
      "loss": 1.3568,
      "step": 2120
    },
    {
      "epoch": 1.7002694879728515,
      "grad_norm": 0.6166363954544067,
      "learning_rate": 8.789431113507685e-05,
      "loss": 1.3621,
      "step": 2130
    },
    {
      "epoch": 1.7082543167980835,
      "grad_norm": 0.5269085168838501,
      "learning_rate": 8.735508223240766e-05,
      "loss": 1.2143,
      "step": 2140
    },
    {
      "epoch": 1.7162391456233157,
      "grad_norm": 0.5124291777610779,
      "learning_rate": 8.681585332973848e-05,
      "loss": 1.484,
      "step": 2150
    },
    {
      "epoch": 1.724223974448548,
      "grad_norm": 0.6161434054374695,
      "learning_rate": 8.62766244270693e-05,
      "loss": 1.5044,
      "step": 2160
    },
    {
      "epoch": 1.73220880327378,
      "grad_norm": 0.5798738598823547,
      "learning_rate": 8.57373955244001e-05,
      "loss": 1.3447,
      "step": 2170
    },
    {
      "epoch": 1.740193632099012,
      "grad_norm": 0.6339479088783264,
      "learning_rate": 8.519816662173092e-05,
      "loss": 1.5164,
      "step": 2180
    },
    {
      "epoch": 1.748178460924244,
      "grad_norm": 0.6442420482635498,
      "learning_rate": 8.465893771906174e-05,
      "loss": 1.2942,
      "step": 2190
    },
    {
      "epoch": 1.7561632897494759,
      "grad_norm": 0.5972416996955872,
      "learning_rate": 8.411970881639256e-05,
      "loss": 1.2688,
      "step": 2200
    },
    {
      "epoch": 1.764148118574708,
      "grad_norm": 0.632719099521637,
      "learning_rate": 8.358047991372338e-05,
      "loss": 1.3357,
      "step": 2210
    },
    {
      "epoch": 1.77213294739994,
      "grad_norm": 0.625564455986023,
      "learning_rate": 8.304125101105419e-05,
      "loss": 1.3218,
      "step": 2220
    },
    {
      "epoch": 1.7801177762251723,
      "grad_norm": 0.6606918573379517,
      "learning_rate": 8.250202210838502e-05,
      "loss": 1.2956,
      "step": 2230
    },
    {
      "epoch": 1.7881026050504043,
      "grad_norm": 0.5637531280517578,
      "learning_rate": 8.196279320571583e-05,
      "loss": 1.2879,
      "step": 2240
    },
    {
      "epoch": 1.7960874338756363,
      "grad_norm": 0.763577401638031,
      "learning_rate": 8.142356430304666e-05,
      "loss": 1.2809,
      "step": 2250
    },
    {
      "epoch": 1.8040722627008683,
      "grad_norm": 0.5536795854568481,
      "learning_rate": 8.088433540037747e-05,
      "loss": 1.3718,
      "step": 2260
    },
    {
      "epoch": 1.8120570915261003,
      "grad_norm": 0.6988975405693054,
      "learning_rate": 8.034510649770828e-05,
      "loss": 1.4968,
      "step": 2270
    },
    {
      "epoch": 1.8200419203513325,
      "grad_norm": 0.6213294863700867,
      "learning_rate": 7.98058775950391e-05,
      "loss": 1.4034,
      "step": 2280
    },
    {
      "epoch": 1.8280267491765645,
      "grad_norm": 0.6076768040657043,
      "learning_rate": 7.926664869236992e-05,
      "loss": 1.4307,
      "step": 2290
    },
    {
      "epoch": 1.8360115780017967,
      "grad_norm": 0.6317535042762756,
      "learning_rate": 7.872741978970073e-05,
      "loss": 1.5285,
      "step": 2300
    },
    {
      "epoch": 1.8439964068270287,
      "grad_norm": 0.6205063462257385,
      "learning_rate": 7.818819088703155e-05,
      "loss": 1.48,
      "step": 2310
    },
    {
      "epoch": 1.8519812356522607,
      "grad_norm": 0.704107940196991,
      "learning_rate": 7.764896198436237e-05,
      "loss": 1.3885,
      "step": 2320
    },
    {
      "epoch": 1.8599660644774927,
      "grad_norm": 0.5968559384346008,
      "learning_rate": 7.710973308169318e-05,
      "loss": 1.3131,
      "step": 2330
    },
    {
      "epoch": 1.8679508933027247,
      "grad_norm": 0.42593416571617126,
      "learning_rate": 7.657050417902399e-05,
      "loss": 1.2274,
      "step": 2340
    },
    {
      "epoch": 1.875935722127957,
      "grad_norm": 0.6827232241630554,
      "learning_rate": 7.603127527635482e-05,
      "loss": 1.2818,
      "step": 2350
    },
    {
      "epoch": 1.8839205509531889,
      "grad_norm": 0.6135967373847961,
      "learning_rate": 7.549204637368563e-05,
      "loss": 1.2467,
      "step": 2360
    },
    {
      "epoch": 1.891905379778421,
      "grad_norm": 0.7380168437957764,
      "learning_rate": 7.495281747101644e-05,
      "loss": 1.3388,
      "step": 2370
    },
    {
      "epoch": 1.899890208603653,
      "grad_norm": 0.5928619503974915,
      "learning_rate": 7.441358856834727e-05,
      "loss": 1.3988,
      "step": 2380
    },
    {
      "epoch": 1.907875037428885,
      "grad_norm": 0.536871612071991,
      "learning_rate": 7.387435966567808e-05,
      "loss": 1.402,
      "step": 2390
    },
    {
      "epoch": 1.915859866254117,
      "grad_norm": 0.5911885499954224,
      "learning_rate": 7.33351307630089e-05,
      "loss": 1.4461,
      "step": 2400
    },
    {
      "epoch": 1.923844695079349,
      "grad_norm": 0.649080216884613,
      "learning_rate": 7.279590186033973e-05,
      "loss": 1.3446,
      "step": 2410
    },
    {
      "epoch": 1.9318295239045813,
      "grad_norm": 0.5395387411117554,
      "learning_rate": 7.225667295767054e-05,
      "loss": 1.1969,
      "step": 2420
    },
    {
      "epoch": 1.9398143527298135,
      "grad_norm": 0.5785351991653442,
      "learning_rate": 7.171744405500135e-05,
      "loss": 1.2188,
      "step": 2430
    },
    {
      "epoch": 1.9477991815550455,
      "grad_norm": 0.6263803839683533,
      "learning_rate": 7.117821515233218e-05,
      "loss": 1.2824,
      "step": 2440
    },
    {
      "epoch": 1.9557840103802775,
      "grad_norm": 0.6148039102554321,
      "learning_rate": 7.063898624966299e-05,
      "loss": 1.357,
      "step": 2450
    },
    {
      "epoch": 1.9637688392055095,
      "grad_norm": 0.6499695181846619,
      "learning_rate": 7.00997573469938e-05,
      "loss": 1.3201,
      "step": 2460
    },
    {
      "epoch": 1.9717536680307415,
      "grad_norm": 0.695311963558197,
      "learning_rate": 6.956052844432463e-05,
      "loss": 1.3533,
      "step": 2470
    },
    {
      "epoch": 1.9797384968559737,
      "grad_norm": 0.6847599148750305,
      "learning_rate": 6.902129954165544e-05,
      "loss": 1.4023,
      "step": 2480
    },
    {
      "epoch": 1.9877233256812057,
      "grad_norm": 0.6081831455230713,
      "learning_rate": 6.848207063898625e-05,
      "loss": 1.3419,
      "step": 2490
    },
    {
      "epoch": 1.995708154506438,
      "grad_norm": 0.6241475343704224,
      "learning_rate": 6.794284173631706e-05,
      "loss": 1.4263,
      "step": 2500
    }
  ],
  "logging_steps": 10,
  "max_steps": 3759,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5954970181369856e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
