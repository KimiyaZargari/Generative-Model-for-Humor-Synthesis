{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2402,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0083281282531751,
      "grad_norm": 23.824613571166992,
      "learning_rate": 1.8e-05,
      "loss": 11.6,
      "step": 10
    },
    {
      "epoch": 0.0166562565063502,
      "grad_norm": 45.750244140625,
      "learning_rate": 3.8e-05,
      "loss": 10.5985,
      "step": 20
    },
    {
      "epoch": 0.024984384759525295,
      "grad_norm": 20.440067291259766,
      "learning_rate": 5.8e-05,
      "loss": 6.3897,
      "step": 30
    },
    {
      "epoch": 0.0333125130127004,
      "grad_norm": 15.120610237121582,
      "learning_rate": 7.800000000000001e-05,
      "loss": 3.6727,
      "step": 40
    },
    {
      "epoch": 0.041640641265875494,
      "grad_norm": 1.9982540607452393,
      "learning_rate": 9.8e-05,
      "loss": 1.7822,
      "step": 50
    },
    {
      "epoch": 0.04996876951905059,
      "grad_norm": 1.4120821952819824,
      "learning_rate": 0.000118,
      "loss": 1.1766,
      "step": 60
    },
    {
      "epoch": 0.05829689777222569,
      "grad_norm": 0.9169183969497681,
      "learning_rate": 0.000138,
      "loss": 0.9723,
      "step": 70
    },
    {
      "epoch": 0.0666250260254008,
      "grad_norm": 0.8249570727348328,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.896,
      "step": 80
    },
    {
      "epoch": 0.07495315427857589,
      "grad_norm": 0.7147775292396545,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.8131,
      "step": 90
    },
    {
      "epoch": 0.08328128253175099,
      "grad_norm": 0.7982261180877686,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.8204,
      "step": 100
    },
    {
      "epoch": 0.09160941078492608,
      "grad_norm": 0.8132607340812683,
      "learning_rate": 0.00019948615472452184,
      "loss": 0.8226,
      "step": 110
    },
    {
      "epoch": 0.09993753903810118,
      "grad_norm": 0.7513646483421326,
      "learning_rate": 0.00019891521552954612,
      "loss": 0.8209,
      "step": 120
    },
    {
      "epoch": 0.10826566729127629,
      "grad_norm": 0.8384955525398254,
      "learning_rate": 0.0001983442763345704,
      "loss": 0.8297,
      "step": 130
    },
    {
      "epoch": 0.11659379554445139,
      "grad_norm": 0.8373925685882568,
      "learning_rate": 0.00019777333713959466,
      "loss": 0.8689,
      "step": 140
    },
    {
      "epoch": 0.12492192379762648,
      "grad_norm": 0.8194049000740051,
      "learning_rate": 0.0001972023979446189,
      "loss": 0.8003,
      "step": 150
    },
    {
      "epoch": 0.1332500520508016,
      "grad_norm": 0.7251324653625488,
      "learning_rate": 0.00019663145874964318,
      "loss": 0.7982,
      "step": 160
    },
    {
      "epoch": 0.14157818030397668,
      "grad_norm": 0.7248123288154602,
      "learning_rate": 0.00019606051955466745,
      "loss": 0.8227,
      "step": 170
    },
    {
      "epoch": 0.14990630855715179,
      "grad_norm": 0.58534175157547,
      "learning_rate": 0.0001954895803596917,
      "loss": 0.7469,
      "step": 180
    },
    {
      "epoch": 0.15823443681032687,
      "grad_norm": 0.7220483422279358,
      "learning_rate": 0.00019491864116471597,
      "loss": 0.8149,
      "step": 190
    },
    {
      "epoch": 0.16656256506350198,
      "grad_norm": 0.7826076745986938,
      "learning_rate": 0.00019434770196974024,
      "loss": 0.8098,
      "step": 200
    },
    {
      "epoch": 0.1748906933166771,
      "grad_norm": 0.6980066299438477,
      "learning_rate": 0.0001937767627747645,
      "loss": 0.8058,
      "step": 210
    },
    {
      "epoch": 0.18321882156985217,
      "grad_norm": 0.5895638465881348,
      "learning_rate": 0.00019320582357978876,
      "loss": 0.7887,
      "step": 220
    },
    {
      "epoch": 0.19154694982302728,
      "grad_norm": 0.6908169984817505,
      "learning_rate": 0.00019263488438481304,
      "loss": 0.7975,
      "step": 230
    },
    {
      "epoch": 0.19987507807620236,
      "grad_norm": 0.760324239730835,
      "learning_rate": 0.0001920639451898373,
      "loss": 0.8216,
      "step": 240
    },
    {
      "epoch": 0.20820320632937747,
      "grad_norm": 0.6377607583999634,
      "learning_rate": 0.00019149300599486155,
      "loss": 0.8046,
      "step": 250
    },
    {
      "epoch": 0.21653133458255258,
      "grad_norm": 0.597603440284729,
      "learning_rate": 0.00019092206679988583,
      "loss": 0.768,
      "step": 260
    },
    {
      "epoch": 0.22485946283572766,
      "grad_norm": 0.6276570558547974,
      "learning_rate": 0.0001903511276049101,
      "loss": 0.8114,
      "step": 270
    },
    {
      "epoch": 0.23318759108890277,
      "grad_norm": 0.6776286959648132,
      "learning_rate": 0.00018978018840993434,
      "loss": 0.8007,
      "step": 280
    },
    {
      "epoch": 0.24151571934207786,
      "grad_norm": 0.5963298082351685,
      "learning_rate": 0.00018920924921495862,
      "loss": 0.7718,
      "step": 290
    },
    {
      "epoch": 0.24984384759525297,
      "grad_norm": 0.5905337333679199,
      "learning_rate": 0.0001886383100199829,
      "loss": 0.7912,
      "step": 300
    },
    {
      "epoch": 0.25817197584842805,
      "grad_norm": 0.5971078276634216,
      "learning_rate": 0.00018806737082500714,
      "loss": 0.799,
      "step": 310
    },
    {
      "epoch": 0.2665001041016032,
      "grad_norm": 0.6722333431243896,
      "learning_rate": 0.0001874964316300314,
      "loss": 0.792,
      "step": 320
    },
    {
      "epoch": 0.27482823235477827,
      "grad_norm": 0.6170521974563599,
      "learning_rate": 0.00018692549243505568,
      "loss": 0.818,
      "step": 330
    },
    {
      "epoch": 0.28315636060795335,
      "grad_norm": 0.6387074589729309,
      "learning_rate": 0.00018635455324007993,
      "loss": 0.7744,
      "step": 340
    },
    {
      "epoch": 0.2914844888611285,
      "grad_norm": 0.5736905932426453,
      "learning_rate": 0.0001857836140451042,
      "loss": 0.7607,
      "step": 350
    },
    {
      "epoch": 0.29981261711430357,
      "grad_norm": 0.6516372561454773,
      "learning_rate": 0.00018521267485012847,
      "loss": 0.7978,
      "step": 360
    },
    {
      "epoch": 0.30814074536747865,
      "grad_norm": 0.6283776760101318,
      "learning_rate": 0.00018464173565515272,
      "loss": 0.7782,
      "step": 370
    },
    {
      "epoch": 0.31646887362065373,
      "grad_norm": 0.6114363670349121,
      "learning_rate": 0.000184070796460177,
      "loss": 0.7832,
      "step": 380
    },
    {
      "epoch": 0.32479700187382887,
      "grad_norm": 0.5515420436859131,
      "learning_rate": 0.00018349985726520126,
      "loss": 0.7841,
      "step": 390
    },
    {
      "epoch": 0.33312513012700395,
      "grad_norm": 0.5090296864509583,
      "learning_rate": 0.0001829289180702255,
      "loss": 0.8289,
      "step": 400
    },
    {
      "epoch": 0.34145325838017904,
      "grad_norm": 0.6078847050666809,
      "learning_rate": 0.00018235797887524978,
      "loss": 0.7795,
      "step": 410
    },
    {
      "epoch": 0.3497813866333542,
      "grad_norm": 0.6059353351593018,
      "learning_rate": 0.00018178703968027405,
      "loss": 0.7685,
      "step": 420
    },
    {
      "epoch": 0.35810951488652926,
      "grad_norm": 0.7710912823677063,
      "learning_rate": 0.00018121610048529833,
      "loss": 0.782,
      "step": 430
    },
    {
      "epoch": 0.36643764313970434,
      "grad_norm": 0.5963722467422485,
      "learning_rate": 0.00018064516129032257,
      "loss": 0.7783,
      "step": 440
    },
    {
      "epoch": 0.3747657713928795,
      "grad_norm": 0.5401337146759033,
      "learning_rate": 0.00018007422209534684,
      "loss": 0.7703,
      "step": 450
    },
    {
      "epoch": 0.38309389964605456,
      "grad_norm": 0.5642045140266418,
      "learning_rate": 0.00017950328290037112,
      "loss": 0.7919,
      "step": 460
    },
    {
      "epoch": 0.39142202789922964,
      "grad_norm": 0.5768611431121826,
      "learning_rate": 0.0001789323437053954,
      "loss": 0.7746,
      "step": 470
    },
    {
      "epoch": 0.3997501561524047,
      "grad_norm": 0.7498918771743774,
      "learning_rate": 0.00017836140451041966,
      "loss": 0.7664,
      "step": 480
    },
    {
      "epoch": 0.40807828440557986,
      "grad_norm": 0.5459244251251221,
      "learning_rate": 0.0001777904653154439,
      "loss": 0.7787,
      "step": 490
    },
    {
      "epoch": 0.41640641265875494,
      "grad_norm": 0.5505867004394531,
      "learning_rate": 0.00017721952612046818,
      "loss": 0.7759,
      "step": 500
    },
    {
      "epoch": 0.42473454091193,
      "grad_norm": 0.6023789644241333,
      "learning_rate": 0.00017664858692549245,
      "loss": 0.7886,
      "step": 510
    },
    {
      "epoch": 0.43306266916510516,
      "grad_norm": 0.5613372325897217,
      "learning_rate": 0.00017607764773051673,
      "loss": 0.736,
      "step": 520
    },
    {
      "epoch": 0.44139079741828025,
      "grad_norm": 0.6594305038452148,
      "learning_rate": 0.00017550670853554097,
      "loss": 0.8212,
      "step": 530
    },
    {
      "epoch": 0.4497189256714553,
      "grad_norm": 0.5827054977416992,
      "learning_rate": 0.00017493576934056524,
      "loss": 0.772,
      "step": 540
    },
    {
      "epoch": 0.45804705392463047,
      "grad_norm": 0.5854480266571045,
      "learning_rate": 0.00017436483014558952,
      "loss": 0.7649,
      "step": 550
    },
    {
      "epoch": 0.46637518217780555,
      "grad_norm": 0.6113579869270325,
      "learning_rate": 0.00017379389095061376,
      "loss": 0.7902,
      "step": 560
    },
    {
      "epoch": 0.47470331043098063,
      "grad_norm": 0.5400941967964172,
      "learning_rate": 0.00017322295175563804,
      "loss": 0.8481,
      "step": 570
    },
    {
      "epoch": 0.4830314386841557,
      "grad_norm": 0.5883111357688904,
      "learning_rate": 0.0001726520125606623,
      "loss": 0.7569,
      "step": 580
    },
    {
      "epoch": 0.49135956693733085,
      "grad_norm": 0.58284991979599,
      "learning_rate": 0.00017208107336568658,
      "loss": 0.7511,
      "step": 590
    },
    {
      "epoch": 0.49968769519050593,
      "grad_norm": 0.5242885947227478,
      "learning_rate": 0.00017151013417071083,
      "loss": 0.7849,
      "step": 600
    },
    {
      "epoch": 0.508015823443681,
      "grad_norm": 0.6077029705047607,
      "learning_rate": 0.0001709391949757351,
      "loss": 0.799,
      "step": 610
    },
    {
      "epoch": 0.5163439516968561,
      "grad_norm": 0.49672332406044006,
      "learning_rate": 0.00017036825578075937,
      "loss": 0.7643,
      "step": 620
    },
    {
      "epoch": 0.5246720799500312,
      "grad_norm": 0.5276833176612854,
      "learning_rate": 0.00016979731658578362,
      "loss": 0.8031,
      "step": 630
    },
    {
      "epoch": 0.5330002082032064,
      "grad_norm": 0.5235550999641418,
      "learning_rate": 0.0001692263773908079,
      "loss": 0.8017,
      "step": 640
    },
    {
      "epoch": 0.5413283364563815,
      "grad_norm": 0.603817343711853,
      "learning_rate": 0.00016865543819583216,
      "loss": 0.7745,
      "step": 650
    },
    {
      "epoch": 0.5496564647095565,
      "grad_norm": 0.5543803572654724,
      "learning_rate": 0.0001680844990008564,
      "loss": 0.7715,
      "step": 660
    },
    {
      "epoch": 0.5579845929627316,
      "grad_norm": 0.5668521523475647,
      "learning_rate": 0.00016751355980588068,
      "loss": 0.7449,
      "step": 670
    },
    {
      "epoch": 0.5663127212159067,
      "grad_norm": 0.5429356694221497,
      "learning_rate": 0.00016694262061090495,
      "loss": 0.7604,
      "step": 680
    },
    {
      "epoch": 0.5746408494690818,
      "grad_norm": 0.5166087746620178,
      "learning_rate": 0.0001663716814159292,
      "loss": 0.7623,
      "step": 690
    },
    {
      "epoch": 0.582968977722257,
      "grad_norm": 0.655295193195343,
      "learning_rate": 0.00016580074222095347,
      "loss": 0.7757,
      "step": 700
    },
    {
      "epoch": 0.5912971059754321,
      "grad_norm": 0.4828481376171112,
      "learning_rate": 0.00016522980302597774,
      "loss": 0.7632,
      "step": 710
    },
    {
      "epoch": 0.5996252342286071,
      "grad_norm": 0.5310595035552979,
      "learning_rate": 0.000164658863831002,
      "loss": 0.7631,
      "step": 720
    },
    {
      "epoch": 0.6079533624817822,
      "grad_norm": 0.5404865741729736,
      "learning_rate": 0.00016408792463602626,
      "loss": 0.7834,
      "step": 730
    },
    {
      "epoch": 0.6162814907349573,
      "grad_norm": 0.49601438641548157,
      "learning_rate": 0.00016351698544105054,
      "loss": 0.7705,
      "step": 740
    },
    {
      "epoch": 0.6246096189881324,
      "grad_norm": 0.5294772386550903,
      "learning_rate": 0.00016294604624607478,
      "loss": 0.7899,
      "step": 750
    },
    {
      "epoch": 0.6329377472413075,
      "grad_norm": 0.6693053841590881,
      "learning_rate": 0.00016237510705109905,
      "loss": 0.7741,
      "step": 760
    },
    {
      "epoch": 0.6412658754944827,
      "grad_norm": 0.5322103500366211,
      "learning_rate": 0.00016180416785612333,
      "loss": 0.7748,
      "step": 770
    },
    {
      "epoch": 0.6495940037476577,
      "grad_norm": 0.6172773241996765,
      "learning_rate": 0.0001612332286611476,
      "loss": 0.7792,
      "step": 780
    },
    {
      "epoch": 0.6579221320008328,
      "grad_norm": 0.5860980153083801,
      "learning_rate": 0.00016066228946617184,
      "loss": 0.7998,
      "step": 790
    },
    {
      "epoch": 0.6662502602540079,
      "grad_norm": 0.49824315309524536,
      "learning_rate": 0.00016009135027119612,
      "loss": 0.7459,
      "step": 800
    },
    {
      "epoch": 0.674578388507183,
      "grad_norm": 0.5096157193183899,
      "learning_rate": 0.0001595204110762204,
      "loss": 0.7656,
      "step": 810
    },
    {
      "epoch": 0.6829065167603581,
      "grad_norm": 0.5909919142723083,
      "learning_rate": 0.00015894947188124464,
      "loss": 0.7966,
      "step": 820
    },
    {
      "epoch": 0.6912346450135332,
      "grad_norm": 0.5401273965835571,
      "learning_rate": 0.0001583785326862689,
      "loss": 0.7456,
      "step": 830
    },
    {
      "epoch": 0.6995627732667083,
      "grad_norm": 0.5608735680580139,
      "learning_rate": 0.00015780759349129318,
      "loss": 0.7621,
      "step": 840
    },
    {
      "epoch": 0.7078909015198834,
      "grad_norm": 0.5576484203338623,
      "learning_rate": 0.00015723665429631745,
      "loss": 0.7607,
      "step": 850
    },
    {
      "epoch": 0.7162190297730585,
      "grad_norm": 0.5092172622680664,
      "learning_rate": 0.00015666571510134173,
      "loss": 0.7669,
      "step": 860
    },
    {
      "epoch": 0.7245471580262336,
      "grad_norm": 0.5104865431785583,
      "learning_rate": 0.000156094775906366,
      "loss": 0.7715,
      "step": 870
    },
    {
      "epoch": 0.7328752862794087,
      "grad_norm": 0.5867599248886108,
      "learning_rate": 0.00015552383671139024,
      "loss": 0.7962,
      "step": 880
    },
    {
      "epoch": 0.7412034145325838,
      "grad_norm": 0.5592814683914185,
      "learning_rate": 0.00015495289751641452,
      "loss": 0.7825,
      "step": 890
    },
    {
      "epoch": 0.749531542785759,
      "grad_norm": 0.5156508088111877,
      "learning_rate": 0.0001543819583214388,
      "loss": 0.7941,
      "step": 900
    },
    {
      "epoch": 0.757859671038934,
      "grad_norm": 0.5148082375526428,
      "learning_rate": 0.00015381101912646304,
      "loss": 0.84,
      "step": 910
    },
    {
      "epoch": 0.7661877992921091,
      "grad_norm": 0.5683316588401794,
      "learning_rate": 0.0001532400799314873,
      "loss": 0.7748,
      "step": 920
    },
    {
      "epoch": 0.7745159275452842,
      "grad_norm": 0.5283454656600952,
      "learning_rate": 0.00015266914073651158,
      "loss": 0.7603,
      "step": 930
    },
    {
      "epoch": 0.7828440557984593,
      "grad_norm": 0.6947246789932251,
      "learning_rate": 0.00015209820154153583,
      "loss": 0.7825,
      "step": 940
    },
    {
      "epoch": 0.7911721840516344,
      "grad_norm": 0.5581014752388,
      "learning_rate": 0.0001515272623465601,
      "loss": 0.7931,
      "step": 950
    },
    {
      "epoch": 0.7995003123048094,
      "grad_norm": 0.5351816415786743,
      "learning_rate": 0.00015095632315158437,
      "loss": 0.7734,
      "step": 960
    },
    {
      "epoch": 0.8078284405579846,
      "grad_norm": 0.5093077421188354,
      "learning_rate": 0.00015038538395660864,
      "loss": 0.7741,
      "step": 970
    },
    {
      "epoch": 0.8161565688111597,
      "grad_norm": 0.549069344997406,
      "learning_rate": 0.0001498144447616329,
      "loss": 0.7756,
      "step": 980
    },
    {
      "epoch": 0.8244846970643348,
      "grad_norm": 0.43704381585121155,
      "learning_rate": 0.00014924350556665716,
      "loss": 0.7737,
      "step": 990
    },
    {
      "epoch": 0.8328128253175099,
      "grad_norm": 0.5176715850830078,
      "learning_rate": 0.00014867256637168144,
      "loss": 0.779,
      "step": 1000
    },
    {
      "epoch": 0.841140953570685,
      "grad_norm": 0.5527398586273193,
      "learning_rate": 0.00014810162717670568,
      "loss": 0.7749,
      "step": 1010
    },
    {
      "epoch": 0.84946908182386,
      "grad_norm": 0.5887276530265808,
      "learning_rate": 0.00014753068798172995,
      "loss": 0.7401,
      "step": 1020
    },
    {
      "epoch": 0.8577972100770351,
      "grad_norm": 0.5736914277076721,
      "learning_rate": 0.00014695974878675423,
      "loss": 0.7789,
      "step": 1030
    },
    {
      "epoch": 0.8661253383302103,
      "grad_norm": 0.5101610422134399,
      "learning_rate": 0.00014638880959177847,
      "loss": 0.7701,
      "step": 1040
    },
    {
      "epoch": 0.8744534665833854,
      "grad_norm": 0.5535489916801453,
      "learning_rate": 0.00014581787039680274,
      "loss": 0.7396,
      "step": 1050
    },
    {
      "epoch": 0.8827815948365605,
      "grad_norm": 0.5174049735069275,
      "learning_rate": 0.00014524693120182702,
      "loss": 0.7702,
      "step": 1060
    },
    {
      "epoch": 0.8911097230897356,
      "grad_norm": 0.5417133569717407,
      "learning_rate": 0.00014467599200685126,
      "loss": 0.7606,
      "step": 1070
    },
    {
      "epoch": 0.8994378513429107,
      "grad_norm": 0.49147987365722656,
      "learning_rate": 0.00014410505281187554,
      "loss": 0.7621,
      "step": 1080
    },
    {
      "epoch": 0.9077659795960857,
      "grad_norm": 0.546907901763916,
      "learning_rate": 0.0001435341136168998,
      "loss": 0.7604,
      "step": 1090
    },
    {
      "epoch": 0.9160941078492609,
      "grad_norm": 0.5813907980918884,
      "learning_rate": 0.00014296317442192405,
      "loss": 0.8069,
      "step": 1100
    },
    {
      "epoch": 0.924422236102436,
      "grad_norm": 0.5818731188774109,
      "learning_rate": 0.00014239223522694833,
      "loss": 0.8083,
      "step": 1110
    },
    {
      "epoch": 0.9327503643556111,
      "grad_norm": 0.4400225877761841,
      "learning_rate": 0.0001418212960319726,
      "loss": 0.7735,
      "step": 1120
    },
    {
      "epoch": 0.9410784926087862,
      "grad_norm": 0.5275735855102539,
      "learning_rate": 0.00014125035683699685,
      "loss": 0.784,
      "step": 1130
    },
    {
      "epoch": 0.9494066208619613,
      "grad_norm": 0.5440353155136108,
      "learning_rate": 0.00014067941764202112,
      "loss": 0.7661,
      "step": 1140
    },
    {
      "epoch": 0.9577347491151363,
      "grad_norm": 0.5043870806694031,
      "learning_rate": 0.0001401084784470454,
      "loss": 0.7602,
      "step": 1150
    },
    {
      "epoch": 0.9660628773683114,
      "grad_norm": 0.565267026424408,
      "learning_rate": 0.00013953753925206966,
      "loss": 0.7429,
      "step": 1160
    },
    {
      "epoch": 0.9743910056214866,
      "grad_norm": 0.531380295753479,
      "learning_rate": 0.0001389666000570939,
      "loss": 0.7596,
      "step": 1170
    },
    {
      "epoch": 0.9827191338746617,
      "grad_norm": 0.46631190180778503,
      "learning_rate": 0.00013839566086211818,
      "loss": 0.7684,
      "step": 1180
    },
    {
      "epoch": 0.9910472621278368,
      "grad_norm": 0.5102959871292114,
      "learning_rate": 0.00013782472166714245,
      "loss": 0.7543,
      "step": 1190
    },
    {
      "epoch": 0.9993753903810119,
      "grad_norm": 0.5351875424385071,
      "learning_rate": 0.00013725378247216673,
      "loss": 0.7516,
      "step": 1200
    },
    {
      "epoch": 1.0074953154278576,
      "grad_norm": 0.5926082730293274,
      "learning_rate": 0.000136682843277191,
      "loss": 0.7609,
      "step": 1210
    },
    {
      "epoch": 1.0158234436810327,
      "grad_norm": 0.519608736038208,
      "learning_rate": 0.00013611190408221525,
      "loss": 0.7094,
      "step": 1220
    },
    {
      "epoch": 1.0241515719342078,
      "grad_norm": 0.5353900194168091,
      "learning_rate": 0.00013554096488723952,
      "loss": 0.7407,
      "step": 1230
    },
    {
      "epoch": 1.0324797001873829,
      "grad_norm": 0.517781138420105,
      "learning_rate": 0.0001349700256922638,
      "loss": 0.7589,
      "step": 1240
    },
    {
      "epoch": 1.040807828440558,
      "grad_norm": 0.5366228222846985,
      "learning_rate": 0.00013439908649728806,
      "loss": 0.7201,
      "step": 1250
    },
    {
      "epoch": 1.049135956693733,
      "grad_norm": 0.4817763566970825,
      "learning_rate": 0.0001338281473023123,
      "loss": 0.785,
      "step": 1260
    },
    {
      "epoch": 1.0574640849469081,
      "grad_norm": 0.524955153465271,
      "learning_rate": 0.00013325720810733658,
      "loss": 0.7267,
      "step": 1270
    },
    {
      "epoch": 1.0657922132000832,
      "grad_norm": 0.5737389922142029,
      "learning_rate": 0.00013268626891236085,
      "loss": 0.778,
      "step": 1280
    },
    {
      "epoch": 1.0741203414532583,
      "grad_norm": 0.5352047681808472,
      "learning_rate": 0.0001321153297173851,
      "loss": 0.7967,
      "step": 1290
    },
    {
      "epoch": 1.0824484697064334,
      "grad_norm": 0.5315895676612854,
      "learning_rate": 0.00013154439052240937,
      "loss": 0.7543,
      "step": 1300
    },
    {
      "epoch": 1.0907765979596085,
      "grad_norm": 0.5481370091438293,
      "learning_rate": 0.00013097345132743365,
      "loss": 0.7594,
      "step": 1310
    },
    {
      "epoch": 1.0991047262127838,
      "grad_norm": 0.5549591183662415,
      "learning_rate": 0.00013040251213245792,
      "loss": 0.7331,
      "step": 1320
    },
    {
      "epoch": 1.1074328544659588,
      "grad_norm": 0.5276317596435547,
      "learning_rate": 0.00012983157293748216,
      "loss": 0.7438,
      "step": 1330
    },
    {
      "epoch": 1.115760982719134,
      "grad_norm": 0.5282708406448364,
      "learning_rate": 0.00012926063374250644,
      "loss": 0.7083,
      "step": 1340
    },
    {
      "epoch": 1.124089110972309,
      "grad_norm": 0.5655364990234375,
      "learning_rate": 0.0001286896945475307,
      "loss": 0.7439,
      "step": 1350
    },
    {
      "epoch": 1.132417239225484,
      "grad_norm": 0.6702127456665039,
      "learning_rate": 0.00012811875535255495,
      "loss": 0.7623,
      "step": 1360
    },
    {
      "epoch": 1.1407453674786592,
      "grad_norm": 0.5132356286048889,
      "learning_rate": 0.00012754781615757923,
      "loss": 0.748,
      "step": 1370
    },
    {
      "epoch": 1.1490734957318343,
      "grad_norm": 0.5308963656425476,
      "learning_rate": 0.0001269768769626035,
      "loss": 0.745,
      "step": 1380
    },
    {
      "epoch": 1.1574016239850093,
      "grad_norm": 0.5107733607292175,
      "learning_rate": 0.00012640593776762775,
      "loss": 0.7402,
      "step": 1390
    },
    {
      "epoch": 1.1657297522381844,
      "grad_norm": 0.6543974876403809,
      "learning_rate": 0.00012583499857265202,
      "loss": 0.76,
      "step": 1400
    },
    {
      "epoch": 1.1740578804913595,
      "grad_norm": 0.5872929096221924,
      "learning_rate": 0.0001252640593776763,
      "loss": 0.7469,
      "step": 1410
    },
    {
      "epoch": 1.1823860087445346,
      "grad_norm": 0.5402197241783142,
      "learning_rate": 0.00012469312018270054,
      "loss": 0.7651,
      "step": 1420
    },
    {
      "epoch": 1.1907141369977097,
      "grad_norm": 0.5554109215736389,
      "learning_rate": 0.0001241221809877248,
      "loss": 0.7543,
      "step": 1430
    },
    {
      "epoch": 1.1990422652508848,
      "grad_norm": 0.5340924263000488,
      "learning_rate": 0.00012355124179274908,
      "loss": 0.7485,
      "step": 1440
    },
    {
      "epoch": 1.20737039350406,
      "grad_norm": 0.5616198778152466,
      "learning_rate": 0.00012298030259777333,
      "loss": 0.7475,
      "step": 1450
    },
    {
      "epoch": 1.2156985217572351,
      "grad_norm": 0.5712025165557861,
      "learning_rate": 0.0001224093634027976,
      "loss": 0.7565,
      "step": 1460
    },
    {
      "epoch": 1.2240266500104102,
      "grad_norm": 0.5363101363182068,
      "learning_rate": 0.00012183842420782187,
      "loss": 0.7376,
      "step": 1470
    },
    {
      "epoch": 1.2323547782635853,
      "grad_norm": 0.5171469449996948,
      "learning_rate": 0.00012126748501284613,
      "loss": 0.7502,
      "step": 1480
    },
    {
      "epoch": 1.2406829065167604,
      "grad_norm": 0.48432135581970215,
      "learning_rate": 0.0001206965458178704,
      "loss": 0.713,
      "step": 1490
    },
    {
      "epoch": 1.2490110347699355,
      "grad_norm": 0.5215731263160706,
      "learning_rate": 0.00012012560662289468,
      "loss": 0.7415,
      "step": 1500
    },
    {
      "epoch": 1.2573391630231106,
      "grad_norm": 0.492549329996109,
      "learning_rate": 0.00011955466742791894,
      "loss": 0.76,
      "step": 1510
    },
    {
      "epoch": 1.2656672912762856,
      "grad_norm": 0.5335714221000671,
      "learning_rate": 0.0001189837282329432,
      "loss": 0.7902,
      "step": 1520
    },
    {
      "epoch": 1.2739954195294607,
      "grad_norm": 0.6450555324554443,
      "learning_rate": 0.00011841278903796747,
      "loss": 0.766,
      "step": 1530
    },
    {
      "epoch": 1.2823235477826358,
      "grad_norm": 0.581842303276062,
      "learning_rate": 0.00011784184984299174,
      "loss": 0.7751,
      "step": 1540
    },
    {
      "epoch": 1.2906516760358109,
      "grad_norm": 0.551160991191864,
      "learning_rate": 0.00011727091064801599,
      "loss": 0.7231,
      "step": 1550
    },
    {
      "epoch": 1.2989798042889862,
      "grad_norm": 0.5536897778511047,
      "learning_rate": 0.00011669997145304026,
      "loss": 0.7346,
      "step": 1560
    },
    {
      "epoch": 1.307307932542161,
      "grad_norm": 0.5846131443977356,
      "learning_rate": 0.00011612903225806453,
      "loss": 0.7328,
      "step": 1570
    },
    {
      "epoch": 1.3156360607953363,
      "grad_norm": 0.5979673266410828,
      "learning_rate": 0.00011555809306308878,
      "loss": 0.7607,
      "step": 1580
    },
    {
      "epoch": 1.3239641890485112,
      "grad_norm": 0.527472972869873,
      "learning_rate": 0.00011498715386811305,
      "loss": 0.7298,
      "step": 1590
    },
    {
      "epoch": 1.3322923173016865,
      "grad_norm": 0.560035765171051,
      "learning_rate": 0.00011441621467313732,
      "loss": 0.7173,
      "step": 1600
    },
    {
      "epoch": 1.3406204455548616,
      "grad_norm": 0.5512088537216187,
      "learning_rate": 0.00011384527547816157,
      "loss": 0.7717,
      "step": 1610
    },
    {
      "epoch": 1.3489485738080367,
      "grad_norm": 0.5368456840515137,
      "learning_rate": 0.00011327433628318584,
      "loss": 0.7696,
      "step": 1620
    },
    {
      "epoch": 1.3572767020612118,
      "grad_norm": 0.5828245282173157,
      "learning_rate": 0.00011270339708821011,
      "loss": 0.7495,
      "step": 1630
    },
    {
      "epoch": 1.3656048303143868,
      "grad_norm": 0.5414822101593018,
      "learning_rate": 0.00011213245789323437,
      "loss": 0.7652,
      "step": 1640
    },
    {
      "epoch": 1.373932958567562,
      "grad_norm": 0.5588496327400208,
      "learning_rate": 0.00011156151869825863,
      "loss": 0.7352,
      "step": 1650
    },
    {
      "epoch": 1.382261086820737,
      "grad_norm": 0.6099138259887695,
      "learning_rate": 0.0001109905795032829,
      "loss": 0.7418,
      "step": 1660
    },
    {
      "epoch": 1.390589215073912,
      "grad_norm": 0.5250871181488037,
      "learning_rate": 0.00011041964030830716,
      "loss": 0.7717,
      "step": 1670
    },
    {
      "epoch": 1.3989173433270872,
      "grad_norm": 0.6001952886581421,
      "learning_rate": 0.00010984870111333144,
      "loss": 0.7447,
      "step": 1680
    },
    {
      "epoch": 1.4072454715802625,
      "grad_norm": 0.6242745518684387,
      "learning_rate": 0.00010927776191835571,
      "loss": 0.7567,
      "step": 1690
    },
    {
      "epoch": 1.4155735998334373,
      "grad_norm": 0.5680369734764099,
      "learning_rate": 0.00010870682272337998,
      "loss": 0.7243,
      "step": 1700
    },
    {
      "epoch": 1.4239017280866126,
      "grad_norm": 0.5359448194503784,
      "learning_rate": 0.00010813588352840423,
      "loss": 0.7384,
      "step": 1710
    },
    {
      "epoch": 1.4322298563397875,
      "grad_norm": 0.507286548614502,
      "learning_rate": 0.0001075649443334285,
      "loss": 0.7265,
      "step": 1720
    },
    {
      "epoch": 1.4405579845929628,
      "grad_norm": 0.5739713907241821,
      "learning_rate": 0.00010699400513845277,
      "loss": 0.732,
      "step": 1730
    },
    {
      "epoch": 1.4488861128461379,
      "grad_norm": 0.530808687210083,
      "learning_rate": 0.00010642306594347702,
      "loss": 0.7692,
      "step": 1740
    },
    {
      "epoch": 1.457214241099313,
      "grad_norm": 0.5544137358665466,
      "learning_rate": 0.00010585212674850129,
      "loss": 0.7343,
      "step": 1750
    },
    {
      "epoch": 1.465542369352488,
      "grad_norm": 0.49795234203338623,
      "learning_rate": 0.00010528118755352556,
      "loss": 0.7259,
      "step": 1760
    },
    {
      "epoch": 1.4738704976056631,
      "grad_norm": 0.5295220017433167,
      "learning_rate": 0.00010471024835854981,
      "loss": 0.7344,
      "step": 1770
    },
    {
      "epoch": 1.4821986258588382,
      "grad_norm": 0.6018770933151245,
      "learning_rate": 0.00010413930916357408,
      "loss": 0.7561,
      "step": 1780
    },
    {
      "epoch": 1.4905267541120133,
      "grad_norm": 0.6146726608276367,
      "learning_rate": 0.00010356836996859835,
      "loss": 0.7717,
      "step": 1790
    },
    {
      "epoch": 1.4988548823651884,
      "grad_norm": 0.542021632194519,
      "learning_rate": 0.0001029974307736226,
      "loss": 0.7488,
      "step": 1800
    },
    {
      "epoch": 1.5071830106183635,
      "grad_norm": 0.6243857145309448,
      "learning_rate": 0.00010242649157864687,
      "loss": 0.7245,
      "step": 1810
    },
    {
      "epoch": 1.5155111388715388,
      "grad_norm": 0.5066047310829163,
      "learning_rate": 0.00010185555238367115,
      "loss": 0.7317,
      "step": 1820
    },
    {
      "epoch": 1.5238392671247136,
      "grad_norm": 0.6073366403579712,
      "learning_rate": 0.0001012846131886954,
      "loss": 0.781,
      "step": 1830
    },
    {
      "epoch": 1.532167395377889,
      "grad_norm": 0.4950423836708069,
      "learning_rate": 0.00010071367399371966,
      "loss": 0.7446,
      "step": 1840
    },
    {
      "epoch": 1.5404955236310638,
      "grad_norm": 0.6082502007484436,
      "learning_rate": 0.00010014273479874394,
      "loss": 0.7577,
      "step": 1850
    },
    {
      "epoch": 1.548823651884239,
      "grad_norm": 0.6058792471885681,
      "learning_rate": 9.957179560376821e-05,
      "loss": 0.7306,
      "step": 1860
    },
    {
      "epoch": 1.557151780137414,
      "grad_norm": 0.4835642874240875,
      "learning_rate": 9.900085640879247e-05,
      "loss": 0.7386,
      "step": 1870
    },
    {
      "epoch": 1.5654799083905893,
      "grad_norm": 0.5892341136932373,
      "learning_rate": 9.842991721381674e-05,
      "loss": 0.7666,
      "step": 1880
    },
    {
      "epoch": 1.5738080366437643,
      "grad_norm": 0.6260331869125366,
      "learning_rate": 9.7858978018841e-05,
      "loss": 0.778,
      "step": 1890
    },
    {
      "epoch": 1.5821361648969394,
      "grad_norm": 0.5706144571304321,
      "learning_rate": 9.728803882386527e-05,
      "loss": 0.7342,
      "step": 1900
    },
    {
      "epoch": 1.5904642931501145,
      "grad_norm": 0.5742250680923462,
      "learning_rate": 9.671709962888953e-05,
      "loss": 0.7969,
      "step": 1910
    },
    {
      "epoch": 1.5987924214032896,
      "grad_norm": 0.5342006683349609,
      "learning_rate": 9.614616043391379e-05,
      "loss": 0.7261,
      "step": 1920
    },
    {
      "epoch": 1.6071205496564647,
      "grad_norm": 0.49549004435539246,
      "learning_rate": 9.557522123893806e-05,
      "loss": 0.724,
      "step": 1930
    },
    {
      "epoch": 1.6154486779096398,
      "grad_norm": 0.5007038116455078,
      "learning_rate": 9.500428204396232e-05,
      "loss": 0.739,
      "step": 1940
    },
    {
      "epoch": 1.623776806162815,
      "grad_norm": 0.5757259130477905,
      "learning_rate": 9.443334284898658e-05,
      "loss": 0.7696,
      "step": 1950
    },
    {
      "epoch": 1.63210493441599,
      "grad_norm": 0.49447137117385864,
      "learning_rate": 9.386240365401085e-05,
      "loss": 0.7609,
      "step": 1960
    },
    {
      "epoch": 1.6404330626691652,
      "grad_norm": 0.5489360690116882,
      "learning_rate": 9.329146445903511e-05,
      "loss": 0.7416,
      "step": 1970
    },
    {
      "epoch": 1.64876119092234,
      "grad_norm": 0.4980350434780121,
      "learning_rate": 9.272052526405937e-05,
      "loss": 0.7292,
      "step": 1980
    },
    {
      "epoch": 1.6570893191755154,
      "grad_norm": 0.6231033205986023,
      "learning_rate": 9.214958606908365e-05,
      "loss": 0.7366,
      "step": 1990
    },
    {
      "epoch": 1.6654174474286902,
      "grad_norm": 0.5242041349411011,
      "learning_rate": 9.15786468741079e-05,
      "loss": 0.7715,
      "step": 2000
    },
    {
      "epoch": 1.6737455756818655,
      "grad_norm": 0.5677677989006042,
      "learning_rate": 9.100770767913218e-05,
      "loss": 0.7458,
      "step": 2010
    },
    {
      "epoch": 1.6820737039350406,
      "grad_norm": 0.4504517614841461,
      "learning_rate": 9.043676848415644e-05,
      "loss": 0.7414,
      "step": 2020
    },
    {
      "epoch": 1.6904018321882157,
      "grad_norm": 0.5147939920425415,
      "learning_rate": 8.986582928918071e-05,
      "loss": 0.7161,
      "step": 2030
    },
    {
      "epoch": 1.6987299604413908,
      "grad_norm": 0.5184817910194397,
      "learning_rate": 8.929489009420497e-05,
      "loss": 0.737,
      "step": 2040
    },
    {
      "epoch": 1.7070580886945659,
      "grad_norm": 0.511454701423645,
      "learning_rate": 8.872395089922924e-05,
      "loss": 0.7326,
      "step": 2050
    },
    {
      "epoch": 1.715386216947741,
      "grad_norm": 0.6407281160354614,
      "learning_rate": 8.81530117042535e-05,
      "loss": 0.757,
      "step": 2060
    },
    {
      "epoch": 1.723714345200916,
      "grad_norm": 0.46770188212394714,
      "learning_rate": 8.758207250927777e-05,
      "loss": 0.7246,
      "step": 2070
    },
    {
      "epoch": 1.7320424734540913,
      "grad_norm": 0.5417940616607666,
      "learning_rate": 8.701113331430203e-05,
      "loss": 0.73,
      "step": 2080
    },
    {
      "epoch": 1.7403706017072662,
      "grad_norm": 0.7172756791114807,
      "learning_rate": 8.64401941193263e-05,
      "loss": 0.7826,
      "step": 2090
    },
    {
      "epoch": 1.7486987299604415,
      "grad_norm": 0.49252116680145264,
      "learning_rate": 8.586925492435056e-05,
      "loss": 0.7414,
      "step": 2100
    },
    {
      "epoch": 1.7570268582136164,
      "grad_norm": 0.5305785536766052,
      "learning_rate": 8.529831572937482e-05,
      "loss": 0.7638,
      "step": 2110
    },
    {
      "epoch": 1.7653549864667917,
      "grad_norm": 0.5835579633712769,
      "learning_rate": 8.47273765343991e-05,
      "loss": 0.7505,
      "step": 2120
    },
    {
      "epoch": 1.7736831147199665,
      "grad_norm": 0.5282079577445984,
      "learning_rate": 8.415643733942336e-05,
      "loss": 0.7477,
      "step": 2130
    },
    {
      "epoch": 1.7820112429731418,
      "grad_norm": 0.5077541470527649,
      "learning_rate": 8.358549814444761e-05,
      "loss": 0.7145,
      "step": 2140
    },
    {
      "epoch": 1.790339371226317,
      "grad_norm": 0.5642469525337219,
      "learning_rate": 8.301455894947189e-05,
      "loss": 0.7467,
      "step": 2150
    },
    {
      "epoch": 1.798667499479492,
      "grad_norm": 0.4849185347557068,
      "learning_rate": 8.244361975449615e-05,
      "loss": 0.7587,
      "step": 2160
    },
    {
      "epoch": 1.806995627732667,
      "grad_norm": 0.502395749092102,
      "learning_rate": 8.187268055952042e-05,
      "loss": 0.7251,
      "step": 2170
    },
    {
      "epoch": 1.8153237559858422,
      "grad_norm": 0.508478045463562,
      "learning_rate": 8.130174136454468e-05,
      "loss": 0.7195,
      "step": 2180
    },
    {
      "epoch": 1.8236518842390173,
      "grad_norm": 0.5607203841209412,
      "learning_rate": 8.073080216956894e-05,
      "loss": 0.7317,
      "step": 2190
    },
    {
      "epoch": 1.8319800124921923,
      "grad_norm": 0.5707418918609619,
      "learning_rate": 8.015986297459321e-05,
      "loss": 0.7568,
      "step": 2200
    },
    {
      "epoch": 1.8403081407453676,
      "grad_norm": 0.6101348400115967,
      "learning_rate": 7.958892377961747e-05,
      "loss": 0.7191,
      "step": 2210
    },
    {
      "epoch": 1.8486362689985425,
      "grad_norm": 0.5471882224082947,
      "learning_rate": 7.901798458464174e-05,
      "loss": 0.7967,
      "step": 2220
    },
    {
      "epoch": 1.8569643972517178,
      "grad_norm": 0.4689045250415802,
      "learning_rate": 7.8447045389666e-05,
      "loss": 0.7265,
      "step": 2230
    },
    {
      "epoch": 1.8652925255048927,
      "grad_norm": 0.5977540016174316,
      "learning_rate": 7.787610619469027e-05,
      "loss": 0.7176,
      "step": 2240
    },
    {
      "epoch": 1.873620653758068,
      "grad_norm": 0.685124397277832,
      "learning_rate": 7.730516699971453e-05,
      "loss": 0.7213,
      "step": 2250
    },
    {
      "epoch": 1.8819487820112428,
      "grad_norm": 0.5801709890365601,
      "learning_rate": 7.67342278047388e-05,
      "loss": 0.7753,
      "step": 2260
    },
    {
      "epoch": 1.8902769102644181,
      "grad_norm": 0.4972231686115265,
      "learning_rate": 7.616328860976306e-05,
      "loss": 0.7393,
      "step": 2270
    },
    {
      "epoch": 1.8986050385175932,
      "grad_norm": 0.6082484722137451,
      "learning_rate": 7.559234941478734e-05,
      "loss": 0.7195,
      "step": 2280
    },
    {
      "epoch": 1.9069331667707683,
      "grad_norm": 0.5566319823265076,
      "learning_rate": 7.50214102198116e-05,
      "loss": 0.7535,
      "step": 2290
    },
    {
      "epoch": 1.9152612950239434,
      "grad_norm": 0.5072596073150635,
      "learning_rate": 7.445047102483586e-05,
      "loss": 0.7623,
      "step": 2300
    },
    {
      "epoch": 1.9235894232771185,
      "grad_norm": 0.48961398005485535,
      "learning_rate": 7.387953182986013e-05,
      "loss": 0.7257,
      "step": 2310
    },
    {
      "epoch": 1.9319175515302935,
      "grad_norm": 0.6331600546836853,
      "learning_rate": 7.330859263488439e-05,
      "loss": 0.7488,
      "step": 2320
    },
    {
      "epoch": 1.9402456797834686,
      "grad_norm": 0.5148442387580872,
      "learning_rate": 7.273765343990865e-05,
      "loss": 0.7323,
      "step": 2330
    },
    {
      "epoch": 1.948573808036644,
      "grad_norm": 0.5545548796653748,
      "learning_rate": 7.216671424493292e-05,
      "loss": 0.7533,
      "step": 2340
    },
    {
      "epoch": 1.9569019362898188,
      "grad_norm": 0.4720025658607483,
      "learning_rate": 7.159577504995718e-05,
      "loss": 0.7071,
      "step": 2350
    },
    {
      "epoch": 1.965230064542994,
      "grad_norm": 0.6042229533195496,
      "learning_rate": 7.102483585498145e-05,
      "loss": 0.7126,
      "step": 2360
    },
    {
      "epoch": 1.973558192796169,
      "grad_norm": 0.4973584711551666,
      "learning_rate": 7.045389666000571e-05,
      "loss": 0.7446,
      "step": 2370
    },
    {
      "epoch": 1.9818863210493443,
      "grad_norm": 0.47420960664749146,
      "learning_rate": 6.988295746502997e-05,
      "loss": 0.7282,
      "step": 2380
    },
    {
      "epoch": 1.9902144493025191,
      "grad_norm": 0.528920590877533,
      "learning_rate": 6.931201827005424e-05,
      "loss": 0.7172,
      "step": 2390
    },
    {
      "epoch": 1.9985425775556944,
      "grad_norm": 0.5135865807533264,
      "learning_rate": 6.87410790750785e-05,
      "loss": 0.774,
      "step": 2400
    }
  ],
  "logging_steps": 10,
  "max_steps": 3603,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5297279525126144e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
