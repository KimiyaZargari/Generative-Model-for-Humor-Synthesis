{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1201,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0083281282531751,
      "grad_norm": 23.824613571166992,
      "learning_rate": 1.8e-05,
      "loss": 11.6,
      "step": 10
    },
    {
      "epoch": 0.0166562565063502,
      "grad_norm": 45.750244140625,
      "learning_rate": 3.8e-05,
      "loss": 10.5985,
      "step": 20
    },
    {
      "epoch": 0.024984384759525295,
      "grad_norm": 20.440067291259766,
      "learning_rate": 5.8e-05,
      "loss": 6.3897,
      "step": 30
    },
    {
      "epoch": 0.0333125130127004,
      "grad_norm": 15.120610237121582,
      "learning_rate": 7.800000000000001e-05,
      "loss": 3.6727,
      "step": 40
    },
    {
      "epoch": 0.041640641265875494,
      "grad_norm": 1.9982540607452393,
      "learning_rate": 9.8e-05,
      "loss": 1.7822,
      "step": 50
    },
    {
      "epoch": 0.04996876951905059,
      "grad_norm": 1.4120821952819824,
      "learning_rate": 0.000118,
      "loss": 1.1766,
      "step": 60
    },
    {
      "epoch": 0.05829689777222569,
      "grad_norm": 0.9169183969497681,
      "learning_rate": 0.000138,
      "loss": 0.9723,
      "step": 70
    },
    {
      "epoch": 0.0666250260254008,
      "grad_norm": 0.8249570727348328,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.896,
      "step": 80
    },
    {
      "epoch": 0.07495315427857589,
      "grad_norm": 0.7147775292396545,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.8131,
      "step": 90
    },
    {
      "epoch": 0.08328128253175099,
      "grad_norm": 0.7982261180877686,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.8204,
      "step": 100
    },
    {
      "epoch": 0.09160941078492608,
      "grad_norm": 0.8132607340812683,
      "learning_rate": 0.00019948615472452184,
      "loss": 0.8226,
      "step": 110
    },
    {
      "epoch": 0.09993753903810118,
      "grad_norm": 0.7513646483421326,
      "learning_rate": 0.00019891521552954612,
      "loss": 0.8209,
      "step": 120
    },
    {
      "epoch": 0.10826566729127629,
      "grad_norm": 0.8384955525398254,
      "learning_rate": 0.0001983442763345704,
      "loss": 0.8297,
      "step": 130
    },
    {
      "epoch": 0.11659379554445139,
      "grad_norm": 0.8373925685882568,
      "learning_rate": 0.00019777333713959466,
      "loss": 0.8689,
      "step": 140
    },
    {
      "epoch": 0.12492192379762648,
      "grad_norm": 0.8194049000740051,
      "learning_rate": 0.0001972023979446189,
      "loss": 0.8003,
      "step": 150
    },
    {
      "epoch": 0.1332500520508016,
      "grad_norm": 0.7251324653625488,
      "learning_rate": 0.00019663145874964318,
      "loss": 0.7982,
      "step": 160
    },
    {
      "epoch": 0.14157818030397668,
      "grad_norm": 0.7248123288154602,
      "learning_rate": 0.00019606051955466745,
      "loss": 0.8227,
      "step": 170
    },
    {
      "epoch": 0.14990630855715179,
      "grad_norm": 0.58534175157547,
      "learning_rate": 0.0001954895803596917,
      "loss": 0.7469,
      "step": 180
    },
    {
      "epoch": 0.15823443681032687,
      "grad_norm": 0.7220483422279358,
      "learning_rate": 0.00019491864116471597,
      "loss": 0.8149,
      "step": 190
    },
    {
      "epoch": 0.16656256506350198,
      "grad_norm": 0.7826076745986938,
      "learning_rate": 0.00019434770196974024,
      "loss": 0.8098,
      "step": 200
    },
    {
      "epoch": 0.1748906933166771,
      "grad_norm": 0.6980066299438477,
      "learning_rate": 0.0001937767627747645,
      "loss": 0.8058,
      "step": 210
    },
    {
      "epoch": 0.18321882156985217,
      "grad_norm": 0.5895638465881348,
      "learning_rate": 0.00019320582357978876,
      "loss": 0.7887,
      "step": 220
    },
    {
      "epoch": 0.19154694982302728,
      "grad_norm": 0.6908169984817505,
      "learning_rate": 0.00019263488438481304,
      "loss": 0.7975,
      "step": 230
    },
    {
      "epoch": 0.19987507807620236,
      "grad_norm": 0.760324239730835,
      "learning_rate": 0.0001920639451898373,
      "loss": 0.8216,
      "step": 240
    },
    {
      "epoch": 0.20820320632937747,
      "grad_norm": 0.6377607583999634,
      "learning_rate": 0.00019149300599486155,
      "loss": 0.8046,
      "step": 250
    },
    {
      "epoch": 0.21653133458255258,
      "grad_norm": 0.597603440284729,
      "learning_rate": 0.00019092206679988583,
      "loss": 0.768,
      "step": 260
    },
    {
      "epoch": 0.22485946283572766,
      "grad_norm": 0.6276570558547974,
      "learning_rate": 0.0001903511276049101,
      "loss": 0.8114,
      "step": 270
    },
    {
      "epoch": 0.23318759108890277,
      "grad_norm": 0.6776286959648132,
      "learning_rate": 0.00018978018840993434,
      "loss": 0.8007,
      "step": 280
    },
    {
      "epoch": 0.24151571934207786,
      "grad_norm": 0.5963298082351685,
      "learning_rate": 0.00018920924921495862,
      "loss": 0.7718,
      "step": 290
    },
    {
      "epoch": 0.24984384759525297,
      "grad_norm": 0.5905337333679199,
      "learning_rate": 0.0001886383100199829,
      "loss": 0.7912,
      "step": 300
    },
    {
      "epoch": 0.25817197584842805,
      "grad_norm": 0.5971078276634216,
      "learning_rate": 0.00018806737082500714,
      "loss": 0.799,
      "step": 310
    },
    {
      "epoch": 0.2665001041016032,
      "grad_norm": 0.6722333431243896,
      "learning_rate": 0.0001874964316300314,
      "loss": 0.792,
      "step": 320
    },
    {
      "epoch": 0.27482823235477827,
      "grad_norm": 0.6170521974563599,
      "learning_rate": 0.00018692549243505568,
      "loss": 0.818,
      "step": 330
    },
    {
      "epoch": 0.28315636060795335,
      "grad_norm": 0.6387074589729309,
      "learning_rate": 0.00018635455324007993,
      "loss": 0.7744,
      "step": 340
    },
    {
      "epoch": 0.2914844888611285,
      "grad_norm": 0.5736905932426453,
      "learning_rate": 0.0001857836140451042,
      "loss": 0.7607,
      "step": 350
    },
    {
      "epoch": 0.29981261711430357,
      "grad_norm": 0.6516372561454773,
      "learning_rate": 0.00018521267485012847,
      "loss": 0.7978,
      "step": 360
    },
    {
      "epoch": 0.30814074536747865,
      "grad_norm": 0.6283776760101318,
      "learning_rate": 0.00018464173565515272,
      "loss": 0.7782,
      "step": 370
    },
    {
      "epoch": 0.31646887362065373,
      "grad_norm": 0.6114363670349121,
      "learning_rate": 0.000184070796460177,
      "loss": 0.7832,
      "step": 380
    },
    {
      "epoch": 0.32479700187382887,
      "grad_norm": 0.5515420436859131,
      "learning_rate": 0.00018349985726520126,
      "loss": 0.7841,
      "step": 390
    },
    {
      "epoch": 0.33312513012700395,
      "grad_norm": 0.5090296864509583,
      "learning_rate": 0.0001829289180702255,
      "loss": 0.8289,
      "step": 400
    },
    {
      "epoch": 0.34145325838017904,
      "grad_norm": 0.6078847050666809,
      "learning_rate": 0.00018235797887524978,
      "loss": 0.7795,
      "step": 410
    },
    {
      "epoch": 0.3497813866333542,
      "grad_norm": 0.6059353351593018,
      "learning_rate": 0.00018178703968027405,
      "loss": 0.7685,
      "step": 420
    },
    {
      "epoch": 0.35810951488652926,
      "grad_norm": 0.7710912823677063,
      "learning_rate": 0.00018121610048529833,
      "loss": 0.782,
      "step": 430
    },
    {
      "epoch": 0.36643764313970434,
      "grad_norm": 0.5963722467422485,
      "learning_rate": 0.00018064516129032257,
      "loss": 0.7783,
      "step": 440
    },
    {
      "epoch": 0.3747657713928795,
      "grad_norm": 0.5401337146759033,
      "learning_rate": 0.00018007422209534684,
      "loss": 0.7703,
      "step": 450
    },
    {
      "epoch": 0.38309389964605456,
      "grad_norm": 0.5642045140266418,
      "learning_rate": 0.00017950328290037112,
      "loss": 0.7919,
      "step": 460
    },
    {
      "epoch": 0.39142202789922964,
      "grad_norm": 0.5768611431121826,
      "learning_rate": 0.0001789323437053954,
      "loss": 0.7746,
      "step": 470
    },
    {
      "epoch": 0.3997501561524047,
      "grad_norm": 0.7498918771743774,
      "learning_rate": 0.00017836140451041966,
      "loss": 0.7664,
      "step": 480
    },
    {
      "epoch": 0.40807828440557986,
      "grad_norm": 0.5459244251251221,
      "learning_rate": 0.0001777904653154439,
      "loss": 0.7787,
      "step": 490
    },
    {
      "epoch": 0.41640641265875494,
      "grad_norm": 0.5505867004394531,
      "learning_rate": 0.00017721952612046818,
      "loss": 0.7759,
      "step": 500
    },
    {
      "epoch": 0.42473454091193,
      "grad_norm": 0.6023789644241333,
      "learning_rate": 0.00017664858692549245,
      "loss": 0.7886,
      "step": 510
    },
    {
      "epoch": 0.43306266916510516,
      "grad_norm": 0.5613372325897217,
      "learning_rate": 0.00017607764773051673,
      "loss": 0.736,
      "step": 520
    },
    {
      "epoch": 0.44139079741828025,
      "grad_norm": 0.6594305038452148,
      "learning_rate": 0.00017550670853554097,
      "loss": 0.8212,
      "step": 530
    },
    {
      "epoch": 0.4497189256714553,
      "grad_norm": 0.5827054977416992,
      "learning_rate": 0.00017493576934056524,
      "loss": 0.772,
      "step": 540
    },
    {
      "epoch": 0.45804705392463047,
      "grad_norm": 0.5854480266571045,
      "learning_rate": 0.00017436483014558952,
      "loss": 0.7649,
      "step": 550
    },
    {
      "epoch": 0.46637518217780555,
      "grad_norm": 0.6113579869270325,
      "learning_rate": 0.00017379389095061376,
      "loss": 0.7902,
      "step": 560
    },
    {
      "epoch": 0.47470331043098063,
      "grad_norm": 0.5400941967964172,
      "learning_rate": 0.00017322295175563804,
      "loss": 0.8481,
      "step": 570
    },
    {
      "epoch": 0.4830314386841557,
      "grad_norm": 0.5883111357688904,
      "learning_rate": 0.0001726520125606623,
      "loss": 0.7569,
      "step": 580
    },
    {
      "epoch": 0.49135956693733085,
      "grad_norm": 0.58284991979599,
      "learning_rate": 0.00017208107336568658,
      "loss": 0.7511,
      "step": 590
    },
    {
      "epoch": 0.49968769519050593,
      "grad_norm": 0.5242885947227478,
      "learning_rate": 0.00017151013417071083,
      "loss": 0.7849,
      "step": 600
    },
    {
      "epoch": 0.508015823443681,
      "grad_norm": 0.6077029705047607,
      "learning_rate": 0.0001709391949757351,
      "loss": 0.799,
      "step": 610
    },
    {
      "epoch": 0.5163439516968561,
      "grad_norm": 0.49672332406044006,
      "learning_rate": 0.00017036825578075937,
      "loss": 0.7643,
      "step": 620
    },
    {
      "epoch": 0.5246720799500312,
      "grad_norm": 0.5276833176612854,
      "learning_rate": 0.00016979731658578362,
      "loss": 0.8031,
      "step": 630
    },
    {
      "epoch": 0.5330002082032064,
      "grad_norm": 0.5235550999641418,
      "learning_rate": 0.0001692263773908079,
      "loss": 0.8017,
      "step": 640
    },
    {
      "epoch": 0.5413283364563815,
      "grad_norm": 0.603817343711853,
      "learning_rate": 0.00016865543819583216,
      "loss": 0.7745,
      "step": 650
    },
    {
      "epoch": 0.5496564647095565,
      "grad_norm": 0.5543803572654724,
      "learning_rate": 0.0001680844990008564,
      "loss": 0.7715,
      "step": 660
    },
    {
      "epoch": 0.5579845929627316,
      "grad_norm": 0.5668521523475647,
      "learning_rate": 0.00016751355980588068,
      "loss": 0.7449,
      "step": 670
    },
    {
      "epoch": 0.5663127212159067,
      "grad_norm": 0.5429356694221497,
      "learning_rate": 0.00016694262061090495,
      "loss": 0.7604,
      "step": 680
    },
    {
      "epoch": 0.5746408494690818,
      "grad_norm": 0.5166087746620178,
      "learning_rate": 0.0001663716814159292,
      "loss": 0.7623,
      "step": 690
    },
    {
      "epoch": 0.582968977722257,
      "grad_norm": 0.655295193195343,
      "learning_rate": 0.00016580074222095347,
      "loss": 0.7757,
      "step": 700
    },
    {
      "epoch": 0.5912971059754321,
      "grad_norm": 0.4828481376171112,
      "learning_rate": 0.00016522980302597774,
      "loss": 0.7632,
      "step": 710
    },
    {
      "epoch": 0.5996252342286071,
      "grad_norm": 0.5310595035552979,
      "learning_rate": 0.000164658863831002,
      "loss": 0.7631,
      "step": 720
    },
    {
      "epoch": 0.6079533624817822,
      "grad_norm": 0.5404865741729736,
      "learning_rate": 0.00016408792463602626,
      "loss": 0.7834,
      "step": 730
    },
    {
      "epoch": 0.6162814907349573,
      "grad_norm": 0.49601438641548157,
      "learning_rate": 0.00016351698544105054,
      "loss": 0.7705,
      "step": 740
    },
    {
      "epoch": 0.6246096189881324,
      "grad_norm": 0.5294772386550903,
      "learning_rate": 0.00016294604624607478,
      "loss": 0.7899,
      "step": 750
    },
    {
      "epoch": 0.6329377472413075,
      "grad_norm": 0.6693053841590881,
      "learning_rate": 0.00016237510705109905,
      "loss": 0.7741,
      "step": 760
    },
    {
      "epoch": 0.6412658754944827,
      "grad_norm": 0.5322103500366211,
      "learning_rate": 0.00016180416785612333,
      "loss": 0.7748,
      "step": 770
    },
    {
      "epoch": 0.6495940037476577,
      "grad_norm": 0.6172773241996765,
      "learning_rate": 0.0001612332286611476,
      "loss": 0.7792,
      "step": 780
    },
    {
      "epoch": 0.6579221320008328,
      "grad_norm": 0.5860980153083801,
      "learning_rate": 0.00016066228946617184,
      "loss": 0.7998,
      "step": 790
    },
    {
      "epoch": 0.6662502602540079,
      "grad_norm": 0.49824315309524536,
      "learning_rate": 0.00016009135027119612,
      "loss": 0.7459,
      "step": 800
    },
    {
      "epoch": 0.674578388507183,
      "grad_norm": 0.5096157193183899,
      "learning_rate": 0.0001595204110762204,
      "loss": 0.7656,
      "step": 810
    },
    {
      "epoch": 0.6829065167603581,
      "grad_norm": 0.5909919142723083,
      "learning_rate": 0.00015894947188124464,
      "loss": 0.7966,
      "step": 820
    },
    {
      "epoch": 0.6912346450135332,
      "grad_norm": 0.5401273965835571,
      "learning_rate": 0.0001583785326862689,
      "loss": 0.7456,
      "step": 830
    },
    {
      "epoch": 0.6995627732667083,
      "grad_norm": 0.5608735680580139,
      "learning_rate": 0.00015780759349129318,
      "loss": 0.7621,
      "step": 840
    },
    {
      "epoch": 0.7078909015198834,
      "grad_norm": 0.5576484203338623,
      "learning_rate": 0.00015723665429631745,
      "loss": 0.7607,
      "step": 850
    },
    {
      "epoch": 0.7162190297730585,
      "grad_norm": 0.5092172622680664,
      "learning_rate": 0.00015666571510134173,
      "loss": 0.7669,
      "step": 860
    },
    {
      "epoch": 0.7245471580262336,
      "grad_norm": 0.5104865431785583,
      "learning_rate": 0.000156094775906366,
      "loss": 0.7715,
      "step": 870
    },
    {
      "epoch": 0.7328752862794087,
      "grad_norm": 0.5867599248886108,
      "learning_rate": 0.00015552383671139024,
      "loss": 0.7962,
      "step": 880
    },
    {
      "epoch": 0.7412034145325838,
      "grad_norm": 0.5592814683914185,
      "learning_rate": 0.00015495289751641452,
      "loss": 0.7825,
      "step": 890
    },
    {
      "epoch": 0.749531542785759,
      "grad_norm": 0.5156508088111877,
      "learning_rate": 0.0001543819583214388,
      "loss": 0.7941,
      "step": 900
    },
    {
      "epoch": 0.757859671038934,
      "grad_norm": 0.5148082375526428,
      "learning_rate": 0.00015381101912646304,
      "loss": 0.84,
      "step": 910
    },
    {
      "epoch": 0.7661877992921091,
      "grad_norm": 0.5683316588401794,
      "learning_rate": 0.0001532400799314873,
      "loss": 0.7748,
      "step": 920
    },
    {
      "epoch": 0.7745159275452842,
      "grad_norm": 0.5283454656600952,
      "learning_rate": 0.00015266914073651158,
      "loss": 0.7603,
      "step": 930
    },
    {
      "epoch": 0.7828440557984593,
      "grad_norm": 0.6947246789932251,
      "learning_rate": 0.00015209820154153583,
      "loss": 0.7825,
      "step": 940
    },
    {
      "epoch": 0.7911721840516344,
      "grad_norm": 0.5581014752388,
      "learning_rate": 0.0001515272623465601,
      "loss": 0.7931,
      "step": 950
    },
    {
      "epoch": 0.7995003123048094,
      "grad_norm": 0.5351816415786743,
      "learning_rate": 0.00015095632315158437,
      "loss": 0.7734,
      "step": 960
    },
    {
      "epoch": 0.8078284405579846,
      "grad_norm": 0.5093077421188354,
      "learning_rate": 0.00015038538395660864,
      "loss": 0.7741,
      "step": 970
    },
    {
      "epoch": 0.8161565688111597,
      "grad_norm": 0.549069344997406,
      "learning_rate": 0.0001498144447616329,
      "loss": 0.7756,
      "step": 980
    },
    {
      "epoch": 0.8244846970643348,
      "grad_norm": 0.43704381585121155,
      "learning_rate": 0.00014924350556665716,
      "loss": 0.7737,
      "step": 990
    },
    {
      "epoch": 0.8328128253175099,
      "grad_norm": 0.5176715850830078,
      "learning_rate": 0.00014867256637168144,
      "loss": 0.779,
      "step": 1000
    },
    {
      "epoch": 0.841140953570685,
      "grad_norm": 0.5527398586273193,
      "learning_rate": 0.00014810162717670568,
      "loss": 0.7749,
      "step": 1010
    },
    {
      "epoch": 0.84946908182386,
      "grad_norm": 0.5887276530265808,
      "learning_rate": 0.00014753068798172995,
      "loss": 0.7401,
      "step": 1020
    },
    {
      "epoch": 0.8577972100770351,
      "grad_norm": 0.5736914277076721,
      "learning_rate": 0.00014695974878675423,
      "loss": 0.7789,
      "step": 1030
    },
    {
      "epoch": 0.8661253383302103,
      "grad_norm": 0.5101610422134399,
      "learning_rate": 0.00014638880959177847,
      "loss": 0.7701,
      "step": 1040
    },
    {
      "epoch": 0.8744534665833854,
      "grad_norm": 0.5535489916801453,
      "learning_rate": 0.00014581787039680274,
      "loss": 0.7396,
      "step": 1050
    },
    {
      "epoch": 0.8827815948365605,
      "grad_norm": 0.5174049735069275,
      "learning_rate": 0.00014524693120182702,
      "loss": 0.7702,
      "step": 1060
    },
    {
      "epoch": 0.8911097230897356,
      "grad_norm": 0.5417133569717407,
      "learning_rate": 0.00014467599200685126,
      "loss": 0.7606,
      "step": 1070
    },
    {
      "epoch": 0.8994378513429107,
      "grad_norm": 0.49147987365722656,
      "learning_rate": 0.00014410505281187554,
      "loss": 0.7621,
      "step": 1080
    },
    {
      "epoch": 0.9077659795960857,
      "grad_norm": 0.546907901763916,
      "learning_rate": 0.0001435341136168998,
      "loss": 0.7604,
      "step": 1090
    },
    {
      "epoch": 0.9160941078492609,
      "grad_norm": 0.5813907980918884,
      "learning_rate": 0.00014296317442192405,
      "loss": 0.8069,
      "step": 1100
    },
    {
      "epoch": 0.924422236102436,
      "grad_norm": 0.5818731188774109,
      "learning_rate": 0.00014239223522694833,
      "loss": 0.8083,
      "step": 1110
    },
    {
      "epoch": 0.9327503643556111,
      "grad_norm": 0.4400225877761841,
      "learning_rate": 0.0001418212960319726,
      "loss": 0.7735,
      "step": 1120
    },
    {
      "epoch": 0.9410784926087862,
      "grad_norm": 0.5275735855102539,
      "learning_rate": 0.00014125035683699685,
      "loss": 0.784,
      "step": 1130
    },
    {
      "epoch": 0.9494066208619613,
      "grad_norm": 0.5440353155136108,
      "learning_rate": 0.00014067941764202112,
      "loss": 0.7661,
      "step": 1140
    },
    {
      "epoch": 0.9577347491151363,
      "grad_norm": 0.5043870806694031,
      "learning_rate": 0.0001401084784470454,
      "loss": 0.7602,
      "step": 1150
    },
    {
      "epoch": 0.9660628773683114,
      "grad_norm": 0.565267026424408,
      "learning_rate": 0.00013953753925206966,
      "loss": 0.7429,
      "step": 1160
    },
    {
      "epoch": 0.9743910056214866,
      "grad_norm": 0.531380295753479,
      "learning_rate": 0.0001389666000570939,
      "loss": 0.7596,
      "step": 1170
    },
    {
      "epoch": 0.9827191338746617,
      "grad_norm": 0.46631190180778503,
      "learning_rate": 0.00013839566086211818,
      "loss": 0.7684,
      "step": 1180
    },
    {
      "epoch": 0.9910472621278368,
      "grad_norm": 0.5102959871292114,
      "learning_rate": 0.00013782472166714245,
      "loss": 0.7543,
      "step": 1190
    },
    {
      "epoch": 0.9993753903810119,
      "grad_norm": 0.5351875424385071,
      "learning_rate": 0.00013725378247216673,
      "loss": 0.7516,
      "step": 1200
    }
  ],
  "logging_steps": 10,
  "max_steps": 3603,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7648639762563072.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
