{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3603,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0083281282531751,
      "grad_norm": 23.824613571166992,
      "learning_rate": 1.8e-05,
      "loss": 11.6,
      "step": 10
    },
    {
      "epoch": 0.0166562565063502,
      "grad_norm": 45.750244140625,
      "learning_rate": 3.8e-05,
      "loss": 10.5985,
      "step": 20
    },
    {
      "epoch": 0.024984384759525295,
      "grad_norm": 20.440067291259766,
      "learning_rate": 5.8e-05,
      "loss": 6.3897,
      "step": 30
    },
    {
      "epoch": 0.0333125130127004,
      "grad_norm": 15.120610237121582,
      "learning_rate": 7.800000000000001e-05,
      "loss": 3.6727,
      "step": 40
    },
    {
      "epoch": 0.041640641265875494,
      "grad_norm": 1.9982540607452393,
      "learning_rate": 9.8e-05,
      "loss": 1.7822,
      "step": 50
    },
    {
      "epoch": 0.04996876951905059,
      "grad_norm": 1.4120821952819824,
      "learning_rate": 0.000118,
      "loss": 1.1766,
      "step": 60
    },
    {
      "epoch": 0.05829689777222569,
      "grad_norm": 0.9169183969497681,
      "learning_rate": 0.000138,
      "loss": 0.9723,
      "step": 70
    },
    {
      "epoch": 0.0666250260254008,
      "grad_norm": 0.8249570727348328,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.896,
      "step": 80
    },
    {
      "epoch": 0.07495315427857589,
      "grad_norm": 0.7147775292396545,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.8131,
      "step": 90
    },
    {
      "epoch": 0.08328128253175099,
      "grad_norm": 0.7982261180877686,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.8204,
      "step": 100
    },
    {
      "epoch": 0.09160941078492608,
      "grad_norm": 0.8132607340812683,
      "learning_rate": 0.00019948615472452184,
      "loss": 0.8226,
      "step": 110
    },
    {
      "epoch": 0.09993753903810118,
      "grad_norm": 0.7513646483421326,
      "learning_rate": 0.00019891521552954612,
      "loss": 0.8209,
      "step": 120
    },
    {
      "epoch": 0.10826566729127629,
      "grad_norm": 0.8384955525398254,
      "learning_rate": 0.0001983442763345704,
      "loss": 0.8297,
      "step": 130
    },
    {
      "epoch": 0.11659379554445139,
      "grad_norm": 0.8373925685882568,
      "learning_rate": 0.00019777333713959466,
      "loss": 0.8689,
      "step": 140
    },
    {
      "epoch": 0.12492192379762648,
      "grad_norm": 0.8194049000740051,
      "learning_rate": 0.0001972023979446189,
      "loss": 0.8003,
      "step": 150
    },
    {
      "epoch": 0.1332500520508016,
      "grad_norm": 0.7251324653625488,
      "learning_rate": 0.00019663145874964318,
      "loss": 0.7982,
      "step": 160
    },
    {
      "epoch": 0.14157818030397668,
      "grad_norm": 0.7248123288154602,
      "learning_rate": 0.00019606051955466745,
      "loss": 0.8227,
      "step": 170
    },
    {
      "epoch": 0.14990630855715179,
      "grad_norm": 0.58534175157547,
      "learning_rate": 0.0001954895803596917,
      "loss": 0.7469,
      "step": 180
    },
    {
      "epoch": 0.15823443681032687,
      "grad_norm": 0.7220483422279358,
      "learning_rate": 0.00019491864116471597,
      "loss": 0.8149,
      "step": 190
    },
    {
      "epoch": 0.16656256506350198,
      "grad_norm": 0.7826076745986938,
      "learning_rate": 0.00019434770196974024,
      "loss": 0.8098,
      "step": 200
    },
    {
      "epoch": 0.1748906933166771,
      "grad_norm": 0.6980066299438477,
      "learning_rate": 0.0001937767627747645,
      "loss": 0.8058,
      "step": 210
    },
    {
      "epoch": 0.18321882156985217,
      "grad_norm": 0.5895638465881348,
      "learning_rate": 0.00019320582357978876,
      "loss": 0.7887,
      "step": 220
    },
    {
      "epoch": 0.19154694982302728,
      "grad_norm": 0.6908169984817505,
      "learning_rate": 0.00019263488438481304,
      "loss": 0.7975,
      "step": 230
    },
    {
      "epoch": 0.19987507807620236,
      "grad_norm": 0.760324239730835,
      "learning_rate": 0.0001920639451898373,
      "loss": 0.8216,
      "step": 240
    },
    {
      "epoch": 0.20820320632937747,
      "grad_norm": 0.6377607583999634,
      "learning_rate": 0.00019149300599486155,
      "loss": 0.8046,
      "step": 250
    },
    {
      "epoch": 0.21653133458255258,
      "grad_norm": 0.597603440284729,
      "learning_rate": 0.00019092206679988583,
      "loss": 0.768,
      "step": 260
    },
    {
      "epoch": 0.22485946283572766,
      "grad_norm": 0.6276570558547974,
      "learning_rate": 0.0001903511276049101,
      "loss": 0.8114,
      "step": 270
    },
    {
      "epoch": 0.23318759108890277,
      "grad_norm": 0.6776286959648132,
      "learning_rate": 0.00018978018840993434,
      "loss": 0.8007,
      "step": 280
    },
    {
      "epoch": 0.24151571934207786,
      "grad_norm": 0.5963298082351685,
      "learning_rate": 0.00018920924921495862,
      "loss": 0.7718,
      "step": 290
    },
    {
      "epoch": 0.24984384759525297,
      "grad_norm": 0.5905337333679199,
      "learning_rate": 0.0001886383100199829,
      "loss": 0.7912,
      "step": 300
    },
    {
      "epoch": 0.25817197584842805,
      "grad_norm": 0.5971078276634216,
      "learning_rate": 0.00018806737082500714,
      "loss": 0.799,
      "step": 310
    },
    {
      "epoch": 0.2665001041016032,
      "grad_norm": 0.6722333431243896,
      "learning_rate": 0.0001874964316300314,
      "loss": 0.792,
      "step": 320
    },
    {
      "epoch": 0.27482823235477827,
      "grad_norm": 0.6170521974563599,
      "learning_rate": 0.00018692549243505568,
      "loss": 0.818,
      "step": 330
    },
    {
      "epoch": 0.28315636060795335,
      "grad_norm": 0.6387074589729309,
      "learning_rate": 0.00018635455324007993,
      "loss": 0.7744,
      "step": 340
    },
    {
      "epoch": 0.2914844888611285,
      "grad_norm": 0.5736905932426453,
      "learning_rate": 0.0001857836140451042,
      "loss": 0.7607,
      "step": 350
    },
    {
      "epoch": 0.29981261711430357,
      "grad_norm": 0.6516372561454773,
      "learning_rate": 0.00018521267485012847,
      "loss": 0.7978,
      "step": 360
    },
    {
      "epoch": 0.30814074536747865,
      "grad_norm": 0.6283776760101318,
      "learning_rate": 0.00018464173565515272,
      "loss": 0.7782,
      "step": 370
    },
    {
      "epoch": 0.31646887362065373,
      "grad_norm": 0.6114363670349121,
      "learning_rate": 0.000184070796460177,
      "loss": 0.7832,
      "step": 380
    },
    {
      "epoch": 0.32479700187382887,
      "grad_norm": 0.5515420436859131,
      "learning_rate": 0.00018349985726520126,
      "loss": 0.7841,
      "step": 390
    },
    {
      "epoch": 0.33312513012700395,
      "grad_norm": 0.5090296864509583,
      "learning_rate": 0.0001829289180702255,
      "loss": 0.8289,
      "step": 400
    },
    {
      "epoch": 0.34145325838017904,
      "grad_norm": 0.6078847050666809,
      "learning_rate": 0.00018235797887524978,
      "loss": 0.7795,
      "step": 410
    },
    {
      "epoch": 0.3497813866333542,
      "grad_norm": 0.6059353351593018,
      "learning_rate": 0.00018178703968027405,
      "loss": 0.7685,
      "step": 420
    },
    {
      "epoch": 0.35810951488652926,
      "grad_norm": 0.7710912823677063,
      "learning_rate": 0.00018121610048529833,
      "loss": 0.782,
      "step": 430
    },
    {
      "epoch": 0.36643764313970434,
      "grad_norm": 0.5963722467422485,
      "learning_rate": 0.00018064516129032257,
      "loss": 0.7783,
      "step": 440
    },
    {
      "epoch": 0.3747657713928795,
      "grad_norm": 0.5401337146759033,
      "learning_rate": 0.00018007422209534684,
      "loss": 0.7703,
      "step": 450
    },
    {
      "epoch": 0.38309389964605456,
      "grad_norm": 0.5642045140266418,
      "learning_rate": 0.00017950328290037112,
      "loss": 0.7919,
      "step": 460
    },
    {
      "epoch": 0.39142202789922964,
      "grad_norm": 0.5768611431121826,
      "learning_rate": 0.0001789323437053954,
      "loss": 0.7746,
      "step": 470
    },
    {
      "epoch": 0.3997501561524047,
      "grad_norm": 0.7498918771743774,
      "learning_rate": 0.00017836140451041966,
      "loss": 0.7664,
      "step": 480
    },
    {
      "epoch": 0.40807828440557986,
      "grad_norm": 0.5459244251251221,
      "learning_rate": 0.0001777904653154439,
      "loss": 0.7787,
      "step": 490
    },
    {
      "epoch": 0.41640641265875494,
      "grad_norm": 0.5505867004394531,
      "learning_rate": 0.00017721952612046818,
      "loss": 0.7759,
      "step": 500
    },
    {
      "epoch": 0.42473454091193,
      "grad_norm": 0.6023789644241333,
      "learning_rate": 0.00017664858692549245,
      "loss": 0.7886,
      "step": 510
    },
    {
      "epoch": 0.43306266916510516,
      "grad_norm": 0.5613372325897217,
      "learning_rate": 0.00017607764773051673,
      "loss": 0.736,
      "step": 520
    },
    {
      "epoch": 0.44139079741828025,
      "grad_norm": 0.6594305038452148,
      "learning_rate": 0.00017550670853554097,
      "loss": 0.8212,
      "step": 530
    },
    {
      "epoch": 0.4497189256714553,
      "grad_norm": 0.5827054977416992,
      "learning_rate": 0.00017493576934056524,
      "loss": 0.772,
      "step": 540
    },
    {
      "epoch": 0.45804705392463047,
      "grad_norm": 0.5854480266571045,
      "learning_rate": 0.00017436483014558952,
      "loss": 0.7649,
      "step": 550
    },
    {
      "epoch": 0.46637518217780555,
      "grad_norm": 0.6113579869270325,
      "learning_rate": 0.00017379389095061376,
      "loss": 0.7902,
      "step": 560
    },
    {
      "epoch": 0.47470331043098063,
      "grad_norm": 0.5400941967964172,
      "learning_rate": 0.00017322295175563804,
      "loss": 0.8481,
      "step": 570
    },
    {
      "epoch": 0.4830314386841557,
      "grad_norm": 0.5883111357688904,
      "learning_rate": 0.0001726520125606623,
      "loss": 0.7569,
      "step": 580
    },
    {
      "epoch": 0.49135956693733085,
      "grad_norm": 0.58284991979599,
      "learning_rate": 0.00017208107336568658,
      "loss": 0.7511,
      "step": 590
    },
    {
      "epoch": 0.49968769519050593,
      "grad_norm": 0.5242885947227478,
      "learning_rate": 0.00017151013417071083,
      "loss": 0.7849,
      "step": 600
    },
    {
      "epoch": 0.508015823443681,
      "grad_norm": 0.6077029705047607,
      "learning_rate": 0.0001709391949757351,
      "loss": 0.799,
      "step": 610
    },
    {
      "epoch": 0.5163439516968561,
      "grad_norm": 0.49672332406044006,
      "learning_rate": 0.00017036825578075937,
      "loss": 0.7643,
      "step": 620
    },
    {
      "epoch": 0.5246720799500312,
      "grad_norm": 0.5276833176612854,
      "learning_rate": 0.00016979731658578362,
      "loss": 0.8031,
      "step": 630
    },
    {
      "epoch": 0.5330002082032064,
      "grad_norm": 0.5235550999641418,
      "learning_rate": 0.0001692263773908079,
      "loss": 0.8017,
      "step": 640
    },
    {
      "epoch": 0.5413283364563815,
      "grad_norm": 0.603817343711853,
      "learning_rate": 0.00016865543819583216,
      "loss": 0.7745,
      "step": 650
    },
    {
      "epoch": 0.5496564647095565,
      "grad_norm": 0.5543803572654724,
      "learning_rate": 0.0001680844990008564,
      "loss": 0.7715,
      "step": 660
    },
    {
      "epoch": 0.5579845929627316,
      "grad_norm": 0.5668521523475647,
      "learning_rate": 0.00016751355980588068,
      "loss": 0.7449,
      "step": 670
    },
    {
      "epoch": 0.5663127212159067,
      "grad_norm": 0.5429356694221497,
      "learning_rate": 0.00016694262061090495,
      "loss": 0.7604,
      "step": 680
    },
    {
      "epoch": 0.5746408494690818,
      "grad_norm": 0.5166087746620178,
      "learning_rate": 0.0001663716814159292,
      "loss": 0.7623,
      "step": 690
    },
    {
      "epoch": 0.582968977722257,
      "grad_norm": 0.655295193195343,
      "learning_rate": 0.00016580074222095347,
      "loss": 0.7757,
      "step": 700
    },
    {
      "epoch": 0.5912971059754321,
      "grad_norm": 0.4828481376171112,
      "learning_rate": 0.00016522980302597774,
      "loss": 0.7632,
      "step": 710
    },
    {
      "epoch": 0.5996252342286071,
      "grad_norm": 0.5310595035552979,
      "learning_rate": 0.000164658863831002,
      "loss": 0.7631,
      "step": 720
    },
    {
      "epoch": 0.6079533624817822,
      "grad_norm": 0.5404865741729736,
      "learning_rate": 0.00016408792463602626,
      "loss": 0.7834,
      "step": 730
    },
    {
      "epoch": 0.6162814907349573,
      "grad_norm": 0.49601438641548157,
      "learning_rate": 0.00016351698544105054,
      "loss": 0.7705,
      "step": 740
    },
    {
      "epoch": 0.6246096189881324,
      "grad_norm": 0.5294772386550903,
      "learning_rate": 0.00016294604624607478,
      "loss": 0.7899,
      "step": 750
    },
    {
      "epoch": 0.6329377472413075,
      "grad_norm": 0.6693053841590881,
      "learning_rate": 0.00016237510705109905,
      "loss": 0.7741,
      "step": 760
    },
    {
      "epoch": 0.6412658754944827,
      "grad_norm": 0.5322103500366211,
      "learning_rate": 0.00016180416785612333,
      "loss": 0.7748,
      "step": 770
    },
    {
      "epoch": 0.6495940037476577,
      "grad_norm": 0.6172773241996765,
      "learning_rate": 0.0001612332286611476,
      "loss": 0.7792,
      "step": 780
    },
    {
      "epoch": 0.6579221320008328,
      "grad_norm": 0.5860980153083801,
      "learning_rate": 0.00016066228946617184,
      "loss": 0.7998,
      "step": 790
    },
    {
      "epoch": 0.6662502602540079,
      "grad_norm": 0.49824315309524536,
      "learning_rate": 0.00016009135027119612,
      "loss": 0.7459,
      "step": 800
    },
    {
      "epoch": 0.674578388507183,
      "grad_norm": 0.5096157193183899,
      "learning_rate": 0.0001595204110762204,
      "loss": 0.7656,
      "step": 810
    },
    {
      "epoch": 0.6829065167603581,
      "grad_norm": 0.5909919142723083,
      "learning_rate": 0.00015894947188124464,
      "loss": 0.7966,
      "step": 820
    },
    {
      "epoch": 0.6912346450135332,
      "grad_norm": 0.5401273965835571,
      "learning_rate": 0.0001583785326862689,
      "loss": 0.7456,
      "step": 830
    },
    {
      "epoch": 0.6995627732667083,
      "grad_norm": 0.5608735680580139,
      "learning_rate": 0.00015780759349129318,
      "loss": 0.7621,
      "step": 840
    },
    {
      "epoch": 0.7078909015198834,
      "grad_norm": 0.5576484203338623,
      "learning_rate": 0.00015723665429631745,
      "loss": 0.7607,
      "step": 850
    },
    {
      "epoch": 0.7162190297730585,
      "grad_norm": 0.5092172622680664,
      "learning_rate": 0.00015666571510134173,
      "loss": 0.7669,
      "step": 860
    },
    {
      "epoch": 0.7245471580262336,
      "grad_norm": 0.5104865431785583,
      "learning_rate": 0.000156094775906366,
      "loss": 0.7715,
      "step": 870
    },
    {
      "epoch": 0.7328752862794087,
      "grad_norm": 0.5867599248886108,
      "learning_rate": 0.00015552383671139024,
      "loss": 0.7962,
      "step": 880
    },
    {
      "epoch": 0.7412034145325838,
      "grad_norm": 0.5592814683914185,
      "learning_rate": 0.00015495289751641452,
      "loss": 0.7825,
      "step": 890
    },
    {
      "epoch": 0.749531542785759,
      "grad_norm": 0.5156508088111877,
      "learning_rate": 0.0001543819583214388,
      "loss": 0.7941,
      "step": 900
    },
    {
      "epoch": 0.757859671038934,
      "grad_norm": 0.5148082375526428,
      "learning_rate": 0.00015381101912646304,
      "loss": 0.84,
      "step": 910
    },
    {
      "epoch": 0.7661877992921091,
      "grad_norm": 0.5683316588401794,
      "learning_rate": 0.0001532400799314873,
      "loss": 0.7748,
      "step": 920
    },
    {
      "epoch": 0.7745159275452842,
      "grad_norm": 0.5283454656600952,
      "learning_rate": 0.00015266914073651158,
      "loss": 0.7603,
      "step": 930
    },
    {
      "epoch": 0.7828440557984593,
      "grad_norm": 0.6947246789932251,
      "learning_rate": 0.00015209820154153583,
      "loss": 0.7825,
      "step": 940
    },
    {
      "epoch": 0.7911721840516344,
      "grad_norm": 0.5581014752388,
      "learning_rate": 0.0001515272623465601,
      "loss": 0.7931,
      "step": 950
    },
    {
      "epoch": 0.7995003123048094,
      "grad_norm": 0.5351816415786743,
      "learning_rate": 0.00015095632315158437,
      "loss": 0.7734,
      "step": 960
    },
    {
      "epoch": 0.8078284405579846,
      "grad_norm": 0.5093077421188354,
      "learning_rate": 0.00015038538395660864,
      "loss": 0.7741,
      "step": 970
    },
    {
      "epoch": 0.8161565688111597,
      "grad_norm": 0.549069344997406,
      "learning_rate": 0.0001498144447616329,
      "loss": 0.7756,
      "step": 980
    },
    {
      "epoch": 0.8244846970643348,
      "grad_norm": 0.43704381585121155,
      "learning_rate": 0.00014924350556665716,
      "loss": 0.7737,
      "step": 990
    },
    {
      "epoch": 0.8328128253175099,
      "grad_norm": 0.5176715850830078,
      "learning_rate": 0.00014867256637168144,
      "loss": 0.779,
      "step": 1000
    },
    {
      "epoch": 0.841140953570685,
      "grad_norm": 0.5527398586273193,
      "learning_rate": 0.00014810162717670568,
      "loss": 0.7749,
      "step": 1010
    },
    {
      "epoch": 0.84946908182386,
      "grad_norm": 0.5887276530265808,
      "learning_rate": 0.00014753068798172995,
      "loss": 0.7401,
      "step": 1020
    },
    {
      "epoch": 0.8577972100770351,
      "grad_norm": 0.5736914277076721,
      "learning_rate": 0.00014695974878675423,
      "loss": 0.7789,
      "step": 1030
    },
    {
      "epoch": 0.8661253383302103,
      "grad_norm": 0.5101610422134399,
      "learning_rate": 0.00014638880959177847,
      "loss": 0.7701,
      "step": 1040
    },
    {
      "epoch": 0.8744534665833854,
      "grad_norm": 0.5535489916801453,
      "learning_rate": 0.00014581787039680274,
      "loss": 0.7396,
      "step": 1050
    },
    {
      "epoch": 0.8827815948365605,
      "grad_norm": 0.5174049735069275,
      "learning_rate": 0.00014524693120182702,
      "loss": 0.7702,
      "step": 1060
    },
    {
      "epoch": 0.8911097230897356,
      "grad_norm": 0.5417133569717407,
      "learning_rate": 0.00014467599200685126,
      "loss": 0.7606,
      "step": 1070
    },
    {
      "epoch": 0.8994378513429107,
      "grad_norm": 0.49147987365722656,
      "learning_rate": 0.00014410505281187554,
      "loss": 0.7621,
      "step": 1080
    },
    {
      "epoch": 0.9077659795960857,
      "grad_norm": 0.546907901763916,
      "learning_rate": 0.0001435341136168998,
      "loss": 0.7604,
      "step": 1090
    },
    {
      "epoch": 0.9160941078492609,
      "grad_norm": 0.5813907980918884,
      "learning_rate": 0.00014296317442192405,
      "loss": 0.8069,
      "step": 1100
    },
    {
      "epoch": 0.924422236102436,
      "grad_norm": 0.5818731188774109,
      "learning_rate": 0.00014239223522694833,
      "loss": 0.8083,
      "step": 1110
    },
    {
      "epoch": 0.9327503643556111,
      "grad_norm": 0.4400225877761841,
      "learning_rate": 0.0001418212960319726,
      "loss": 0.7735,
      "step": 1120
    },
    {
      "epoch": 0.9410784926087862,
      "grad_norm": 0.5275735855102539,
      "learning_rate": 0.00014125035683699685,
      "loss": 0.784,
      "step": 1130
    },
    {
      "epoch": 0.9494066208619613,
      "grad_norm": 0.5440353155136108,
      "learning_rate": 0.00014067941764202112,
      "loss": 0.7661,
      "step": 1140
    },
    {
      "epoch": 0.9577347491151363,
      "grad_norm": 0.5043870806694031,
      "learning_rate": 0.0001401084784470454,
      "loss": 0.7602,
      "step": 1150
    },
    {
      "epoch": 0.9660628773683114,
      "grad_norm": 0.565267026424408,
      "learning_rate": 0.00013953753925206966,
      "loss": 0.7429,
      "step": 1160
    },
    {
      "epoch": 0.9743910056214866,
      "grad_norm": 0.531380295753479,
      "learning_rate": 0.0001389666000570939,
      "loss": 0.7596,
      "step": 1170
    },
    {
      "epoch": 0.9827191338746617,
      "grad_norm": 0.46631190180778503,
      "learning_rate": 0.00013839566086211818,
      "loss": 0.7684,
      "step": 1180
    },
    {
      "epoch": 0.9910472621278368,
      "grad_norm": 0.5102959871292114,
      "learning_rate": 0.00013782472166714245,
      "loss": 0.7543,
      "step": 1190
    },
    {
      "epoch": 0.9993753903810119,
      "grad_norm": 0.5351875424385071,
      "learning_rate": 0.00013725378247216673,
      "loss": 0.7516,
      "step": 1200
    },
    {
      "epoch": 1.0074953154278576,
      "grad_norm": 0.5926082730293274,
      "learning_rate": 0.000136682843277191,
      "loss": 0.7609,
      "step": 1210
    },
    {
      "epoch": 1.0158234436810327,
      "grad_norm": 0.519608736038208,
      "learning_rate": 0.00013611190408221525,
      "loss": 0.7094,
      "step": 1220
    },
    {
      "epoch": 1.0241515719342078,
      "grad_norm": 0.5353900194168091,
      "learning_rate": 0.00013554096488723952,
      "loss": 0.7407,
      "step": 1230
    },
    {
      "epoch": 1.0324797001873829,
      "grad_norm": 0.517781138420105,
      "learning_rate": 0.0001349700256922638,
      "loss": 0.7589,
      "step": 1240
    },
    {
      "epoch": 1.040807828440558,
      "grad_norm": 0.5366228222846985,
      "learning_rate": 0.00013439908649728806,
      "loss": 0.7201,
      "step": 1250
    },
    {
      "epoch": 1.049135956693733,
      "grad_norm": 0.4817763566970825,
      "learning_rate": 0.0001338281473023123,
      "loss": 0.785,
      "step": 1260
    },
    {
      "epoch": 1.0574640849469081,
      "grad_norm": 0.524955153465271,
      "learning_rate": 0.00013325720810733658,
      "loss": 0.7267,
      "step": 1270
    },
    {
      "epoch": 1.0657922132000832,
      "grad_norm": 0.5737389922142029,
      "learning_rate": 0.00013268626891236085,
      "loss": 0.778,
      "step": 1280
    },
    {
      "epoch": 1.0741203414532583,
      "grad_norm": 0.5352047681808472,
      "learning_rate": 0.0001321153297173851,
      "loss": 0.7967,
      "step": 1290
    },
    {
      "epoch": 1.0824484697064334,
      "grad_norm": 0.5315895676612854,
      "learning_rate": 0.00013154439052240937,
      "loss": 0.7543,
      "step": 1300
    },
    {
      "epoch": 1.0907765979596085,
      "grad_norm": 0.5481370091438293,
      "learning_rate": 0.00013097345132743365,
      "loss": 0.7594,
      "step": 1310
    },
    {
      "epoch": 1.0991047262127838,
      "grad_norm": 0.5549591183662415,
      "learning_rate": 0.00013040251213245792,
      "loss": 0.7331,
      "step": 1320
    },
    {
      "epoch": 1.1074328544659588,
      "grad_norm": 0.5276317596435547,
      "learning_rate": 0.00012983157293748216,
      "loss": 0.7438,
      "step": 1330
    },
    {
      "epoch": 1.115760982719134,
      "grad_norm": 0.5282708406448364,
      "learning_rate": 0.00012926063374250644,
      "loss": 0.7083,
      "step": 1340
    },
    {
      "epoch": 1.124089110972309,
      "grad_norm": 0.5655364990234375,
      "learning_rate": 0.0001286896945475307,
      "loss": 0.7439,
      "step": 1350
    },
    {
      "epoch": 1.132417239225484,
      "grad_norm": 0.6702127456665039,
      "learning_rate": 0.00012811875535255495,
      "loss": 0.7623,
      "step": 1360
    },
    {
      "epoch": 1.1407453674786592,
      "grad_norm": 0.5132356286048889,
      "learning_rate": 0.00012754781615757923,
      "loss": 0.748,
      "step": 1370
    },
    {
      "epoch": 1.1490734957318343,
      "grad_norm": 0.5308963656425476,
      "learning_rate": 0.0001269768769626035,
      "loss": 0.745,
      "step": 1380
    },
    {
      "epoch": 1.1574016239850093,
      "grad_norm": 0.5107733607292175,
      "learning_rate": 0.00012640593776762775,
      "loss": 0.7402,
      "step": 1390
    },
    {
      "epoch": 1.1657297522381844,
      "grad_norm": 0.6543974876403809,
      "learning_rate": 0.00012583499857265202,
      "loss": 0.76,
      "step": 1400
    },
    {
      "epoch": 1.1740578804913595,
      "grad_norm": 0.5872929096221924,
      "learning_rate": 0.0001252640593776763,
      "loss": 0.7469,
      "step": 1410
    },
    {
      "epoch": 1.1823860087445346,
      "grad_norm": 0.5402197241783142,
      "learning_rate": 0.00012469312018270054,
      "loss": 0.7651,
      "step": 1420
    },
    {
      "epoch": 1.1907141369977097,
      "grad_norm": 0.5554109215736389,
      "learning_rate": 0.0001241221809877248,
      "loss": 0.7543,
      "step": 1430
    },
    {
      "epoch": 1.1990422652508848,
      "grad_norm": 0.5340924263000488,
      "learning_rate": 0.00012355124179274908,
      "loss": 0.7485,
      "step": 1440
    },
    {
      "epoch": 1.20737039350406,
      "grad_norm": 0.5616198778152466,
      "learning_rate": 0.00012298030259777333,
      "loss": 0.7475,
      "step": 1450
    },
    {
      "epoch": 1.2156985217572351,
      "grad_norm": 0.5712025165557861,
      "learning_rate": 0.0001224093634027976,
      "loss": 0.7565,
      "step": 1460
    },
    {
      "epoch": 1.2240266500104102,
      "grad_norm": 0.5363101363182068,
      "learning_rate": 0.00012183842420782187,
      "loss": 0.7376,
      "step": 1470
    },
    {
      "epoch": 1.2323547782635853,
      "grad_norm": 0.5171469449996948,
      "learning_rate": 0.00012126748501284613,
      "loss": 0.7502,
      "step": 1480
    },
    {
      "epoch": 1.2406829065167604,
      "grad_norm": 0.48432135581970215,
      "learning_rate": 0.0001206965458178704,
      "loss": 0.713,
      "step": 1490
    },
    {
      "epoch": 1.2490110347699355,
      "grad_norm": 0.5215731263160706,
      "learning_rate": 0.00012012560662289468,
      "loss": 0.7415,
      "step": 1500
    },
    {
      "epoch": 1.2573391630231106,
      "grad_norm": 0.492549329996109,
      "learning_rate": 0.00011955466742791894,
      "loss": 0.76,
      "step": 1510
    },
    {
      "epoch": 1.2656672912762856,
      "grad_norm": 0.5335714221000671,
      "learning_rate": 0.0001189837282329432,
      "loss": 0.7902,
      "step": 1520
    },
    {
      "epoch": 1.2739954195294607,
      "grad_norm": 0.6450555324554443,
      "learning_rate": 0.00011841278903796747,
      "loss": 0.766,
      "step": 1530
    },
    {
      "epoch": 1.2823235477826358,
      "grad_norm": 0.581842303276062,
      "learning_rate": 0.00011784184984299174,
      "loss": 0.7751,
      "step": 1540
    },
    {
      "epoch": 1.2906516760358109,
      "grad_norm": 0.551160991191864,
      "learning_rate": 0.00011727091064801599,
      "loss": 0.7231,
      "step": 1550
    },
    {
      "epoch": 1.2989798042889862,
      "grad_norm": 0.5536897778511047,
      "learning_rate": 0.00011669997145304026,
      "loss": 0.7346,
      "step": 1560
    },
    {
      "epoch": 1.307307932542161,
      "grad_norm": 0.5846131443977356,
      "learning_rate": 0.00011612903225806453,
      "loss": 0.7328,
      "step": 1570
    },
    {
      "epoch": 1.3156360607953363,
      "grad_norm": 0.5979673266410828,
      "learning_rate": 0.00011555809306308878,
      "loss": 0.7607,
      "step": 1580
    },
    {
      "epoch": 1.3239641890485112,
      "grad_norm": 0.527472972869873,
      "learning_rate": 0.00011498715386811305,
      "loss": 0.7298,
      "step": 1590
    },
    {
      "epoch": 1.3322923173016865,
      "grad_norm": 0.560035765171051,
      "learning_rate": 0.00011441621467313732,
      "loss": 0.7173,
      "step": 1600
    },
    {
      "epoch": 1.3406204455548616,
      "grad_norm": 0.5512088537216187,
      "learning_rate": 0.00011384527547816157,
      "loss": 0.7717,
      "step": 1610
    },
    {
      "epoch": 1.3489485738080367,
      "grad_norm": 0.5368456840515137,
      "learning_rate": 0.00011327433628318584,
      "loss": 0.7696,
      "step": 1620
    },
    {
      "epoch": 1.3572767020612118,
      "grad_norm": 0.5828245282173157,
      "learning_rate": 0.00011270339708821011,
      "loss": 0.7495,
      "step": 1630
    },
    {
      "epoch": 1.3656048303143868,
      "grad_norm": 0.5414822101593018,
      "learning_rate": 0.00011213245789323437,
      "loss": 0.7652,
      "step": 1640
    },
    {
      "epoch": 1.373932958567562,
      "grad_norm": 0.5588496327400208,
      "learning_rate": 0.00011156151869825863,
      "loss": 0.7352,
      "step": 1650
    },
    {
      "epoch": 1.382261086820737,
      "grad_norm": 0.6099138259887695,
      "learning_rate": 0.0001109905795032829,
      "loss": 0.7418,
      "step": 1660
    },
    {
      "epoch": 1.390589215073912,
      "grad_norm": 0.5250871181488037,
      "learning_rate": 0.00011041964030830716,
      "loss": 0.7717,
      "step": 1670
    },
    {
      "epoch": 1.3989173433270872,
      "grad_norm": 0.6001952886581421,
      "learning_rate": 0.00010984870111333144,
      "loss": 0.7447,
      "step": 1680
    },
    {
      "epoch": 1.4072454715802625,
      "grad_norm": 0.6242745518684387,
      "learning_rate": 0.00010927776191835571,
      "loss": 0.7567,
      "step": 1690
    },
    {
      "epoch": 1.4155735998334373,
      "grad_norm": 0.5680369734764099,
      "learning_rate": 0.00010870682272337998,
      "loss": 0.7243,
      "step": 1700
    },
    {
      "epoch": 1.4239017280866126,
      "grad_norm": 0.5359448194503784,
      "learning_rate": 0.00010813588352840423,
      "loss": 0.7384,
      "step": 1710
    },
    {
      "epoch": 1.4322298563397875,
      "grad_norm": 0.507286548614502,
      "learning_rate": 0.0001075649443334285,
      "loss": 0.7265,
      "step": 1720
    },
    {
      "epoch": 1.4405579845929628,
      "grad_norm": 0.5739713907241821,
      "learning_rate": 0.00010699400513845277,
      "loss": 0.732,
      "step": 1730
    },
    {
      "epoch": 1.4488861128461379,
      "grad_norm": 0.530808687210083,
      "learning_rate": 0.00010642306594347702,
      "loss": 0.7692,
      "step": 1740
    },
    {
      "epoch": 1.457214241099313,
      "grad_norm": 0.5544137358665466,
      "learning_rate": 0.00010585212674850129,
      "loss": 0.7343,
      "step": 1750
    },
    {
      "epoch": 1.465542369352488,
      "grad_norm": 0.49795234203338623,
      "learning_rate": 0.00010528118755352556,
      "loss": 0.7259,
      "step": 1760
    },
    {
      "epoch": 1.4738704976056631,
      "grad_norm": 0.5295220017433167,
      "learning_rate": 0.00010471024835854981,
      "loss": 0.7344,
      "step": 1770
    },
    {
      "epoch": 1.4821986258588382,
      "grad_norm": 0.6018770933151245,
      "learning_rate": 0.00010413930916357408,
      "loss": 0.7561,
      "step": 1780
    },
    {
      "epoch": 1.4905267541120133,
      "grad_norm": 0.6146726608276367,
      "learning_rate": 0.00010356836996859835,
      "loss": 0.7717,
      "step": 1790
    },
    {
      "epoch": 1.4988548823651884,
      "grad_norm": 0.542021632194519,
      "learning_rate": 0.0001029974307736226,
      "loss": 0.7488,
      "step": 1800
    },
    {
      "epoch": 1.5071830106183635,
      "grad_norm": 0.6243857145309448,
      "learning_rate": 0.00010242649157864687,
      "loss": 0.7245,
      "step": 1810
    },
    {
      "epoch": 1.5155111388715388,
      "grad_norm": 0.5066047310829163,
      "learning_rate": 0.00010185555238367115,
      "loss": 0.7317,
      "step": 1820
    },
    {
      "epoch": 1.5238392671247136,
      "grad_norm": 0.6073366403579712,
      "learning_rate": 0.0001012846131886954,
      "loss": 0.781,
      "step": 1830
    },
    {
      "epoch": 1.532167395377889,
      "grad_norm": 0.4950423836708069,
      "learning_rate": 0.00010071367399371966,
      "loss": 0.7446,
      "step": 1840
    },
    {
      "epoch": 1.5404955236310638,
      "grad_norm": 0.6082502007484436,
      "learning_rate": 0.00010014273479874394,
      "loss": 0.7577,
      "step": 1850
    },
    {
      "epoch": 1.548823651884239,
      "grad_norm": 0.6058792471885681,
      "learning_rate": 9.957179560376821e-05,
      "loss": 0.7306,
      "step": 1860
    },
    {
      "epoch": 1.557151780137414,
      "grad_norm": 0.4835642874240875,
      "learning_rate": 9.900085640879247e-05,
      "loss": 0.7386,
      "step": 1870
    },
    {
      "epoch": 1.5654799083905893,
      "grad_norm": 0.5892341136932373,
      "learning_rate": 9.842991721381674e-05,
      "loss": 0.7666,
      "step": 1880
    },
    {
      "epoch": 1.5738080366437643,
      "grad_norm": 0.6260331869125366,
      "learning_rate": 9.7858978018841e-05,
      "loss": 0.778,
      "step": 1890
    },
    {
      "epoch": 1.5821361648969394,
      "grad_norm": 0.5706144571304321,
      "learning_rate": 9.728803882386527e-05,
      "loss": 0.7342,
      "step": 1900
    },
    {
      "epoch": 1.5904642931501145,
      "grad_norm": 0.5742250680923462,
      "learning_rate": 9.671709962888953e-05,
      "loss": 0.7969,
      "step": 1910
    },
    {
      "epoch": 1.5987924214032896,
      "grad_norm": 0.5342006683349609,
      "learning_rate": 9.614616043391379e-05,
      "loss": 0.7261,
      "step": 1920
    },
    {
      "epoch": 1.6071205496564647,
      "grad_norm": 0.49549004435539246,
      "learning_rate": 9.557522123893806e-05,
      "loss": 0.724,
      "step": 1930
    },
    {
      "epoch": 1.6154486779096398,
      "grad_norm": 0.5007038116455078,
      "learning_rate": 9.500428204396232e-05,
      "loss": 0.739,
      "step": 1940
    },
    {
      "epoch": 1.623776806162815,
      "grad_norm": 0.5757259130477905,
      "learning_rate": 9.443334284898658e-05,
      "loss": 0.7696,
      "step": 1950
    },
    {
      "epoch": 1.63210493441599,
      "grad_norm": 0.49447137117385864,
      "learning_rate": 9.386240365401085e-05,
      "loss": 0.7609,
      "step": 1960
    },
    {
      "epoch": 1.6404330626691652,
      "grad_norm": 0.5489360690116882,
      "learning_rate": 9.329146445903511e-05,
      "loss": 0.7416,
      "step": 1970
    },
    {
      "epoch": 1.64876119092234,
      "grad_norm": 0.4980350434780121,
      "learning_rate": 9.272052526405937e-05,
      "loss": 0.7292,
      "step": 1980
    },
    {
      "epoch": 1.6570893191755154,
      "grad_norm": 0.6231033205986023,
      "learning_rate": 9.214958606908365e-05,
      "loss": 0.7366,
      "step": 1990
    },
    {
      "epoch": 1.6654174474286902,
      "grad_norm": 0.5242041349411011,
      "learning_rate": 9.15786468741079e-05,
      "loss": 0.7715,
      "step": 2000
    },
    {
      "epoch": 1.6737455756818655,
      "grad_norm": 0.5677677989006042,
      "learning_rate": 9.100770767913218e-05,
      "loss": 0.7458,
      "step": 2010
    },
    {
      "epoch": 1.6820737039350406,
      "grad_norm": 0.4504517614841461,
      "learning_rate": 9.043676848415644e-05,
      "loss": 0.7414,
      "step": 2020
    },
    {
      "epoch": 1.6904018321882157,
      "grad_norm": 0.5147939920425415,
      "learning_rate": 8.986582928918071e-05,
      "loss": 0.7161,
      "step": 2030
    },
    {
      "epoch": 1.6987299604413908,
      "grad_norm": 0.5184817910194397,
      "learning_rate": 8.929489009420497e-05,
      "loss": 0.737,
      "step": 2040
    },
    {
      "epoch": 1.7070580886945659,
      "grad_norm": 0.511454701423645,
      "learning_rate": 8.872395089922924e-05,
      "loss": 0.7326,
      "step": 2050
    },
    {
      "epoch": 1.715386216947741,
      "grad_norm": 0.6407281160354614,
      "learning_rate": 8.81530117042535e-05,
      "loss": 0.757,
      "step": 2060
    },
    {
      "epoch": 1.723714345200916,
      "grad_norm": 0.46770188212394714,
      "learning_rate": 8.758207250927777e-05,
      "loss": 0.7246,
      "step": 2070
    },
    {
      "epoch": 1.7320424734540913,
      "grad_norm": 0.5417940616607666,
      "learning_rate": 8.701113331430203e-05,
      "loss": 0.73,
      "step": 2080
    },
    {
      "epoch": 1.7403706017072662,
      "grad_norm": 0.7172756791114807,
      "learning_rate": 8.64401941193263e-05,
      "loss": 0.7826,
      "step": 2090
    },
    {
      "epoch": 1.7486987299604415,
      "grad_norm": 0.49252116680145264,
      "learning_rate": 8.586925492435056e-05,
      "loss": 0.7414,
      "step": 2100
    },
    {
      "epoch": 1.7570268582136164,
      "grad_norm": 0.5305785536766052,
      "learning_rate": 8.529831572937482e-05,
      "loss": 0.7638,
      "step": 2110
    },
    {
      "epoch": 1.7653549864667917,
      "grad_norm": 0.5835579633712769,
      "learning_rate": 8.47273765343991e-05,
      "loss": 0.7505,
      "step": 2120
    },
    {
      "epoch": 1.7736831147199665,
      "grad_norm": 0.5282079577445984,
      "learning_rate": 8.415643733942336e-05,
      "loss": 0.7477,
      "step": 2130
    },
    {
      "epoch": 1.7820112429731418,
      "grad_norm": 0.5077541470527649,
      "learning_rate": 8.358549814444761e-05,
      "loss": 0.7145,
      "step": 2140
    },
    {
      "epoch": 1.790339371226317,
      "grad_norm": 0.5642469525337219,
      "learning_rate": 8.301455894947189e-05,
      "loss": 0.7467,
      "step": 2150
    },
    {
      "epoch": 1.798667499479492,
      "grad_norm": 0.4849185347557068,
      "learning_rate": 8.244361975449615e-05,
      "loss": 0.7587,
      "step": 2160
    },
    {
      "epoch": 1.806995627732667,
      "grad_norm": 0.502395749092102,
      "learning_rate": 8.187268055952042e-05,
      "loss": 0.7251,
      "step": 2170
    },
    {
      "epoch": 1.8153237559858422,
      "grad_norm": 0.508478045463562,
      "learning_rate": 8.130174136454468e-05,
      "loss": 0.7195,
      "step": 2180
    },
    {
      "epoch": 1.8236518842390173,
      "grad_norm": 0.5607203841209412,
      "learning_rate": 8.073080216956894e-05,
      "loss": 0.7317,
      "step": 2190
    },
    {
      "epoch": 1.8319800124921923,
      "grad_norm": 0.5707418918609619,
      "learning_rate": 8.015986297459321e-05,
      "loss": 0.7568,
      "step": 2200
    },
    {
      "epoch": 1.8403081407453676,
      "grad_norm": 0.6101348400115967,
      "learning_rate": 7.958892377961747e-05,
      "loss": 0.7191,
      "step": 2210
    },
    {
      "epoch": 1.8486362689985425,
      "grad_norm": 0.5471882224082947,
      "learning_rate": 7.901798458464174e-05,
      "loss": 0.7967,
      "step": 2220
    },
    {
      "epoch": 1.8569643972517178,
      "grad_norm": 0.4689045250415802,
      "learning_rate": 7.8447045389666e-05,
      "loss": 0.7265,
      "step": 2230
    },
    {
      "epoch": 1.8652925255048927,
      "grad_norm": 0.5977540016174316,
      "learning_rate": 7.787610619469027e-05,
      "loss": 0.7176,
      "step": 2240
    },
    {
      "epoch": 1.873620653758068,
      "grad_norm": 0.685124397277832,
      "learning_rate": 7.730516699971453e-05,
      "loss": 0.7213,
      "step": 2250
    },
    {
      "epoch": 1.8819487820112428,
      "grad_norm": 0.5801709890365601,
      "learning_rate": 7.67342278047388e-05,
      "loss": 0.7753,
      "step": 2260
    },
    {
      "epoch": 1.8902769102644181,
      "grad_norm": 0.4972231686115265,
      "learning_rate": 7.616328860976306e-05,
      "loss": 0.7393,
      "step": 2270
    },
    {
      "epoch": 1.8986050385175932,
      "grad_norm": 0.6082484722137451,
      "learning_rate": 7.559234941478734e-05,
      "loss": 0.7195,
      "step": 2280
    },
    {
      "epoch": 1.9069331667707683,
      "grad_norm": 0.5566319823265076,
      "learning_rate": 7.50214102198116e-05,
      "loss": 0.7535,
      "step": 2290
    },
    {
      "epoch": 1.9152612950239434,
      "grad_norm": 0.5072596073150635,
      "learning_rate": 7.445047102483586e-05,
      "loss": 0.7623,
      "step": 2300
    },
    {
      "epoch": 1.9235894232771185,
      "grad_norm": 0.48961398005485535,
      "learning_rate": 7.387953182986013e-05,
      "loss": 0.7257,
      "step": 2310
    },
    {
      "epoch": 1.9319175515302935,
      "grad_norm": 0.6331600546836853,
      "learning_rate": 7.330859263488439e-05,
      "loss": 0.7488,
      "step": 2320
    },
    {
      "epoch": 1.9402456797834686,
      "grad_norm": 0.5148442387580872,
      "learning_rate": 7.273765343990865e-05,
      "loss": 0.7323,
      "step": 2330
    },
    {
      "epoch": 1.948573808036644,
      "grad_norm": 0.5545548796653748,
      "learning_rate": 7.216671424493292e-05,
      "loss": 0.7533,
      "step": 2340
    },
    {
      "epoch": 1.9569019362898188,
      "grad_norm": 0.4720025658607483,
      "learning_rate": 7.159577504995718e-05,
      "loss": 0.7071,
      "step": 2350
    },
    {
      "epoch": 1.965230064542994,
      "grad_norm": 0.6042229533195496,
      "learning_rate": 7.102483585498145e-05,
      "loss": 0.7126,
      "step": 2360
    },
    {
      "epoch": 1.973558192796169,
      "grad_norm": 0.4973584711551666,
      "learning_rate": 7.045389666000571e-05,
      "loss": 0.7446,
      "step": 2370
    },
    {
      "epoch": 1.9818863210493443,
      "grad_norm": 0.47420960664749146,
      "learning_rate": 6.988295746502997e-05,
      "loss": 0.7282,
      "step": 2380
    },
    {
      "epoch": 1.9902144493025191,
      "grad_norm": 0.528920590877533,
      "learning_rate": 6.931201827005424e-05,
      "loss": 0.7172,
      "step": 2390
    },
    {
      "epoch": 1.9985425775556944,
      "grad_norm": 0.5135865807533264,
      "learning_rate": 6.87410790750785e-05,
      "loss": 0.774,
      "step": 2400
    },
    {
      "epoch": 2.00666250260254,
      "grad_norm": 0.6088364124298096,
      "learning_rate": 6.817013988010277e-05,
      "loss": 0.7577,
      "step": 2410
    },
    {
      "epoch": 2.0149906308557153,
      "grad_norm": 0.4967699944972992,
      "learning_rate": 6.759920068512705e-05,
      "loss": 0.732,
      "step": 2420
    },
    {
      "epoch": 2.02331875910889,
      "grad_norm": 0.5022122263908386,
      "learning_rate": 6.70282614901513e-05,
      "loss": 0.678,
      "step": 2430
    },
    {
      "epoch": 2.0316468873620654,
      "grad_norm": 0.5465041399002075,
      "learning_rate": 6.645732229517558e-05,
      "loss": 0.7343,
      "step": 2440
    },
    {
      "epoch": 2.0399750156152403,
      "grad_norm": 0.5363464951515198,
      "learning_rate": 6.588638310019984e-05,
      "loss": 0.6823,
      "step": 2450
    },
    {
      "epoch": 2.0483031438684156,
      "grad_norm": 0.64159095287323,
      "learning_rate": 6.53154439052241e-05,
      "loss": 0.7264,
      "step": 2460
    },
    {
      "epoch": 2.0566312721215905,
      "grad_norm": 0.5434449315071106,
      "learning_rate": 6.474450471024837e-05,
      "loss": 0.7099,
      "step": 2470
    },
    {
      "epoch": 2.0649594003747658,
      "grad_norm": 0.6437425017356873,
      "learning_rate": 6.417356551527263e-05,
      "loss": 0.6887,
      "step": 2480
    },
    {
      "epoch": 2.073287528627941,
      "grad_norm": 0.52934730052948,
      "learning_rate": 6.360262632029689e-05,
      "loss": 0.7083,
      "step": 2490
    },
    {
      "epoch": 2.081615656881116,
      "grad_norm": 0.6342054009437561,
      "learning_rate": 6.303168712532116e-05,
      "loss": 0.7251,
      "step": 2500
    },
    {
      "epoch": 2.0899437851342912,
      "grad_norm": 0.6315168738365173,
      "learning_rate": 6.246074793034542e-05,
      "loss": 0.7013,
      "step": 2510
    },
    {
      "epoch": 2.098271913387466,
      "grad_norm": 0.5208100080490112,
      "learning_rate": 6.188980873536968e-05,
      "loss": 0.7324,
      "step": 2520
    },
    {
      "epoch": 2.1066000416406414,
      "grad_norm": 0.4927663207054138,
      "learning_rate": 6.131886954039395e-05,
      "loss": 0.7251,
      "step": 2530
    },
    {
      "epoch": 2.1149281698938163,
      "grad_norm": 0.5925302505493164,
      "learning_rate": 6.074793034541822e-05,
      "loss": 0.7343,
      "step": 2540
    },
    {
      "epoch": 2.1232562981469916,
      "grad_norm": 0.6094543933868408,
      "learning_rate": 6.017699115044248e-05,
      "loss": 0.7549,
      "step": 2550
    },
    {
      "epoch": 2.1315844264001664,
      "grad_norm": 0.6076758503913879,
      "learning_rate": 5.960605195546675e-05,
      "loss": 0.7319,
      "step": 2560
    },
    {
      "epoch": 2.1399125546533417,
      "grad_norm": 0.5469070076942444,
      "learning_rate": 5.903511276049101e-05,
      "loss": 0.7096,
      "step": 2570
    },
    {
      "epoch": 2.1482406829065166,
      "grad_norm": 0.579449474811554,
      "learning_rate": 5.846417356551528e-05,
      "loss": 0.7053,
      "step": 2580
    },
    {
      "epoch": 2.156568811159692,
      "grad_norm": 0.6032702326774597,
      "learning_rate": 5.789323437053954e-05,
      "loss": 0.7217,
      "step": 2590
    },
    {
      "epoch": 2.1648969394128668,
      "grad_norm": 0.5625011324882507,
      "learning_rate": 5.73222951755638e-05,
      "loss": 0.7313,
      "step": 2600
    },
    {
      "epoch": 2.173225067666042,
      "grad_norm": 0.5538488030433655,
      "learning_rate": 5.675135598058807e-05,
      "loss": 0.7495,
      "step": 2610
    },
    {
      "epoch": 2.181553195919217,
      "grad_norm": 0.5596705079078674,
      "learning_rate": 5.618041678561233e-05,
      "loss": 0.7184,
      "step": 2620
    },
    {
      "epoch": 2.1898813241723922,
      "grad_norm": 0.572691798210144,
      "learning_rate": 5.56094775906366e-05,
      "loss": 0.7235,
      "step": 2630
    },
    {
      "epoch": 2.1982094524255675,
      "grad_norm": 0.5423110127449036,
      "learning_rate": 5.503853839566087e-05,
      "loss": 0.7031,
      "step": 2640
    },
    {
      "epoch": 2.2065375806787424,
      "grad_norm": 0.6671520471572876,
      "learning_rate": 5.446759920068513e-05,
      "loss": 0.7041,
      "step": 2650
    },
    {
      "epoch": 2.2148657089319177,
      "grad_norm": 0.629589855670929,
      "learning_rate": 5.38966600057094e-05,
      "loss": 0.706,
      "step": 2660
    },
    {
      "epoch": 2.2231938371850926,
      "grad_norm": 0.5419472455978394,
      "learning_rate": 5.332572081073366e-05,
      "loss": 0.7113,
      "step": 2670
    },
    {
      "epoch": 2.231521965438268,
      "grad_norm": 0.5866771936416626,
      "learning_rate": 5.275478161575792e-05,
      "loss": 0.7348,
      "step": 2680
    },
    {
      "epoch": 2.2398500936914427,
      "grad_norm": 0.6944882869720459,
      "learning_rate": 5.218384242078219e-05,
      "loss": 0.7301,
      "step": 2690
    },
    {
      "epoch": 2.248178221944618,
      "grad_norm": 0.6462831497192383,
      "learning_rate": 5.161290322580645e-05,
      "loss": 0.7164,
      "step": 2700
    },
    {
      "epoch": 2.256506350197793,
      "grad_norm": 0.6132184267044067,
      "learning_rate": 5.104196403083071e-05,
      "loss": 0.7476,
      "step": 2710
    },
    {
      "epoch": 2.264834478450968,
      "grad_norm": 0.5689401030540466,
      "learning_rate": 5.047102483585498e-05,
      "loss": 0.744,
      "step": 2720
    },
    {
      "epoch": 2.273162606704143,
      "grad_norm": 0.5365736484527588,
      "learning_rate": 4.990008564087925e-05,
      "loss": 0.7161,
      "step": 2730
    },
    {
      "epoch": 2.2814907349573184,
      "grad_norm": 0.558684229850769,
      "learning_rate": 4.9329146445903515e-05,
      "loss": 0.7209,
      "step": 2740
    },
    {
      "epoch": 2.2898188632104937,
      "grad_norm": 0.6040905117988586,
      "learning_rate": 4.875820725092778e-05,
      "loss": 0.7122,
      "step": 2750
    },
    {
      "epoch": 2.2981469914636685,
      "grad_norm": 0.5483527779579163,
      "learning_rate": 4.8187268055952046e-05,
      "loss": 0.7076,
      "step": 2760
    },
    {
      "epoch": 2.306475119716844,
      "grad_norm": 0.5816839337348938,
      "learning_rate": 4.7616328860976306e-05,
      "loss": 0.6859,
      "step": 2770
    },
    {
      "epoch": 2.3148032479700187,
      "grad_norm": 0.700238049030304,
      "learning_rate": 4.704538966600057e-05,
      "loss": 0.7346,
      "step": 2780
    },
    {
      "epoch": 2.323131376223194,
      "grad_norm": 0.5008053779602051,
      "learning_rate": 4.647445047102484e-05,
      "loss": 0.7315,
      "step": 2790
    },
    {
      "epoch": 2.331459504476369,
      "grad_norm": 0.6523973345756531,
      "learning_rate": 4.59035112760491e-05,
      "loss": 0.7101,
      "step": 2800
    },
    {
      "epoch": 2.339787632729544,
      "grad_norm": 0.5900940299034119,
      "learning_rate": 4.533257208107336e-05,
      "loss": 0.6953,
      "step": 2810
    },
    {
      "epoch": 2.348115760982719,
      "grad_norm": 0.6449031233787537,
      "learning_rate": 4.4761632886097635e-05,
      "loss": 0.7535,
      "step": 2820
    },
    {
      "epoch": 2.3564438892358943,
      "grad_norm": 0.6350171566009521,
      "learning_rate": 4.41906936911219e-05,
      "loss": 0.695,
      "step": 2830
    },
    {
      "epoch": 2.364772017489069,
      "grad_norm": 0.5762678384780884,
      "learning_rate": 4.361975449614617e-05,
      "loss": 0.7643,
      "step": 2840
    },
    {
      "epoch": 2.3731001457422445,
      "grad_norm": 0.5630966424942017,
      "learning_rate": 4.3048815301170426e-05,
      "loss": 0.7543,
      "step": 2850
    },
    {
      "epoch": 2.3814282739954193,
      "grad_norm": 0.6208187937736511,
      "learning_rate": 4.247787610619469e-05,
      "loss": 0.7243,
      "step": 2860
    },
    {
      "epoch": 2.3897564022485946,
      "grad_norm": 0.5450327396392822,
      "learning_rate": 4.190693691121896e-05,
      "loss": 0.7416,
      "step": 2870
    },
    {
      "epoch": 2.3980845305017695,
      "grad_norm": 0.6244034171104431,
      "learning_rate": 4.1335997716243224e-05,
      "loss": 0.7045,
      "step": 2880
    },
    {
      "epoch": 2.406412658754945,
      "grad_norm": 0.5462100505828857,
      "learning_rate": 4.076505852126748e-05,
      "loss": 0.727,
      "step": 2890
    },
    {
      "epoch": 2.41474078700812,
      "grad_norm": 0.6393170356750488,
      "learning_rate": 4.019411932629175e-05,
      "loss": 0.7286,
      "step": 2900
    },
    {
      "epoch": 2.423068915261295,
      "grad_norm": 0.6815822124481201,
      "learning_rate": 3.9623180131316015e-05,
      "loss": 0.7237,
      "step": 2910
    },
    {
      "epoch": 2.4313970435144703,
      "grad_norm": 0.6020609736442566,
      "learning_rate": 3.905224093634028e-05,
      "loss": 0.7349,
      "step": 2920
    },
    {
      "epoch": 2.439725171767645,
      "grad_norm": 0.5906940698623657,
      "learning_rate": 3.848130174136455e-05,
      "loss": 0.7088,
      "step": 2930
    },
    {
      "epoch": 2.4480533000208204,
      "grad_norm": 0.571682870388031,
      "learning_rate": 3.791036254638881e-05,
      "loss": 0.7096,
      "step": 2940
    },
    {
      "epoch": 2.4563814282739953,
      "grad_norm": 0.6196916699409485,
      "learning_rate": 3.733942335141308e-05,
      "loss": 0.7207,
      "step": 2950
    },
    {
      "epoch": 2.4647095565271706,
      "grad_norm": 0.6043674349784851,
      "learning_rate": 3.6768484156437344e-05,
      "loss": 0.7573,
      "step": 2960
    },
    {
      "epoch": 2.4730376847803455,
      "grad_norm": 0.5997941493988037,
      "learning_rate": 3.6197544961461604e-05,
      "loss": 0.7344,
      "step": 2970
    },
    {
      "epoch": 2.4813658130335208,
      "grad_norm": 0.5057125687599182,
      "learning_rate": 3.562660576648587e-05,
      "loss": 0.7028,
      "step": 2980
    },
    {
      "epoch": 2.4896939412866956,
      "grad_norm": 0.5430371761322021,
      "learning_rate": 3.5055666571510135e-05,
      "loss": 0.7251,
      "step": 2990
    },
    {
      "epoch": 2.498022069539871,
      "grad_norm": 0.6099908351898193,
      "learning_rate": 3.44847273765344e-05,
      "loss": 0.7233,
      "step": 3000
    },
    {
      "epoch": 2.5063501977930462,
      "grad_norm": 0.7470085024833679,
      "learning_rate": 3.391378818155867e-05,
      "loss": 0.7191,
      "step": 3010
    },
    {
      "epoch": 2.514678326046221,
      "grad_norm": 0.609671413898468,
      "learning_rate": 3.334284898658293e-05,
      "loss": 0.6953,
      "step": 3020
    },
    {
      "epoch": 2.523006454299396,
      "grad_norm": 0.546688437461853,
      "learning_rate": 3.27719097916072e-05,
      "loss": 0.6822,
      "step": 3030
    },
    {
      "epoch": 2.5313345825525713,
      "grad_norm": 0.6172854900360107,
      "learning_rate": 3.220097059663146e-05,
      "loss": 0.7165,
      "step": 3040
    },
    {
      "epoch": 2.5396627108057466,
      "grad_norm": 0.6603505611419678,
      "learning_rate": 3.1630031401655724e-05,
      "loss": 0.7489,
      "step": 3050
    },
    {
      "epoch": 2.5479908390589214,
      "grad_norm": 0.549051821231842,
      "learning_rate": 3.105909220667999e-05,
      "loss": 0.738,
      "step": 3060
    },
    {
      "epoch": 2.5563189673120967,
      "grad_norm": 0.5787954926490784,
      "learning_rate": 3.0488153011704256e-05,
      "loss": 0.7363,
      "step": 3070
    },
    {
      "epoch": 2.5646470955652716,
      "grad_norm": 0.592290461063385,
      "learning_rate": 2.991721381672852e-05,
      "loss": 0.7117,
      "step": 3080
    },
    {
      "epoch": 2.572975223818447,
      "grad_norm": 0.6130115389823914,
      "learning_rate": 2.9346274621752784e-05,
      "loss": 0.7572,
      "step": 3090
    },
    {
      "epoch": 2.5813033520716218,
      "grad_norm": 0.7555122971534729,
      "learning_rate": 2.877533542677705e-05,
      "loss": 0.7155,
      "step": 3100
    },
    {
      "epoch": 2.589631480324797,
      "grad_norm": 0.7690214514732361,
      "learning_rate": 2.8204396231801316e-05,
      "loss": 0.7365,
      "step": 3110
    },
    {
      "epoch": 2.5979596085779724,
      "grad_norm": 0.6256539225578308,
      "learning_rate": 2.763345703682558e-05,
      "loss": 0.7113,
      "step": 3120
    },
    {
      "epoch": 2.6062877368311472,
      "grad_norm": 0.5478750467300415,
      "learning_rate": 2.7062517841849845e-05,
      "loss": 0.6952,
      "step": 3130
    },
    {
      "epoch": 2.614615865084322,
      "grad_norm": 0.6098704934120178,
      "learning_rate": 2.649157864687411e-05,
      "loss": 0.7338,
      "step": 3140
    },
    {
      "epoch": 2.6229439933374974,
      "grad_norm": 0.5984623432159424,
      "learning_rate": 2.5920639451898376e-05,
      "loss": 0.7233,
      "step": 3150
    },
    {
      "epoch": 2.6312721215906727,
      "grad_norm": 0.5710356831550598,
      "learning_rate": 2.5349700256922636e-05,
      "loss": 0.7253,
      "step": 3160
    },
    {
      "epoch": 2.6396002498438476,
      "grad_norm": 0.5916727781295776,
      "learning_rate": 2.4778761061946905e-05,
      "loss": 0.7206,
      "step": 3170
    },
    {
      "epoch": 2.6479283780970224,
      "grad_norm": 0.545582115650177,
      "learning_rate": 2.420782186697117e-05,
      "loss": 0.7507,
      "step": 3180
    },
    {
      "epoch": 2.6562565063501977,
      "grad_norm": 0.5639556646347046,
      "learning_rate": 2.3636882671995433e-05,
      "loss": 0.7335,
      "step": 3190
    },
    {
      "epoch": 2.664584634603373,
      "grad_norm": 0.5350824594497681,
      "learning_rate": 2.30659434770197e-05,
      "loss": 0.7229,
      "step": 3200
    },
    {
      "epoch": 2.672912762856548,
      "grad_norm": 0.6157368421554565,
      "learning_rate": 2.2495004282043962e-05,
      "loss": 0.6898,
      "step": 3210
    },
    {
      "epoch": 2.681240891109723,
      "grad_norm": 0.6029368042945862,
      "learning_rate": 2.192406508706823e-05,
      "loss": 0.7303,
      "step": 3220
    },
    {
      "epoch": 2.689569019362898,
      "grad_norm": 0.5505029559135437,
      "learning_rate": 2.1353125892092494e-05,
      "loss": 0.7424,
      "step": 3230
    },
    {
      "epoch": 2.6978971476160734,
      "grad_norm": 0.6065691113471985,
      "learning_rate": 2.0782186697116756e-05,
      "loss": 0.7026,
      "step": 3240
    },
    {
      "epoch": 2.706225275869248,
      "grad_norm": 0.5178166031837463,
      "learning_rate": 2.0211247502141022e-05,
      "loss": 0.7063,
      "step": 3250
    },
    {
      "epoch": 2.7145534041224235,
      "grad_norm": 0.5506499409675598,
      "learning_rate": 1.9640308307165288e-05,
      "loss": 0.7516,
      "step": 3260
    },
    {
      "epoch": 2.722881532375599,
      "grad_norm": 0.603176474571228,
      "learning_rate": 1.9069369112189554e-05,
      "loss": 0.6889,
      "step": 3270
    },
    {
      "epoch": 2.7312096606287737,
      "grad_norm": 0.5664488077163696,
      "learning_rate": 1.8498429917213816e-05,
      "loss": 0.7118,
      "step": 3280
    },
    {
      "epoch": 2.7395377888819485,
      "grad_norm": 0.6981827020645142,
      "learning_rate": 1.7927490722238082e-05,
      "loss": 0.7271,
      "step": 3290
    },
    {
      "epoch": 2.747865917135124,
      "grad_norm": 0.5283637642860413,
      "learning_rate": 1.7356551527262345e-05,
      "loss": 0.7103,
      "step": 3300
    },
    {
      "epoch": 2.756194045388299,
      "grad_norm": 0.6051203608512878,
      "learning_rate": 1.6785612332286614e-05,
      "loss": 0.7512,
      "step": 3310
    },
    {
      "epoch": 2.764522173641474,
      "grad_norm": 0.6428741812705994,
      "learning_rate": 1.6214673137310877e-05,
      "loss": 0.7319,
      "step": 3320
    },
    {
      "epoch": 2.7728503018946493,
      "grad_norm": 0.5652990937232971,
      "learning_rate": 1.5643733942335142e-05,
      "loss": 0.746,
      "step": 3330
    },
    {
      "epoch": 2.781178430147824,
      "grad_norm": 0.5264344811439514,
      "learning_rate": 1.5072794747359407e-05,
      "loss": 0.7083,
      "step": 3340
    },
    {
      "epoch": 2.7895065584009995,
      "grad_norm": 0.58184814453125,
      "learning_rate": 1.4501855552383673e-05,
      "loss": 0.7277,
      "step": 3350
    },
    {
      "epoch": 2.7978346866541743,
      "grad_norm": 0.5882187485694885,
      "learning_rate": 1.3930916357407937e-05,
      "loss": 0.7448,
      "step": 3360
    },
    {
      "epoch": 2.8061628149073496,
      "grad_norm": 0.5568033456802368,
      "learning_rate": 1.3359977162432203e-05,
      "loss": 0.6837,
      "step": 3370
    },
    {
      "epoch": 2.814490943160525,
      "grad_norm": 0.6143646836280823,
      "learning_rate": 1.2789037967456465e-05,
      "loss": 0.7077,
      "step": 3380
    },
    {
      "epoch": 2.8228190714137,
      "grad_norm": 0.6539711952209473,
      "learning_rate": 1.2218098772480731e-05,
      "loss": 0.7198,
      "step": 3390
    },
    {
      "epoch": 2.8311471996668747,
      "grad_norm": 0.5738390684127808,
      "learning_rate": 1.1647159577504995e-05,
      "loss": 0.7049,
      "step": 3400
    },
    {
      "epoch": 2.83947532792005,
      "grad_norm": 0.569998562335968,
      "learning_rate": 1.1076220382529261e-05,
      "loss": 0.7315,
      "step": 3410
    },
    {
      "epoch": 2.8478034561732253,
      "grad_norm": 0.568832516670227,
      "learning_rate": 1.0505281187553526e-05,
      "loss": 0.71,
      "step": 3420
    },
    {
      "epoch": 2.8561315844264,
      "grad_norm": 0.5950223803520203,
      "learning_rate": 9.93434199257779e-06,
      "loss": 0.7437,
      "step": 3430
    },
    {
      "epoch": 2.864459712679575,
      "grad_norm": 0.5446391105651855,
      "learning_rate": 9.363402797602056e-06,
      "loss": 0.6859,
      "step": 3440
    },
    {
      "epoch": 2.8727878409327503,
      "grad_norm": 0.5308313369750977,
      "learning_rate": 8.79246360262632e-06,
      "loss": 0.7036,
      "step": 3450
    },
    {
      "epoch": 2.8811159691859256,
      "grad_norm": 0.5646246075630188,
      "learning_rate": 8.221524407650586e-06,
      "loss": 0.693,
      "step": 3460
    },
    {
      "epoch": 2.8894440974391005,
      "grad_norm": 0.5866724848747253,
      "learning_rate": 7.65058521267485e-06,
      "loss": 0.7242,
      "step": 3470
    },
    {
      "epoch": 2.8977722256922758,
      "grad_norm": 0.5774953365325928,
      "learning_rate": 7.079646017699115e-06,
      "loss": 0.7221,
      "step": 3480
    },
    {
      "epoch": 2.9061003539454506,
      "grad_norm": 0.5799909234046936,
      "learning_rate": 6.50870682272338e-06,
      "loss": 0.7111,
      "step": 3490
    },
    {
      "epoch": 2.914428482198626,
      "grad_norm": 0.5817180275917053,
      "learning_rate": 5.937767627747645e-06,
      "loss": 0.732,
      "step": 3500
    },
    {
      "epoch": 2.922756610451801,
      "grad_norm": 0.5753963589668274,
      "learning_rate": 5.36682843277191e-06,
      "loss": 0.7227,
      "step": 3510
    },
    {
      "epoch": 2.931084738704976,
      "grad_norm": 0.7013340592384338,
      "learning_rate": 4.795889237796175e-06,
      "loss": 0.7107,
      "step": 3520
    },
    {
      "epoch": 2.9394128669581514,
      "grad_norm": 0.6763131022453308,
      "learning_rate": 4.22495004282044e-06,
      "loss": 0.7079,
      "step": 3530
    },
    {
      "epoch": 2.9477409952113263,
      "grad_norm": 0.6113001108169556,
      "learning_rate": 3.654010847844705e-06,
      "loss": 0.7307,
      "step": 3540
    },
    {
      "epoch": 2.956069123464501,
      "grad_norm": 0.6017876267433167,
      "learning_rate": 3.0830716528689697e-06,
      "loss": 0.7222,
      "step": 3550
    },
    {
      "epoch": 2.9643972517176764,
      "grad_norm": 0.5557355284690857,
      "learning_rate": 2.5121324578932343e-06,
      "loss": 0.7288,
      "step": 3560
    },
    {
      "epoch": 2.9727253799708517,
      "grad_norm": 0.6558921337127686,
      "learning_rate": 1.9411932629174994e-06,
      "loss": 0.7607,
      "step": 3570
    },
    {
      "epoch": 2.9810535082240266,
      "grad_norm": 0.5581225156784058,
      "learning_rate": 1.3702540679417644e-06,
      "loss": 0.7367,
      "step": 3580
    },
    {
      "epoch": 2.989381636477202,
      "grad_norm": 0.6002179384231567,
      "learning_rate": 7.993148729660291e-07,
      "loss": 0.7646,
      "step": 3590
    },
    {
      "epoch": 2.9977097647303768,
      "grad_norm": 0.5548163652420044,
      "learning_rate": 2.2837567799029406e-07,
      "loss": 0.7213,
      "step": 3600
    }
  ],
  "logging_steps": 10,
  "max_steps": 3603,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2945919287689216e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
