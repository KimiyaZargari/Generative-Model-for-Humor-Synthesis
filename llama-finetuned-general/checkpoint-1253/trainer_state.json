{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1253,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00798482882523206,
      "grad_norm": 14.175909042358398,
      "learning_rate": 1.8e-05,
      "loss": 6.0183,
      "step": 10
    },
    {
      "epoch": 0.01596965765046412,
      "grad_norm": 34.798118591308594,
      "learning_rate": 3.8e-05,
      "loss": 6.8866,
      "step": 20
    },
    {
      "epoch": 0.023954486475696177,
      "grad_norm": 15.539424896240234,
      "learning_rate": 5.8e-05,
      "loss": 5.0388,
      "step": 30
    },
    {
      "epoch": 0.03193931530092824,
      "grad_norm": 14.114166259765625,
      "learning_rate": 7.800000000000001e-05,
      "loss": 3.1308,
      "step": 40
    },
    {
      "epoch": 0.03992414412616029,
      "grad_norm": 1.6417806148529053,
      "learning_rate": 9.8e-05,
      "loss": 2.1812,
      "step": 50
    },
    {
      "epoch": 0.047908972951392355,
      "grad_norm": 1.1767231225967407,
      "learning_rate": 0.000118,
      "loss": 1.6158,
      "step": 60
    },
    {
      "epoch": 0.05589380177662441,
      "grad_norm": 1.109983205795288,
      "learning_rate": 0.000138,
      "loss": 1.4594,
      "step": 70
    },
    {
      "epoch": 0.06387863060185647,
      "grad_norm": 0.9596033096313477,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.5152,
      "step": 80
    },
    {
      "epoch": 0.07186345942708854,
      "grad_norm": 1.0967129468917847,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.391,
      "step": 90
    },
    {
      "epoch": 0.07984828825232058,
      "grad_norm": 0.8139938116073608,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.466,
      "step": 100
    },
    {
      "epoch": 0.08783311707755265,
      "grad_norm": 0.7623734474182129,
      "learning_rate": 0.00019950806231210715,
      "loss": 1.4388,
      "step": 110
    },
    {
      "epoch": 0.09581794590278471,
      "grad_norm": 0.7544693350791931,
      "learning_rate": 0.00019896146488111507,
      "loss": 1.5322,
      "step": 120
    },
    {
      "epoch": 0.10380277472801677,
      "grad_norm": 0.788325846195221,
      "learning_rate": 0.000198414867450123,
      "loss": 1.4618,
      "step": 130
    },
    {
      "epoch": 0.11178760355324882,
      "grad_norm": 0.664431095123291,
      "learning_rate": 0.00019786827001913094,
      "loss": 1.3273,
      "step": 140
    },
    {
      "epoch": 0.11977243237848088,
      "grad_norm": 0.6197460293769836,
      "learning_rate": 0.00019732167258813886,
      "loss": 1.415,
      "step": 150
    },
    {
      "epoch": 0.12775726120371295,
      "grad_norm": 0.6923646330833435,
      "learning_rate": 0.00019677507515714678,
      "loss": 1.5002,
      "step": 160
    },
    {
      "epoch": 0.135742090028945,
      "grad_norm": 0.7221943140029907,
      "learning_rate": 0.0001962284777261547,
      "loss": 1.3866,
      "step": 170
    },
    {
      "epoch": 0.14372691885417707,
      "grad_norm": 0.6633984446525574,
      "learning_rate": 0.00019568188029516262,
      "loss": 1.3886,
      "step": 180
    },
    {
      "epoch": 0.15171174767940912,
      "grad_norm": 0.6840412616729736,
      "learning_rate": 0.00019513528286417054,
      "loss": 1.3489,
      "step": 190
    },
    {
      "epoch": 0.15969657650464117,
      "grad_norm": 0.6476105451583862,
      "learning_rate": 0.00019458868543317846,
      "loss": 1.4538,
      "step": 200
    },
    {
      "epoch": 0.16768140532987325,
      "grad_norm": 0.5629487037658691,
      "learning_rate": 0.00019404208800218639,
      "loss": 1.2909,
      "step": 210
    },
    {
      "epoch": 0.1756662341551053,
      "grad_norm": 0.6811816692352295,
      "learning_rate": 0.0001934954905711943,
      "loss": 1.4265,
      "step": 220
    },
    {
      "epoch": 0.18365106298033737,
      "grad_norm": 0.6009359955787659,
      "learning_rate": 0.00019294889314020225,
      "loss": 1.5452,
      "step": 230
    },
    {
      "epoch": 0.19163589180556942,
      "grad_norm": 0.6899468302726746,
      "learning_rate": 0.00019240229570921018,
      "loss": 1.4706,
      "step": 240
    },
    {
      "epoch": 0.19962072063080147,
      "grad_norm": 0.5981785655021667,
      "learning_rate": 0.0001918556982782181,
      "loss": 1.4586,
      "step": 250
    },
    {
      "epoch": 0.20760554945603354,
      "grad_norm": 0.5945307016372681,
      "learning_rate": 0.00019130910084722602,
      "loss": 1.3041,
      "step": 260
    },
    {
      "epoch": 0.2155903782812656,
      "grad_norm": 0.6095382571220398,
      "learning_rate": 0.00019076250341623397,
      "loss": 1.2916,
      "step": 270
    },
    {
      "epoch": 0.22357520710649764,
      "grad_norm": 0.6631801128387451,
      "learning_rate": 0.0001902159059852419,
      "loss": 1.3042,
      "step": 280
    },
    {
      "epoch": 0.23156003593172972,
      "grad_norm": 0.717195451259613,
      "learning_rate": 0.0001896693085542498,
      "loss": 1.4866,
      "step": 290
    },
    {
      "epoch": 0.23954486475696177,
      "grad_norm": 0.6512163877487183,
      "learning_rate": 0.00018912271112325773,
      "loss": 1.3163,
      "step": 300
    },
    {
      "epoch": 0.24752969358219384,
      "grad_norm": 0.7407152652740479,
      "learning_rate": 0.00018857611369226568,
      "loss": 1.204,
      "step": 310
    },
    {
      "epoch": 0.2555145224074259,
      "grad_norm": 0.5953931212425232,
      "learning_rate": 0.0001880295162612736,
      "loss": 1.3192,
      "step": 320
    },
    {
      "epoch": 0.26349935123265794,
      "grad_norm": 0.679246723651886,
      "learning_rate": 0.00018748291883028152,
      "loss": 1.3519,
      "step": 330
    },
    {
      "epoch": 0.27148418005789,
      "grad_norm": 0.6404103636741638,
      "learning_rate": 0.00018693632139928944,
      "loss": 1.419,
      "step": 340
    },
    {
      "epoch": 0.2794690088831221,
      "grad_norm": 0.6714981198310852,
      "learning_rate": 0.00018638972396829736,
      "loss": 1.4719,
      "step": 350
    },
    {
      "epoch": 0.28745383770835414,
      "grad_norm": 0.661389172077179,
      "learning_rate": 0.00018584312653730528,
      "loss": 1.2997,
      "step": 360
    },
    {
      "epoch": 0.2954386665335862,
      "grad_norm": 0.6830627918243408,
      "learning_rate": 0.0001852965291063132,
      "loss": 1.4366,
      "step": 370
    },
    {
      "epoch": 0.30342349535881824,
      "grad_norm": 0.6025201678276062,
      "learning_rate": 0.00018474993167532112,
      "loss": 1.409,
      "step": 380
    },
    {
      "epoch": 0.3114083241840503,
      "grad_norm": 0.5970479846000671,
      "learning_rate": 0.00018420333424432904,
      "loss": 1.2349,
      "step": 390
    },
    {
      "epoch": 0.31939315300928234,
      "grad_norm": 0.6773830056190491,
      "learning_rate": 0.000183656736813337,
      "loss": 1.4345,
      "step": 400
    },
    {
      "epoch": 0.32737798183451444,
      "grad_norm": 0.7417182326316833,
      "learning_rate": 0.0001831101393823449,
      "loss": 1.3425,
      "step": 410
    },
    {
      "epoch": 0.3353628106597465,
      "grad_norm": 0.6531981825828552,
      "learning_rate": 0.00018256354195135283,
      "loss": 1.3514,
      "step": 420
    },
    {
      "epoch": 0.34334763948497854,
      "grad_norm": 0.6055872440338135,
      "learning_rate": 0.00018201694452036075,
      "loss": 1.2879,
      "step": 430
    },
    {
      "epoch": 0.3513324683102106,
      "grad_norm": 0.6917208433151245,
      "learning_rate": 0.0001814703470893687,
      "loss": 1.3446,
      "step": 440
    },
    {
      "epoch": 0.35931729713544264,
      "grad_norm": 0.48751044273376465,
      "learning_rate": 0.00018092374965837662,
      "loss": 1.2995,
      "step": 450
    },
    {
      "epoch": 0.36730212596067474,
      "grad_norm": 0.617014467716217,
      "learning_rate": 0.00018037715222738454,
      "loss": 1.3659,
      "step": 460
    },
    {
      "epoch": 0.3752869547859068,
      "grad_norm": 0.5512115955352783,
      "learning_rate": 0.00017983055479639247,
      "loss": 1.2838,
      "step": 470
    },
    {
      "epoch": 0.38327178361113884,
      "grad_norm": 0.4715064465999603,
      "learning_rate": 0.00017928395736540041,
      "loss": 1.26,
      "step": 480
    },
    {
      "epoch": 0.3912566124363709,
      "grad_norm": 4.052940845489502,
      "learning_rate": 0.00017873735993440833,
      "loss": 1.3174,
      "step": 490
    },
    {
      "epoch": 0.39924144126160294,
      "grad_norm": 0.5562351942062378,
      "learning_rate": 0.00017819076250341623,
      "loss": 1.3744,
      "step": 500
    },
    {
      "epoch": 0.40722627008683504,
      "grad_norm": 0.4865943491458893,
      "learning_rate": 0.00017764416507242415,
      "loss": 1.3203,
      "step": 510
    },
    {
      "epoch": 0.4152110989120671,
      "grad_norm": 0.6609598398208618,
      "learning_rate": 0.00017709756764143207,
      "loss": 1.3474,
      "step": 520
    },
    {
      "epoch": 0.42319592773729914,
      "grad_norm": 0.592033326625824,
      "learning_rate": 0.00017655097021044002,
      "loss": 1.2589,
      "step": 530
    },
    {
      "epoch": 0.4311807565625312,
      "grad_norm": 0.5409432649612427,
      "learning_rate": 0.00017600437277944794,
      "loss": 1.3612,
      "step": 540
    },
    {
      "epoch": 0.43916558538776324,
      "grad_norm": 0.7002755999565125,
      "learning_rate": 0.00017545777534845586,
      "loss": 1.4023,
      "step": 550
    },
    {
      "epoch": 0.4471504142129953,
      "grad_norm": 0.5982208251953125,
      "learning_rate": 0.00017491117791746378,
      "loss": 1.3963,
      "step": 560
    },
    {
      "epoch": 0.4551352430382274,
      "grad_norm": 0.5262832641601562,
      "learning_rate": 0.00017436458048647173,
      "loss": 1.3182,
      "step": 570
    },
    {
      "epoch": 0.46312007186345944,
      "grad_norm": 0.4948408305644989,
      "learning_rate": 0.00017381798305547965,
      "loss": 1.3415,
      "step": 580
    },
    {
      "epoch": 0.4711049006886915,
      "grad_norm": 0.5980569124221802,
      "learning_rate": 0.00017327138562448757,
      "loss": 1.4417,
      "step": 590
    },
    {
      "epoch": 0.47908972951392353,
      "grad_norm": 0.6246198415756226,
      "learning_rate": 0.0001727247881934955,
      "loss": 1.5452,
      "step": 600
    },
    {
      "epoch": 0.4870745583391556,
      "grad_norm": 0.5215001702308655,
      "learning_rate": 0.00017217819076250344,
      "loss": 1.3727,
      "step": 610
    },
    {
      "epoch": 0.4950593871643877,
      "grad_norm": 0.47151726484298706,
      "learning_rate": 0.00017163159333151136,
      "loss": 1.2165,
      "step": 620
    },
    {
      "epoch": 0.5030442159896197,
      "grad_norm": 0.5435952544212341,
      "learning_rate": 0.00017108499590051928,
      "loss": 1.371,
      "step": 630
    },
    {
      "epoch": 0.5110290448148518,
      "grad_norm": 0.5582594275474548,
      "learning_rate": 0.0001705383984695272,
      "loss": 1.2938,
      "step": 640
    },
    {
      "epoch": 0.5190138736400839,
      "grad_norm": 0.5746598243713379,
      "learning_rate": 0.00016999180103853512,
      "loss": 1.3964,
      "step": 650
    },
    {
      "epoch": 0.5269987024653159,
      "grad_norm": 0.5210142731666565,
      "learning_rate": 0.00016944520360754304,
      "loss": 1.3525,
      "step": 660
    },
    {
      "epoch": 0.534983531290548,
      "grad_norm": 0.5856946110725403,
      "learning_rate": 0.00016889860617655097,
      "loss": 1.23,
      "step": 670
    },
    {
      "epoch": 0.54296836011578,
      "grad_norm": 0.6032091379165649,
      "learning_rate": 0.0001683520087455589,
      "loss": 1.337,
      "step": 680
    },
    {
      "epoch": 0.5509531889410121,
      "grad_norm": 0.6480275392532349,
      "learning_rate": 0.00016780541131456683,
      "loss": 1.3397,
      "step": 690
    },
    {
      "epoch": 0.5589380177662442,
      "grad_norm": 0.6032138466835022,
      "learning_rate": 0.00016725881388357476,
      "loss": 1.4433,
      "step": 700
    },
    {
      "epoch": 0.5669228465914762,
      "grad_norm": 0.488605797290802,
      "learning_rate": 0.00016671221645258268,
      "loss": 1.2995,
      "step": 710
    },
    {
      "epoch": 0.5749076754167083,
      "grad_norm": 0.5105956196784973,
      "learning_rate": 0.0001661656190215906,
      "loss": 1.3581,
      "step": 720
    },
    {
      "epoch": 0.5828925042419403,
      "grad_norm": 0.5056358575820923,
      "learning_rate": 0.00016561902159059852,
      "loss": 1.2465,
      "step": 730
    },
    {
      "epoch": 0.5908773330671724,
      "grad_norm": 0.5561421513557434,
      "learning_rate": 0.00016507242415960647,
      "loss": 1.3517,
      "step": 740
    },
    {
      "epoch": 0.5988621618924044,
      "grad_norm": 0.5266373157501221,
      "learning_rate": 0.0001645258267286144,
      "loss": 1.2115,
      "step": 750
    },
    {
      "epoch": 0.6068469907176365,
      "grad_norm": 0.5700095295906067,
      "learning_rate": 0.0001639792292976223,
      "loss": 1.374,
      "step": 760
    },
    {
      "epoch": 0.6148318195428686,
      "grad_norm": 0.5363860726356506,
      "learning_rate": 0.00016343263186663023,
      "loss": 1.2547,
      "step": 770
    },
    {
      "epoch": 0.6228166483681006,
      "grad_norm": 0.6298756003379822,
      "learning_rate": 0.00016288603443563818,
      "loss": 1.443,
      "step": 780
    },
    {
      "epoch": 0.6308014771933327,
      "grad_norm": 0.5629196166992188,
      "learning_rate": 0.0001623394370046461,
      "loss": 1.4515,
      "step": 790
    },
    {
      "epoch": 0.6387863060185647,
      "grad_norm": 0.5617161989212036,
      "learning_rate": 0.00016179283957365402,
      "loss": 1.2843,
      "step": 800
    },
    {
      "epoch": 0.6467711348437968,
      "grad_norm": 0.6375137567520142,
      "learning_rate": 0.00016124624214266194,
      "loss": 1.3604,
      "step": 810
    },
    {
      "epoch": 0.6547559636690289,
      "grad_norm": 0.6448327302932739,
      "learning_rate": 0.00016069964471166986,
      "loss": 1.216,
      "step": 820
    },
    {
      "epoch": 0.6627407924942609,
      "grad_norm": 0.5904508233070374,
      "learning_rate": 0.00016015304728067778,
      "loss": 1.3139,
      "step": 830
    },
    {
      "epoch": 0.670725621319493,
      "grad_norm": 0.6290187239646912,
      "learning_rate": 0.0001596064498496857,
      "loss": 1.362,
      "step": 840
    },
    {
      "epoch": 0.678710450144725,
      "grad_norm": 0.5268416404724121,
      "learning_rate": 0.00015905985241869362,
      "loss": 1.4342,
      "step": 850
    },
    {
      "epoch": 0.6866952789699571,
      "grad_norm": 0.5327848792076111,
      "learning_rate": 0.00015851325498770157,
      "loss": 1.2493,
      "step": 860
    },
    {
      "epoch": 0.6946801077951892,
      "grad_norm": 0.48933684825897217,
      "learning_rate": 0.0001579666575567095,
      "loss": 1.2967,
      "step": 870
    },
    {
      "epoch": 0.7026649366204212,
      "grad_norm": 0.5929020047187805,
      "learning_rate": 0.00015742006012571741,
      "loss": 1.2417,
      "step": 880
    },
    {
      "epoch": 0.7106497654456533,
      "grad_norm": 0.5054856538772583,
      "learning_rate": 0.00015687346269472533,
      "loss": 1.3217,
      "step": 890
    },
    {
      "epoch": 0.7186345942708853,
      "grad_norm": 0.4775446355342865,
      "learning_rate": 0.00015632686526373326,
      "loss": 1.3728,
      "step": 900
    },
    {
      "epoch": 0.7266194230961174,
      "grad_norm": 0.5833633542060852,
      "learning_rate": 0.0001557802678327412,
      "loss": 1.4837,
      "step": 910
    },
    {
      "epoch": 0.7346042519213495,
      "grad_norm": 0.5743684768676758,
      "learning_rate": 0.00015523367040174912,
      "loss": 1.3989,
      "step": 920
    },
    {
      "epoch": 0.7425890807465815,
      "grad_norm": 0.581531822681427,
      "learning_rate": 0.00015468707297075705,
      "loss": 1.3288,
      "step": 930
    },
    {
      "epoch": 0.7505739095718136,
      "grad_norm": 0.51092928647995,
      "learning_rate": 0.00015414047553976497,
      "loss": 1.311,
      "step": 940
    },
    {
      "epoch": 0.7585587383970456,
      "grad_norm": 0.521176278591156,
      "learning_rate": 0.00015359387810877291,
      "loss": 1.4288,
      "step": 950
    },
    {
      "epoch": 0.7665435672222777,
      "grad_norm": 0.5330055356025696,
      "learning_rate": 0.00015304728067778084,
      "loss": 1.4166,
      "step": 960
    },
    {
      "epoch": 0.7745283960475098,
      "grad_norm": 0.6757665872573853,
      "learning_rate": 0.00015250068324678876,
      "loss": 1.406,
      "step": 970
    },
    {
      "epoch": 0.7825132248727418,
      "grad_norm": 0.6430591344833374,
      "learning_rate": 0.00015195408581579668,
      "loss": 1.3061,
      "step": 980
    },
    {
      "epoch": 0.7904980536979739,
      "grad_norm": 0.5074537396430969,
      "learning_rate": 0.0001514074883848046,
      "loss": 1.4954,
      "step": 990
    },
    {
      "epoch": 0.7984828825232059,
      "grad_norm": 0.5122517347335815,
      "learning_rate": 0.00015086089095381252,
      "loss": 1.263,
      "step": 1000
    },
    {
      "epoch": 0.806467711348438,
      "grad_norm": 0.559742271900177,
      "learning_rate": 0.00015031429352282044,
      "loss": 1.4698,
      "step": 1010
    },
    {
      "epoch": 0.8144525401736701,
      "grad_norm": 0.5885198712348938,
      "learning_rate": 0.00014976769609182836,
      "loss": 1.3371,
      "step": 1020
    },
    {
      "epoch": 0.8224373689989021,
      "grad_norm": 0.5291991233825684,
      "learning_rate": 0.0001492210986608363,
      "loss": 1.4724,
      "step": 1030
    },
    {
      "epoch": 0.8304221978241342,
      "grad_norm": 0.5913505554199219,
      "learning_rate": 0.00014867450122984423,
      "loss": 1.3729,
      "step": 1040
    },
    {
      "epoch": 0.8384070266493662,
      "grad_norm": 0.5120719075202942,
      "learning_rate": 0.00014812790379885215,
      "loss": 1.3669,
      "step": 1050
    },
    {
      "epoch": 0.8463918554745983,
      "grad_norm": 0.5078720450401306,
      "learning_rate": 0.00014758130636786007,
      "loss": 1.384,
      "step": 1060
    },
    {
      "epoch": 0.8543766842998303,
      "grad_norm": 0.5168160200119019,
      "learning_rate": 0.000147034708936868,
      "loss": 1.27,
      "step": 1070
    },
    {
      "epoch": 0.8623615131250624,
      "grad_norm": 0.49535223841667175,
      "learning_rate": 0.00014648811150587594,
      "loss": 1.303,
      "step": 1080
    },
    {
      "epoch": 0.8703463419502945,
      "grad_norm": 0.5638179183006287,
      "learning_rate": 0.00014594151407488386,
      "loss": 1.3132,
      "step": 1090
    },
    {
      "epoch": 0.8783311707755265,
      "grad_norm": 0.6295135021209717,
      "learning_rate": 0.00014539491664389178,
      "loss": 1.3064,
      "step": 1100
    },
    {
      "epoch": 0.8863159996007586,
      "grad_norm": 0.650824248790741,
      "learning_rate": 0.0001448483192128997,
      "loss": 1.4721,
      "step": 1110
    },
    {
      "epoch": 0.8943008284259906,
      "grad_norm": 0.5576419830322266,
      "learning_rate": 0.00014430172178190765,
      "loss": 1.2953,
      "step": 1120
    },
    {
      "epoch": 0.9022856572512227,
      "grad_norm": 0.6166331768035889,
      "learning_rate": 0.00014375512435091557,
      "loss": 1.262,
      "step": 1130
    },
    {
      "epoch": 0.9102704860764548,
      "grad_norm": 0.4660778343677521,
      "learning_rate": 0.00014320852691992347,
      "loss": 1.2978,
      "step": 1140
    },
    {
      "epoch": 0.9182553149016868,
      "grad_norm": 0.5099622011184692,
      "learning_rate": 0.0001426619294889314,
      "loss": 1.3504,
      "step": 1150
    },
    {
      "epoch": 0.9262401437269189,
      "grad_norm": 0.5256348252296448,
      "learning_rate": 0.00014211533205793934,
      "loss": 1.3383,
      "step": 1160
    },
    {
      "epoch": 0.9342249725521509,
      "grad_norm": 0.5360493659973145,
      "learning_rate": 0.00014156873462694726,
      "loss": 1.2941,
      "step": 1170
    },
    {
      "epoch": 0.942209801377383,
      "grad_norm": 0.5467320680618286,
      "learning_rate": 0.00014102213719595518,
      "loss": 1.3538,
      "step": 1180
    },
    {
      "epoch": 0.9501946302026151,
      "grad_norm": 0.5714187026023865,
      "learning_rate": 0.0001404755397649631,
      "loss": 1.4855,
      "step": 1190
    },
    {
      "epoch": 0.9581794590278471,
      "grad_norm": 0.5414113402366638,
      "learning_rate": 0.00013992894233397105,
      "loss": 1.4414,
      "step": 1200
    },
    {
      "epoch": 0.9661642878530792,
      "grad_norm": 0.4803536832332611,
      "learning_rate": 0.00013938234490297897,
      "loss": 1.2233,
      "step": 1210
    },
    {
      "epoch": 0.9741491166783112,
      "grad_norm": 0.5846128463745117,
      "learning_rate": 0.0001388357474719869,
      "loss": 1.328,
      "step": 1220
    },
    {
      "epoch": 0.9821339455035433,
      "grad_norm": 0.5435988306999207,
      "learning_rate": 0.0001382891500409948,
      "loss": 1.2944,
      "step": 1230
    },
    {
      "epoch": 0.9901187743287754,
      "grad_norm": 0.4551745057106018,
      "learning_rate": 0.00013774255261000273,
      "loss": 1.3352,
      "step": 1240
    },
    {
      "epoch": 0.9981036031540074,
      "grad_norm": 0.5687288045883179,
      "learning_rate": 0.00013719595517901068,
      "loss": 1.3495,
      "step": 1250
    }
  ],
  "logging_steps": 10,
  "max_steps": 3759,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7977485090684928.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
