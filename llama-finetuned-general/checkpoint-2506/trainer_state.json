{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2506,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00798482882523206,
      "grad_norm": 14.175909042358398,
      "learning_rate": 1.8e-05,
      "loss": 6.0183,
      "step": 10
    },
    {
      "epoch": 0.01596965765046412,
      "grad_norm": 34.798118591308594,
      "learning_rate": 3.8e-05,
      "loss": 6.8866,
      "step": 20
    },
    {
      "epoch": 0.023954486475696177,
      "grad_norm": 15.539424896240234,
      "learning_rate": 5.8e-05,
      "loss": 5.0388,
      "step": 30
    },
    {
      "epoch": 0.03193931530092824,
      "grad_norm": 14.114166259765625,
      "learning_rate": 7.800000000000001e-05,
      "loss": 3.1308,
      "step": 40
    },
    {
      "epoch": 0.03992414412616029,
      "grad_norm": 1.6417806148529053,
      "learning_rate": 9.8e-05,
      "loss": 2.1812,
      "step": 50
    },
    {
      "epoch": 0.047908972951392355,
      "grad_norm": 1.1767231225967407,
      "learning_rate": 0.000118,
      "loss": 1.6158,
      "step": 60
    },
    {
      "epoch": 0.05589380177662441,
      "grad_norm": 1.109983205795288,
      "learning_rate": 0.000138,
      "loss": 1.4594,
      "step": 70
    },
    {
      "epoch": 0.06387863060185647,
      "grad_norm": 0.9596033096313477,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.5152,
      "step": 80
    },
    {
      "epoch": 0.07186345942708854,
      "grad_norm": 1.0967129468917847,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.391,
      "step": 90
    },
    {
      "epoch": 0.07984828825232058,
      "grad_norm": 0.8139938116073608,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.466,
      "step": 100
    },
    {
      "epoch": 0.08783311707755265,
      "grad_norm": 0.7623734474182129,
      "learning_rate": 0.00019950806231210715,
      "loss": 1.4388,
      "step": 110
    },
    {
      "epoch": 0.09581794590278471,
      "grad_norm": 0.7544693350791931,
      "learning_rate": 0.00019896146488111507,
      "loss": 1.5322,
      "step": 120
    },
    {
      "epoch": 0.10380277472801677,
      "grad_norm": 0.788325846195221,
      "learning_rate": 0.000198414867450123,
      "loss": 1.4618,
      "step": 130
    },
    {
      "epoch": 0.11178760355324882,
      "grad_norm": 0.664431095123291,
      "learning_rate": 0.00019786827001913094,
      "loss": 1.3273,
      "step": 140
    },
    {
      "epoch": 0.11977243237848088,
      "grad_norm": 0.6197460293769836,
      "learning_rate": 0.00019732167258813886,
      "loss": 1.415,
      "step": 150
    },
    {
      "epoch": 0.12775726120371295,
      "grad_norm": 0.6923646330833435,
      "learning_rate": 0.00019677507515714678,
      "loss": 1.5002,
      "step": 160
    },
    {
      "epoch": 0.135742090028945,
      "grad_norm": 0.7221943140029907,
      "learning_rate": 0.0001962284777261547,
      "loss": 1.3866,
      "step": 170
    },
    {
      "epoch": 0.14372691885417707,
      "grad_norm": 0.6633984446525574,
      "learning_rate": 0.00019568188029516262,
      "loss": 1.3886,
      "step": 180
    },
    {
      "epoch": 0.15171174767940912,
      "grad_norm": 0.6840412616729736,
      "learning_rate": 0.00019513528286417054,
      "loss": 1.3489,
      "step": 190
    },
    {
      "epoch": 0.15969657650464117,
      "grad_norm": 0.6476105451583862,
      "learning_rate": 0.00019458868543317846,
      "loss": 1.4538,
      "step": 200
    },
    {
      "epoch": 0.16768140532987325,
      "grad_norm": 0.5629487037658691,
      "learning_rate": 0.00019404208800218639,
      "loss": 1.2909,
      "step": 210
    },
    {
      "epoch": 0.1756662341551053,
      "grad_norm": 0.6811816692352295,
      "learning_rate": 0.0001934954905711943,
      "loss": 1.4265,
      "step": 220
    },
    {
      "epoch": 0.18365106298033737,
      "grad_norm": 0.6009359955787659,
      "learning_rate": 0.00019294889314020225,
      "loss": 1.5452,
      "step": 230
    },
    {
      "epoch": 0.19163589180556942,
      "grad_norm": 0.6899468302726746,
      "learning_rate": 0.00019240229570921018,
      "loss": 1.4706,
      "step": 240
    },
    {
      "epoch": 0.19962072063080147,
      "grad_norm": 0.5981785655021667,
      "learning_rate": 0.0001918556982782181,
      "loss": 1.4586,
      "step": 250
    },
    {
      "epoch": 0.20760554945603354,
      "grad_norm": 0.5945307016372681,
      "learning_rate": 0.00019130910084722602,
      "loss": 1.3041,
      "step": 260
    },
    {
      "epoch": 0.2155903782812656,
      "grad_norm": 0.6095382571220398,
      "learning_rate": 0.00019076250341623397,
      "loss": 1.2916,
      "step": 270
    },
    {
      "epoch": 0.22357520710649764,
      "grad_norm": 0.6631801128387451,
      "learning_rate": 0.0001902159059852419,
      "loss": 1.3042,
      "step": 280
    },
    {
      "epoch": 0.23156003593172972,
      "grad_norm": 0.717195451259613,
      "learning_rate": 0.0001896693085542498,
      "loss": 1.4866,
      "step": 290
    },
    {
      "epoch": 0.23954486475696177,
      "grad_norm": 0.6512163877487183,
      "learning_rate": 0.00018912271112325773,
      "loss": 1.3163,
      "step": 300
    },
    {
      "epoch": 0.24752969358219384,
      "grad_norm": 0.7407152652740479,
      "learning_rate": 0.00018857611369226568,
      "loss": 1.204,
      "step": 310
    },
    {
      "epoch": 0.2555145224074259,
      "grad_norm": 0.5953931212425232,
      "learning_rate": 0.0001880295162612736,
      "loss": 1.3192,
      "step": 320
    },
    {
      "epoch": 0.26349935123265794,
      "grad_norm": 0.679246723651886,
      "learning_rate": 0.00018748291883028152,
      "loss": 1.3519,
      "step": 330
    },
    {
      "epoch": 0.27148418005789,
      "grad_norm": 0.6404103636741638,
      "learning_rate": 0.00018693632139928944,
      "loss": 1.419,
      "step": 340
    },
    {
      "epoch": 0.2794690088831221,
      "grad_norm": 0.6714981198310852,
      "learning_rate": 0.00018638972396829736,
      "loss": 1.4719,
      "step": 350
    },
    {
      "epoch": 0.28745383770835414,
      "grad_norm": 0.661389172077179,
      "learning_rate": 0.00018584312653730528,
      "loss": 1.2997,
      "step": 360
    },
    {
      "epoch": 0.2954386665335862,
      "grad_norm": 0.6830627918243408,
      "learning_rate": 0.0001852965291063132,
      "loss": 1.4366,
      "step": 370
    },
    {
      "epoch": 0.30342349535881824,
      "grad_norm": 0.6025201678276062,
      "learning_rate": 0.00018474993167532112,
      "loss": 1.409,
      "step": 380
    },
    {
      "epoch": 0.3114083241840503,
      "grad_norm": 0.5970479846000671,
      "learning_rate": 0.00018420333424432904,
      "loss": 1.2349,
      "step": 390
    },
    {
      "epoch": 0.31939315300928234,
      "grad_norm": 0.6773830056190491,
      "learning_rate": 0.000183656736813337,
      "loss": 1.4345,
      "step": 400
    },
    {
      "epoch": 0.32737798183451444,
      "grad_norm": 0.7417182326316833,
      "learning_rate": 0.0001831101393823449,
      "loss": 1.3425,
      "step": 410
    },
    {
      "epoch": 0.3353628106597465,
      "grad_norm": 0.6531981825828552,
      "learning_rate": 0.00018256354195135283,
      "loss": 1.3514,
      "step": 420
    },
    {
      "epoch": 0.34334763948497854,
      "grad_norm": 0.6055872440338135,
      "learning_rate": 0.00018201694452036075,
      "loss": 1.2879,
      "step": 430
    },
    {
      "epoch": 0.3513324683102106,
      "grad_norm": 0.6917208433151245,
      "learning_rate": 0.0001814703470893687,
      "loss": 1.3446,
      "step": 440
    },
    {
      "epoch": 0.35931729713544264,
      "grad_norm": 0.48751044273376465,
      "learning_rate": 0.00018092374965837662,
      "loss": 1.2995,
      "step": 450
    },
    {
      "epoch": 0.36730212596067474,
      "grad_norm": 0.617014467716217,
      "learning_rate": 0.00018037715222738454,
      "loss": 1.3659,
      "step": 460
    },
    {
      "epoch": 0.3752869547859068,
      "grad_norm": 0.5512115955352783,
      "learning_rate": 0.00017983055479639247,
      "loss": 1.2838,
      "step": 470
    },
    {
      "epoch": 0.38327178361113884,
      "grad_norm": 0.4715064465999603,
      "learning_rate": 0.00017928395736540041,
      "loss": 1.26,
      "step": 480
    },
    {
      "epoch": 0.3912566124363709,
      "grad_norm": 4.052940845489502,
      "learning_rate": 0.00017873735993440833,
      "loss": 1.3174,
      "step": 490
    },
    {
      "epoch": 0.39924144126160294,
      "grad_norm": 0.5562351942062378,
      "learning_rate": 0.00017819076250341623,
      "loss": 1.3744,
      "step": 500
    },
    {
      "epoch": 0.40722627008683504,
      "grad_norm": 0.4865943491458893,
      "learning_rate": 0.00017764416507242415,
      "loss": 1.3203,
      "step": 510
    },
    {
      "epoch": 0.4152110989120671,
      "grad_norm": 0.6609598398208618,
      "learning_rate": 0.00017709756764143207,
      "loss": 1.3474,
      "step": 520
    },
    {
      "epoch": 0.42319592773729914,
      "grad_norm": 0.592033326625824,
      "learning_rate": 0.00017655097021044002,
      "loss": 1.2589,
      "step": 530
    },
    {
      "epoch": 0.4311807565625312,
      "grad_norm": 0.5409432649612427,
      "learning_rate": 0.00017600437277944794,
      "loss": 1.3612,
      "step": 540
    },
    {
      "epoch": 0.43916558538776324,
      "grad_norm": 0.7002755999565125,
      "learning_rate": 0.00017545777534845586,
      "loss": 1.4023,
      "step": 550
    },
    {
      "epoch": 0.4471504142129953,
      "grad_norm": 0.5982208251953125,
      "learning_rate": 0.00017491117791746378,
      "loss": 1.3963,
      "step": 560
    },
    {
      "epoch": 0.4551352430382274,
      "grad_norm": 0.5262832641601562,
      "learning_rate": 0.00017436458048647173,
      "loss": 1.3182,
      "step": 570
    },
    {
      "epoch": 0.46312007186345944,
      "grad_norm": 0.4948408305644989,
      "learning_rate": 0.00017381798305547965,
      "loss": 1.3415,
      "step": 580
    },
    {
      "epoch": 0.4711049006886915,
      "grad_norm": 0.5980569124221802,
      "learning_rate": 0.00017327138562448757,
      "loss": 1.4417,
      "step": 590
    },
    {
      "epoch": 0.47908972951392353,
      "grad_norm": 0.6246198415756226,
      "learning_rate": 0.0001727247881934955,
      "loss": 1.5452,
      "step": 600
    },
    {
      "epoch": 0.4870745583391556,
      "grad_norm": 0.5215001702308655,
      "learning_rate": 0.00017217819076250344,
      "loss": 1.3727,
      "step": 610
    },
    {
      "epoch": 0.4950593871643877,
      "grad_norm": 0.47151726484298706,
      "learning_rate": 0.00017163159333151136,
      "loss": 1.2165,
      "step": 620
    },
    {
      "epoch": 0.5030442159896197,
      "grad_norm": 0.5435952544212341,
      "learning_rate": 0.00017108499590051928,
      "loss": 1.371,
      "step": 630
    },
    {
      "epoch": 0.5110290448148518,
      "grad_norm": 0.5582594275474548,
      "learning_rate": 0.0001705383984695272,
      "loss": 1.2938,
      "step": 640
    },
    {
      "epoch": 0.5190138736400839,
      "grad_norm": 0.5746598243713379,
      "learning_rate": 0.00016999180103853512,
      "loss": 1.3964,
      "step": 650
    },
    {
      "epoch": 0.5269987024653159,
      "grad_norm": 0.5210142731666565,
      "learning_rate": 0.00016944520360754304,
      "loss": 1.3525,
      "step": 660
    },
    {
      "epoch": 0.534983531290548,
      "grad_norm": 0.5856946110725403,
      "learning_rate": 0.00016889860617655097,
      "loss": 1.23,
      "step": 670
    },
    {
      "epoch": 0.54296836011578,
      "grad_norm": 0.6032091379165649,
      "learning_rate": 0.0001683520087455589,
      "loss": 1.337,
      "step": 680
    },
    {
      "epoch": 0.5509531889410121,
      "grad_norm": 0.6480275392532349,
      "learning_rate": 0.00016780541131456683,
      "loss": 1.3397,
      "step": 690
    },
    {
      "epoch": 0.5589380177662442,
      "grad_norm": 0.6032138466835022,
      "learning_rate": 0.00016725881388357476,
      "loss": 1.4433,
      "step": 700
    },
    {
      "epoch": 0.5669228465914762,
      "grad_norm": 0.488605797290802,
      "learning_rate": 0.00016671221645258268,
      "loss": 1.2995,
      "step": 710
    },
    {
      "epoch": 0.5749076754167083,
      "grad_norm": 0.5105956196784973,
      "learning_rate": 0.0001661656190215906,
      "loss": 1.3581,
      "step": 720
    },
    {
      "epoch": 0.5828925042419403,
      "grad_norm": 0.5056358575820923,
      "learning_rate": 0.00016561902159059852,
      "loss": 1.2465,
      "step": 730
    },
    {
      "epoch": 0.5908773330671724,
      "grad_norm": 0.5561421513557434,
      "learning_rate": 0.00016507242415960647,
      "loss": 1.3517,
      "step": 740
    },
    {
      "epoch": 0.5988621618924044,
      "grad_norm": 0.5266373157501221,
      "learning_rate": 0.0001645258267286144,
      "loss": 1.2115,
      "step": 750
    },
    {
      "epoch": 0.6068469907176365,
      "grad_norm": 0.5700095295906067,
      "learning_rate": 0.0001639792292976223,
      "loss": 1.374,
      "step": 760
    },
    {
      "epoch": 0.6148318195428686,
      "grad_norm": 0.5363860726356506,
      "learning_rate": 0.00016343263186663023,
      "loss": 1.2547,
      "step": 770
    },
    {
      "epoch": 0.6228166483681006,
      "grad_norm": 0.6298756003379822,
      "learning_rate": 0.00016288603443563818,
      "loss": 1.443,
      "step": 780
    },
    {
      "epoch": 0.6308014771933327,
      "grad_norm": 0.5629196166992188,
      "learning_rate": 0.0001623394370046461,
      "loss": 1.4515,
      "step": 790
    },
    {
      "epoch": 0.6387863060185647,
      "grad_norm": 0.5617161989212036,
      "learning_rate": 0.00016179283957365402,
      "loss": 1.2843,
      "step": 800
    },
    {
      "epoch": 0.6467711348437968,
      "grad_norm": 0.6375137567520142,
      "learning_rate": 0.00016124624214266194,
      "loss": 1.3604,
      "step": 810
    },
    {
      "epoch": 0.6547559636690289,
      "grad_norm": 0.6448327302932739,
      "learning_rate": 0.00016069964471166986,
      "loss": 1.216,
      "step": 820
    },
    {
      "epoch": 0.6627407924942609,
      "grad_norm": 0.5904508233070374,
      "learning_rate": 0.00016015304728067778,
      "loss": 1.3139,
      "step": 830
    },
    {
      "epoch": 0.670725621319493,
      "grad_norm": 0.6290187239646912,
      "learning_rate": 0.0001596064498496857,
      "loss": 1.362,
      "step": 840
    },
    {
      "epoch": 0.678710450144725,
      "grad_norm": 0.5268416404724121,
      "learning_rate": 0.00015905985241869362,
      "loss": 1.4342,
      "step": 850
    },
    {
      "epoch": 0.6866952789699571,
      "grad_norm": 0.5327848792076111,
      "learning_rate": 0.00015851325498770157,
      "loss": 1.2493,
      "step": 860
    },
    {
      "epoch": 0.6946801077951892,
      "grad_norm": 0.48933684825897217,
      "learning_rate": 0.0001579666575567095,
      "loss": 1.2967,
      "step": 870
    },
    {
      "epoch": 0.7026649366204212,
      "grad_norm": 0.5929020047187805,
      "learning_rate": 0.00015742006012571741,
      "loss": 1.2417,
      "step": 880
    },
    {
      "epoch": 0.7106497654456533,
      "grad_norm": 0.5054856538772583,
      "learning_rate": 0.00015687346269472533,
      "loss": 1.3217,
      "step": 890
    },
    {
      "epoch": 0.7186345942708853,
      "grad_norm": 0.4775446355342865,
      "learning_rate": 0.00015632686526373326,
      "loss": 1.3728,
      "step": 900
    },
    {
      "epoch": 0.7266194230961174,
      "grad_norm": 0.5833633542060852,
      "learning_rate": 0.0001557802678327412,
      "loss": 1.4837,
      "step": 910
    },
    {
      "epoch": 0.7346042519213495,
      "grad_norm": 0.5743684768676758,
      "learning_rate": 0.00015523367040174912,
      "loss": 1.3989,
      "step": 920
    },
    {
      "epoch": 0.7425890807465815,
      "grad_norm": 0.581531822681427,
      "learning_rate": 0.00015468707297075705,
      "loss": 1.3288,
      "step": 930
    },
    {
      "epoch": 0.7505739095718136,
      "grad_norm": 0.51092928647995,
      "learning_rate": 0.00015414047553976497,
      "loss": 1.311,
      "step": 940
    },
    {
      "epoch": 0.7585587383970456,
      "grad_norm": 0.521176278591156,
      "learning_rate": 0.00015359387810877291,
      "loss": 1.4288,
      "step": 950
    },
    {
      "epoch": 0.7665435672222777,
      "grad_norm": 0.5330055356025696,
      "learning_rate": 0.00015304728067778084,
      "loss": 1.4166,
      "step": 960
    },
    {
      "epoch": 0.7745283960475098,
      "grad_norm": 0.6757665872573853,
      "learning_rate": 0.00015250068324678876,
      "loss": 1.406,
      "step": 970
    },
    {
      "epoch": 0.7825132248727418,
      "grad_norm": 0.6430591344833374,
      "learning_rate": 0.00015195408581579668,
      "loss": 1.3061,
      "step": 980
    },
    {
      "epoch": 0.7904980536979739,
      "grad_norm": 0.5074537396430969,
      "learning_rate": 0.0001514074883848046,
      "loss": 1.4954,
      "step": 990
    },
    {
      "epoch": 0.7984828825232059,
      "grad_norm": 0.5122517347335815,
      "learning_rate": 0.00015086089095381252,
      "loss": 1.263,
      "step": 1000
    },
    {
      "epoch": 0.806467711348438,
      "grad_norm": 0.559742271900177,
      "learning_rate": 0.00015031429352282044,
      "loss": 1.4698,
      "step": 1010
    },
    {
      "epoch": 0.8144525401736701,
      "grad_norm": 0.5885198712348938,
      "learning_rate": 0.00014976769609182836,
      "loss": 1.3371,
      "step": 1020
    },
    {
      "epoch": 0.8224373689989021,
      "grad_norm": 0.5291991233825684,
      "learning_rate": 0.0001492210986608363,
      "loss": 1.4724,
      "step": 1030
    },
    {
      "epoch": 0.8304221978241342,
      "grad_norm": 0.5913505554199219,
      "learning_rate": 0.00014867450122984423,
      "loss": 1.3729,
      "step": 1040
    },
    {
      "epoch": 0.8384070266493662,
      "grad_norm": 0.5120719075202942,
      "learning_rate": 0.00014812790379885215,
      "loss": 1.3669,
      "step": 1050
    },
    {
      "epoch": 0.8463918554745983,
      "grad_norm": 0.5078720450401306,
      "learning_rate": 0.00014758130636786007,
      "loss": 1.384,
      "step": 1060
    },
    {
      "epoch": 0.8543766842998303,
      "grad_norm": 0.5168160200119019,
      "learning_rate": 0.000147034708936868,
      "loss": 1.27,
      "step": 1070
    },
    {
      "epoch": 0.8623615131250624,
      "grad_norm": 0.49535223841667175,
      "learning_rate": 0.00014648811150587594,
      "loss": 1.303,
      "step": 1080
    },
    {
      "epoch": 0.8703463419502945,
      "grad_norm": 0.5638179183006287,
      "learning_rate": 0.00014594151407488386,
      "loss": 1.3132,
      "step": 1090
    },
    {
      "epoch": 0.8783311707755265,
      "grad_norm": 0.6295135021209717,
      "learning_rate": 0.00014539491664389178,
      "loss": 1.3064,
      "step": 1100
    },
    {
      "epoch": 0.8863159996007586,
      "grad_norm": 0.650824248790741,
      "learning_rate": 0.0001448483192128997,
      "loss": 1.4721,
      "step": 1110
    },
    {
      "epoch": 0.8943008284259906,
      "grad_norm": 0.5576419830322266,
      "learning_rate": 0.00014430172178190765,
      "loss": 1.2953,
      "step": 1120
    },
    {
      "epoch": 0.9022856572512227,
      "grad_norm": 0.6166331768035889,
      "learning_rate": 0.00014375512435091557,
      "loss": 1.262,
      "step": 1130
    },
    {
      "epoch": 0.9102704860764548,
      "grad_norm": 0.4660778343677521,
      "learning_rate": 0.00014320852691992347,
      "loss": 1.2978,
      "step": 1140
    },
    {
      "epoch": 0.9182553149016868,
      "grad_norm": 0.5099622011184692,
      "learning_rate": 0.0001426619294889314,
      "loss": 1.3504,
      "step": 1150
    },
    {
      "epoch": 0.9262401437269189,
      "grad_norm": 0.5256348252296448,
      "learning_rate": 0.00014211533205793934,
      "loss": 1.3383,
      "step": 1160
    },
    {
      "epoch": 0.9342249725521509,
      "grad_norm": 0.5360493659973145,
      "learning_rate": 0.00014156873462694726,
      "loss": 1.2941,
      "step": 1170
    },
    {
      "epoch": 0.942209801377383,
      "grad_norm": 0.5467320680618286,
      "learning_rate": 0.00014102213719595518,
      "loss": 1.3538,
      "step": 1180
    },
    {
      "epoch": 0.9501946302026151,
      "grad_norm": 0.5714187026023865,
      "learning_rate": 0.0001404755397649631,
      "loss": 1.4855,
      "step": 1190
    },
    {
      "epoch": 0.9581794590278471,
      "grad_norm": 0.5414113402366638,
      "learning_rate": 0.00013992894233397105,
      "loss": 1.4414,
      "step": 1200
    },
    {
      "epoch": 0.9661642878530792,
      "grad_norm": 0.4803536832332611,
      "learning_rate": 0.00013938234490297897,
      "loss": 1.2233,
      "step": 1210
    },
    {
      "epoch": 0.9741491166783112,
      "grad_norm": 0.5846128463745117,
      "learning_rate": 0.0001388357474719869,
      "loss": 1.328,
      "step": 1220
    },
    {
      "epoch": 0.9821339455035433,
      "grad_norm": 0.5435988306999207,
      "learning_rate": 0.0001382891500409948,
      "loss": 1.2944,
      "step": 1230
    },
    {
      "epoch": 0.9901187743287754,
      "grad_norm": 0.4551745057106018,
      "learning_rate": 0.00013774255261000273,
      "loss": 1.3352,
      "step": 1240
    },
    {
      "epoch": 0.9981036031540074,
      "grad_norm": 0.5687288045883179,
      "learning_rate": 0.00013719595517901068,
      "loss": 1.3495,
      "step": 1250
    },
    {
      "epoch": 1.0055893801776625,
      "grad_norm": 0.6147413849830627,
      "learning_rate": 0.0001366493577480186,
      "loss": 1.2714,
      "step": 1260
    },
    {
      "epoch": 1.0135742090028945,
      "grad_norm": 0.5752452611923218,
      "learning_rate": 0.00013610276031702652,
      "loss": 1.2399,
      "step": 1270
    },
    {
      "epoch": 1.0215590378281265,
      "grad_norm": 0.44226157665252686,
      "learning_rate": 0.00013555616288603444,
      "loss": 1.336,
      "step": 1280
    },
    {
      "epoch": 1.0295438666533587,
      "grad_norm": 0.5694147348403931,
      "learning_rate": 0.00013500956545504236,
      "loss": 1.2683,
      "step": 1290
    },
    {
      "epoch": 1.0375286954785907,
      "grad_norm": 0.5240720510482788,
      "learning_rate": 0.00013446296802405028,
      "loss": 1.3465,
      "step": 1300
    },
    {
      "epoch": 1.0455135243038227,
      "grad_norm": 0.5949749946594238,
      "learning_rate": 0.0001339163705930582,
      "loss": 1.1827,
      "step": 1310
    },
    {
      "epoch": 1.0534983531290547,
      "grad_norm": 0.584977924823761,
      "learning_rate": 0.00013336977316206613,
      "loss": 1.2714,
      "step": 1320
    },
    {
      "epoch": 1.061483181954287,
      "grad_norm": 0.5388108491897583,
      "learning_rate": 0.00013282317573107407,
      "loss": 1.2261,
      "step": 1330
    },
    {
      "epoch": 1.069468010779519,
      "grad_norm": 0.47467076778411865,
      "learning_rate": 0.000132276578300082,
      "loss": 1.3767,
      "step": 1340
    },
    {
      "epoch": 1.077452839604751,
      "grad_norm": 0.5110718607902527,
      "learning_rate": 0.00013172998086908992,
      "loss": 1.2093,
      "step": 1350
    },
    {
      "epoch": 1.0854376684299831,
      "grad_norm": 0.5613273978233337,
      "learning_rate": 0.00013118338343809784,
      "loss": 1.2461,
      "step": 1360
    },
    {
      "epoch": 1.0934224972552151,
      "grad_norm": 0.5806577801704407,
      "learning_rate": 0.00013063678600710578,
      "loss": 1.3775,
      "step": 1370
    },
    {
      "epoch": 1.1014073260804471,
      "grad_norm": 0.627375066280365,
      "learning_rate": 0.0001300901885761137,
      "loss": 1.3352,
      "step": 1380
    },
    {
      "epoch": 1.109392154905679,
      "grad_norm": 0.5916884541511536,
      "learning_rate": 0.00012954359114512163,
      "loss": 1.2211,
      "step": 1390
    },
    {
      "epoch": 1.1173769837309113,
      "grad_norm": 0.46799546480178833,
      "learning_rate": 0.00012899699371412955,
      "loss": 1.2844,
      "step": 1400
    },
    {
      "epoch": 1.1253618125561433,
      "grad_norm": 0.5180982947349548,
      "learning_rate": 0.0001284503962831375,
      "loss": 1.3327,
      "step": 1410
    },
    {
      "epoch": 1.1333466413813753,
      "grad_norm": 0.5181404948234558,
      "learning_rate": 0.00012790379885214542,
      "loss": 1.3445,
      "step": 1420
    },
    {
      "epoch": 1.1413314702066075,
      "grad_norm": 0.5337644815444946,
      "learning_rate": 0.00012735720142115334,
      "loss": 1.4227,
      "step": 1430
    },
    {
      "epoch": 1.1493162990318395,
      "grad_norm": 0.492093563079834,
      "learning_rate": 0.00012681060399016126,
      "loss": 1.3223,
      "step": 1440
    },
    {
      "epoch": 1.1573011278570715,
      "grad_norm": 0.4835357367992401,
      "learning_rate": 0.00012626400655916918,
      "loss": 1.2118,
      "step": 1450
    },
    {
      "epoch": 1.1652859566823035,
      "grad_norm": 0.6771097779273987,
      "learning_rate": 0.0001257174091281771,
      "loss": 1.4138,
      "step": 1460
    },
    {
      "epoch": 1.1732707855075357,
      "grad_norm": 0.6037405133247375,
      "learning_rate": 0.00012517081169718502,
      "loss": 1.2316,
      "step": 1470
    },
    {
      "epoch": 1.1812556143327677,
      "grad_norm": 0.5883229374885559,
      "learning_rate": 0.00012462421426619294,
      "loss": 1.1877,
      "step": 1480
    },
    {
      "epoch": 1.1892404431579997,
      "grad_norm": 0.5982654094696045,
      "learning_rate": 0.00012407761683520086,
      "loss": 1.385,
      "step": 1490
    },
    {
      "epoch": 1.197225271983232,
      "grad_norm": 0.61395263671875,
      "learning_rate": 0.0001235310194042088,
      "loss": 1.3524,
      "step": 1500
    },
    {
      "epoch": 1.205210100808464,
      "grad_norm": 0.6099626421928406,
      "learning_rate": 0.00012298442197321673,
      "loss": 1.2662,
      "step": 1510
    },
    {
      "epoch": 1.213194929633696,
      "grad_norm": 0.5448947548866272,
      "learning_rate": 0.00012243782454222465,
      "loss": 1.2906,
      "step": 1520
    },
    {
      "epoch": 1.2211797584589281,
      "grad_norm": 0.663535475730896,
      "learning_rate": 0.00012189122711123257,
      "loss": 1.4018,
      "step": 1530
    },
    {
      "epoch": 1.2291645872841601,
      "grad_norm": 0.5025274753570557,
      "learning_rate": 0.00012134462968024052,
      "loss": 1.3426,
      "step": 1540
    },
    {
      "epoch": 1.237149416109392,
      "grad_norm": 0.5539695620536804,
      "learning_rate": 0.00012079803224924844,
      "loss": 1.3158,
      "step": 1550
    },
    {
      "epoch": 1.2451342449346243,
      "grad_norm": 0.6050847768783569,
      "learning_rate": 0.00012025143481825636,
      "loss": 1.2782,
      "step": 1560
    },
    {
      "epoch": 1.2531190737598563,
      "grad_norm": 0.5565800070762634,
      "learning_rate": 0.00011970483738726428,
      "loss": 1.3086,
      "step": 1570
    },
    {
      "epoch": 1.2611039025850883,
      "grad_norm": 0.5069237351417542,
      "learning_rate": 0.00011915823995627222,
      "loss": 1.358,
      "step": 1580
    },
    {
      "epoch": 1.2690887314103203,
      "grad_norm": 0.5766551494598389,
      "learning_rate": 0.00011861164252528014,
      "loss": 1.2281,
      "step": 1590
    },
    {
      "epoch": 1.2770735602355525,
      "grad_norm": 0.5761545896530151,
      "learning_rate": 0.00011806504509428806,
      "loss": 1.3655,
      "step": 1600
    },
    {
      "epoch": 1.2850583890607845,
      "grad_norm": 0.5453554391860962,
      "learning_rate": 0.00011751844766329598,
      "loss": 1.2708,
      "step": 1610
    },
    {
      "epoch": 1.2930432178860165,
      "grad_norm": 0.57380211353302,
      "learning_rate": 0.0001169718502323039,
      "loss": 1.2431,
      "step": 1620
    },
    {
      "epoch": 1.3010280467112487,
      "grad_norm": 0.5441863536834717,
      "learning_rate": 0.00011642525280131185,
      "loss": 1.5067,
      "step": 1630
    },
    {
      "epoch": 1.3090128755364807,
      "grad_norm": 0.6105745434761047,
      "learning_rate": 0.00011587865537031977,
      "loss": 1.3535,
      "step": 1640
    },
    {
      "epoch": 1.3169977043617127,
      "grad_norm": 0.48059919476509094,
      "learning_rate": 0.00011533205793932769,
      "loss": 1.1696,
      "step": 1650
    },
    {
      "epoch": 1.3249825331869447,
      "grad_norm": 0.6408901214599609,
      "learning_rate": 0.00011478546050833561,
      "loss": 1.3802,
      "step": 1660
    },
    {
      "epoch": 1.332967362012177,
      "grad_norm": 0.5836838483810425,
      "learning_rate": 0.00011423886307734355,
      "loss": 1.2753,
      "step": 1670
    },
    {
      "epoch": 1.340952190837409,
      "grad_norm": 0.6482663154602051,
      "learning_rate": 0.00011369226564635147,
      "loss": 1.2721,
      "step": 1680
    },
    {
      "epoch": 1.348937019662641,
      "grad_norm": 0.5463566184043884,
      "learning_rate": 0.00011314566821535939,
      "loss": 1.2676,
      "step": 1690
    },
    {
      "epoch": 1.3569218484878731,
      "grad_norm": 0.6982365250587463,
      "learning_rate": 0.00011259907078436731,
      "loss": 1.3708,
      "step": 1700
    },
    {
      "epoch": 1.364906677313105,
      "grad_norm": 0.5628905296325684,
      "learning_rate": 0.00011205247335337526,
      "loss": 1.3634,
      "step": 1710
    },
    {
      "epoch": 1.372891506138337,
      "grad_norm": 0.552383542060852,
      "learning_rate": 0.00011150587592238318,
      "loss": 1.4114,
      "step": 1720
    },
    {
      "epoch": 1.380876334963569,
      "grad_norm": 0.5684675574302673,
      "learning_rate": 0.0001109592784913911,
      "loss": 1.2546,
      "step": 1730
    },
    {
      "epoch": 1.3888611637888013,
      "grad_norm": 0.6585953235626221,
      "learning_rate": 0.00011041268106039902,
      "loss": 1.3507,
      "step": 1740
    },
    {
      "epoch": 1.3968459926140333,
      "grad_norm": 0.5924091935157776,
      "learning_rate": 0.00010986608362940696,
      "loss": 1.4594,
      "step": 1750
    },
    {
      "epoch": 1.4048308214392655,
      "grad_norm": 0.5740030407905579,
      "learning_rate": 0.00010931948619841488,
      "loss": 1.2787,
      "step": 1760
    },
    {
      "epoch": 1.4128156502644975,
      "grad_norm": 0.5865775346755981,
      "learning_rate": 0.0001087728887674228,
      "loss": 1.4569,
      "step": 1770
    },
    {
      "epoch": 1.4208004790897295,
      "grad_norm": 0.5659471154212952,
      "learning_rate": 0.00010822629133643072,
      "loss": 1.2797,
      "step": 1780
    },
    {
      "epoch": 1.4287853079149615,
      "grad_norm": 0.5548444390296936,
      "learning_rate": 0.00010767969390543864,
      "loss": 1.2271,
      "step": 1790
    },
    {
      "epoch": 1.4367701367401937,
      "grad_norm": 0.5706769227981567,
      "learning_rate": 0.00010713309647444659,
      "loss": 1.2955,
      "step": 1800
    },
    {
      "epoch": 1.4447549655654257,
      "grad_norm": 0.6444665193557739,
      "learning_rate": 0.0001065864990434545,
      "loss": 1.3486,
      "step": 1810
    },
    {
      "epoch": 1.4527397943906577,
      "grad_norm": 0.6083551049232483,
      "learning_rate": 0.00010603990161246242,
      "loss": 1.3292,
      "step": 1820
    },
    {
      "epoch": 1.46072462321589,
      "grad_norm": 0.6818403601646423,
      "learning_rate": 0.00010549330418147034,
      "loss": 1.319,
      "step": 1830
    },
    {
      "epoch": 1.468709452041122,
      "grad_norm": 0.5302691459655762,
      "learning_rate": 0.00010494670675047829,
      "loss": 1.2489,
      "step": 1840
    },
    {
      "epoch": 1.476694280866354,
      "grad_norm": 0.5308419466018677,
      "learning_rate": 0.0001044001093194862,
      "loss": 1.2102,
      "step": 1850
    },
    {
      "epoch": 1.484679109691586,
      "grad_norm": 0.5722593665122986,
      "learning_rate": 0.00010385351188849413,
      "loss": 1.4282,
      "step": 1860
    },
    {
      "epoch": 1.4926639385168181,
      "grad_norm": 0.5663117170333862,
      "learning_rate": 0.00010330691445750205,
      "loss": 1.3306,
      "step": 1870
    },
    {
      "epoch": 1.50064876734205,
      "grad_norm": 0.6569058299064636,
      "learning_rate": 0.00010276031702650998,
      "loss": 1.3814,
      "step": 1880
    },
    {
      "epoch": 1.5086335961672823,
      "grad_norm": 0.5725376009941101,
      "learning_rate": 0.0001022137195955179,
      "loss": 1.2311,
      "step": 1890
    },
    {
      "epoch": 1.5166184249925143,
      "grad_norm": 0.8573007583618164,
      "learning_rate": 0.00010166712216452582,
      "loss": 1.2968,
      "step": 1900
    },
    {
      "epoch": 1.5246032538177463,
      "grad_norm": 0.5060429573059082,
      "learning_rate": 0.00010112052473353375,
      "loss": 1.379,
      "step": 1910
    },
    {
      "epoch": 1.5325880826429783,
      "grad_norm": 0.5409675240516663,
      "learning_rate": 0.0001005739273025417,
      "loss": 1.2398,
      "step": 1920
    },
    {
      "epoch": 1.5405729114682103,
      "grad_norm": 0.6139855980873108,
      "learning_rate": 0.00010002732987154961,
      "loss": 1.3786,
      "step": 1930
    },
    {
      "epoch": 1.5485577402934425,
      "grad_norm": 0.656960666179657,
      "learning_rate": 9.948073244055754e-05,
      "loss": 1.4423,
      "step": 1940
    },
    {
      "epoch": 1.5565425691186745,
      "grad_norm": 0.6295264363288879,
      "learning_rate": 9.893413500956547e-05,
      "loss": 1.3824,
      "step": 1950
    },
    {
      "epoch": 1.5645273979439067,
      "grad_norm": 0.5935800075531006,
      "learning_rate": 9.838753757857339e-05,
      "loss": 1.3732,
      "step": 1960
    },
    {
      "epoch": 1.5725122267691387,
      "grad_norm": 0.558279812335968,
      "learning_rate": 9.784094014758131e-05,
      "loss": 1.2444,
      "step": 1970
    },
    {
      "epoch": 1.5804970555943707,
      "grad_norm": 0.5697979927062988,
      "learning_rate": 9.729434271658923e-05,
      "loss": 1.2315,
      "step": 1980
    },
    {
      "epoch": 1.5884818844196027,
      "grad_norm": 0.6276721358299255,
      "learning_rate": 9.674774528559715e-05,
      "loss": 1.2669,
      "step": 1990
    },
    {
      "epoch": 1.5964667132448347,
      "grad_norm": 0.5390774011611938,
      "learning_rate": 9.620114785460509e-05,
      "loss": 1.3912,
      "step": 2000
    },
    {
      "epoch": 1.604451542070067,
      "grad_norm": 0.5768475532531738,
      "learning_rate": 9.565455042361301e-05,
      "loss": 1.3076,
      "step": 2010
    },
    {
      "epoch": 1.612436370895299,
      "grad_norm": 0.6196742653846741,
      "learning_rate": 9.510795299262094e-05,
      "loss": 1.445,
      "step": 2020
    },
    {
      "epoch": 1.6204211997205311,
      "grad_norm": 0.6306917667388916,
      "learning_rate": 9.456135556162886e-05,
      "loss": 1.2951,
      "step": 2030
    },
    {
      "epoch": 1.628406028545763,
      "grad_norm": 0.6040884852409363,
      "learning_rate": 9.40147581306368e-05,
      "loss": 1.4174,
      "step": 2040
    },
    {
      "epoch": 1.636390857370995,
      "grad_norm": 0.5716307163238525,
      "learning_rate": 9.346816069964472e-05,
      "loss": 1.3472,
      "step": 2050
    },
    {
      "epoch": 1.644375686196227,
      "grad_norm": 0.6425180435180664,
      "learning_rate": 9.292156326865264e-05,
      "loss": 1.3849,
      "step": 2060
    },
    {
      "epoch": 1.652360515021459,
      "grad_norm": 0.5202532410621643,
      "learning_rate": 9.237496583766056e-05,
      "loss": 1.1907,
      "step": 2070
    },
    {
      "epoch": 1.6603453438466913,
      "grad_norm": 0.6191459894180298,
      "learning_rate": 9.18283684066685e-05,
      "loss": 1.3705,
      "step": 2080
    },
    {
      "epoch": 1.6683301726719233,
      "grad_norm": 0.5960055589675903,
      "learning_rate": 9.128177097567642e-05,
      "loss": 1.4388,
      "step": 2090
    },
    {
      "epoch": 1.6763150014971555,
      "grad_norm": 0.6293194890022278,
      "learning_rate": 9.073517354468435e-05,
      "loss": 1.3658,
      "step": 2100
    },
    {
      "epoch": 1.6842998303223875,
      "grad_norm": 0.6591172814369202,
      "learning_rate": 9.018857611369227e-05,
      "loss": 1.3107,
      "step": 2110
    },
    {
      "epoch": 1.6922846591476195,
      "grad_norm": 0.5533837080001831,
      "learning_rate": 8.964197868270021e-05,
      "loss": 1.3434,
      "step": 2120
    },
    {
      "epoch": 1.7002694879728515,
      "grad_norm": 0.5919175744056702,
      "learning_rate": 8.909538125170811e-05,
      "loss": 1.3458,
      "step": 2130
    },
    {
      "epoch": 1.7082543167980835,
      "grad_norm": 0.5011740922927856,
      "learning_rate": 8.854878382071604e-05,
      "loss": 1.175,
      "step": 2140
    },
    {
      "epoch": 1.7162391456233157,
      "grad_norm": 0.518734872341156,
      "learning_rate": 8.800218638972397e-05,
      "loss": 1.4266,
      "step": 2150
    },
    {
      "epoch": 1.724223974448548,
      "grad_norm": 0.5686336755752563,
      "learning_rate": 8.745558895873189e-05,
      "loss": 1.4499,
      "step": 2160
    },
    {
      "epoch": 1.73220880327378,
      "grad_norm": 0.5487619042396545,
      "learning_rate": 8.690899152773983e-05,
      "loss": 1.3251,
      "step": 2170
    },
    {
      "epoch": 1.740193632099012,
      "grad_norm": 0.5979844331741333,
      "learning_rate": 8.636239409674775e-05,
      "loss": 1.4586,
      "step": 2180
    },
    {
      "epoch": 1.748178460924244,
      "grad_norm": 0.6200103163719177,
      "learning_rate": 8.581579666575568e-05,
      "loss": 1.2786,
      "step": 2190
    },
    {
      "epoch": 1.7561632897494759,
      "grad_norm": 0.6213494539260864,
      "learning_rate": 8.52691992347636e-05,
      "loss": 1.2484,
      "step": 2200
    },
    {
      "epoch": 1.764148118574708,
      "grad_norm": 0.5733300447463989,
      "learning_rate": 8.472260180377152e-05,
      "loss": 1.3281,
      "step": 2210
    },
    {
      "epoch": 1.77213294739994,
      "grad_norm": 0.6394009590148926,
      "learning_rate": 8.417600437277944e-05,
      "loss": 1.305,
      "step": 2220
    },
    {
      "epoch": 1.7801177762251723,
      "grad_norm": 0.6631125807762146,
      "learning_rate": 8.362940694178738e-05,
      "loss": 1.2849,
      "step": 2230
    },
    {
      "epoch": 1.7881026050504043,
      "grad_norm": 0.5924826264381409,
      "learning_rate": 8.30828095107953e-05,
      "loss": 1.2771,
      "step": 2240
    },
    {
      "epoch": 1.7960874338756363,
      "grad_norm": 0.7146218419075012,
      "learning_rate": 8.253621207980323e-05,
      "loss": 1.2552,
      "step": 2250
    },
    {
      "epoch": 1.8040722627008683,
      "grad_norm": 0.537765383720398,
      "learning_rate": 8.198961464881115e-05,
      "loss": 1.3411,
      "step": 2260
    },
    {
      "epoch": 1.8120570915261003,
      "grad_norm": 0.689427375793457,
      "learning_rate": 8.144301721781909e-05,
      "loss": 1.4587,
      "step": 2270
    },
    {
      "epoch": 1.8200419203513325,
      "grad_norm": 0.6246468424797058,
      "learning_rate": 8.089641978682701e-05,
      "loss": 1.3589,
      "step": 2280
    },
    {
      "epoch": 1.8280267491765645,
      "grad_norm": 0.6091035604476929,
      "learning_rate": 8.034982235583493e-05,
      "loss": 1.3689,
      "step": 2290
    },
    {
      "epoch": 1.8360115780017967,
      "grad_norm": 0.6384833455085754,
      "learning_rate": 7.980322492484285e-05,
      "loss": 1.4632,
      "step": 2300
    },
    {
      "epoch": 1.8439964068270287,
      "grad_norm": 0.6305555105209351,
      "learning_rate": 7.925662749385079e-05,
      "loss": 1.4352,
      "step": 2310
    },
    {
      "epoch": 1.8519812356522607,
      "grad_norm": 0.7024858593940735,
      "learning_rate": 7.871003006285871e-05,
      "loss": 1.3472,
      "step": 2320
    },
    {
      "epoch": 1.8599660644774927,
      "grad_norm": 0.5453495979309082,
      "learning_rate": 7.816343263186663e-05,
      "loss": 1.2253,
      "step": 2330
    },
    {
      "epoch": 1.8679508933027247,
      "grad_norm": 0.40056824684143066,
      "learning_rate": 7.761683520087456e-05,
      "loss": 1.2252,
      "step": 2340
    },
    {
      "epoch": 1.875935722127957,
      "grad_norm": 0.6671093702316284,
      "learning_rate": 7.707023776988248e-05,
      "loss": 1.2576,
      "step": 2350
    },
    {
      "epoch": 1.8839205509531889,
      "grad_norm": 0.5754145383834839,
      "learning_rate": 7.652364033889042e-05,
      "loss": 1.2375,
      "step": 2360
    },
    {
      "epoch": 1.891905379778421,
      "grad_norm": 0.6706652641296387,
      "learning_rate": 7.597704290789834e-05,
      "loss": 1.304,
      "step": 2370
    },
    {
      "epoch": 1.899890208603653,
      "grad_norm": 0.5747717022895813,
      "learning_rate": 7.543044547690626e-05,
      "loss": 1.3533,
      "step": 2380
    },
    {
      "epoch": 1.907875037428885,
      "grad_norm": 0.5292475819587708,
      "learning_rate": 7.488384804591418e-05,
      "loss": 1.3708,
      "step": 2390
    },
    {
      "epoch": 1.915859866254117,
      "grad_norm": 0.5526370406150818,
      "learning_rate": 7.433725061492212e-05,
      "loss": 1.3888,
      "step": 2400
    },
    {
      "epoch": 1.923844695079349,
      "grad_norm": 0.6657689213752747,
      "learning_rate": 7.379065318393004e-05,
      "loss": 1.3229,
      "step": 2410
    },
    {
      "epoch": 1.9318295239045813,
      "grad_norm": 0.5254282355308533,
      "learning_rate": 7.324405575293797e-05,
      "loss": 1.1719,
      "step": 2420
    },
    {
      "epoch": 1.9398143527298135,
      "grad_norm": 0.5865083932876587,
      "learning_rate": 7.269745832194589e-05,
      "loss": 1.1966,
      "step": 2430
    },
    {
      "epoch": 1.9477991815550455,
      "grad_norm": 0.6165686845779419,
      "learning_rate": 7.215086089095383e-05,
      "loss": 1.2682,
      "step": 2440
    },
    {
      "epoch": 1.9557840103802775,
      "grad_norm": 0.6027122735977173,
      "learning_rate": 7.160426345996173e-05,
      "loss": 1.3323,
      "step": 2450
    },
    {
      "epoch": 1.9637688392055095,
      "grad_norm": 0.6196164488792419,
      "learning_rate": 7.105766602896967e-05,
      "loss": 1.3017,
      "step": 2460
    },
    {
      "epoch": 1.9717536680307415,
      "grad_norm": 0.6684461832046509,
      "learning_rate": 7.051106859797759e-05,
      "loss": 1.3292,
      "step": 2470
    },
    {
      "epoch": 1.9797384968559737,
      "grad_norm": 0.6814669966697693,
      "learning_rate": 6.996447116698552e-05,
      "loss": 1.3857,
      "step": 2480
    },
    {
      "epoch": 1.9877233256812057,
      "grad_norm": 0.5861378312110901,
      "learning_rate": 6.941787373599344e-05,
      "loss": 1.3041,
      "step": 2490
    },
    {
      "epoch": 1.995708154506438,
      "grad_norm": 0.5663155317306519,
      "learning_rate": 6.887127630500137e-05,
      "loss": 1.3399,
      "step": 2500
    }
  ],
  "logging_steps": 10,
  "max_steps": 3759,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5954970181369856e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
